
\newcommand{\adrian}[1]{\textcolor{orange}{#1}}

\section{Introduction}

\gf{IR}, as defined by \href{https://en.wikipedia.org/wiki/Information_retrieval}{Wikipedia} is ``the task of identifying and retrieving information system resources that are relevant to an information need.'' Traditionally, \gf{IR} has long been structured around the ``index-retrieve-then-rank'' pipeline \cite{10.1145/3589335.3641239, Metzler_2021}, with the most popular (and best performing) methods using sparse retrieval, such as BM25 \cite{bm25robertson2009} and SPLADE \cite{splade10.1145/3404835.3463098}. These rely on lexical representations but suffer when terms do not overlap exactly \cite{Choi_2022,     biswas2024efficientinterpretableinformationretrieval, gao2021coilrevisitexactlexical}. A newer method called \gf{DR}, popularized by frameworks such as DPR \cite{karpukhin2020densepassageretrievalopendomain} utilize the bidirectional-encoder architectures (typically from BERT \cite{devlin-etal-2019-bert}) to address this issue by encoding queries and documents into semantic vector representations and then calculating the similarity between query vectors and document vectors, usually via cosine similarity. However, \gs{DR} systems still depend on separate vector index structures \cite{xiong2020approximatenearestneighbornegative, li2023constructingtreebasedindexefficient}, and queries and documents are encoded independently with limited semantic interaction \cite{ni2021largedualencodersgeneralizable}. %todo is this a bad thing?

\gf{GR} represents an emerging framework in \gf{IR} which aims to directly generate document identifiers through autoregressive language models such as BART \cite{lewis-etal-2020-bart} and T5 \cite{t5-raffel2023exploringlimitstransferlearning}, consolidating the retrieval process into a single end-to-end model \cite{zhang2024generativeretrievaltermset, li2025matchinggenerationsurveygenerative, kuo2024surveygenerativeinformationretrieval}. %todo recheck the latter part
using the \gs{LM} to encode the entire document corpus into the parameters of a single transformer. %todo cite
This architecture offers several advantages: it eliminates the need for separate dense vector indexes, reduces storage needs \cite{bevilacqua2022autoregressivesearchenginesgenerating} %todo check
reduces memory footprint \cite{zeng2024planningaheadgenerativeretrieval}, enables the model to learn document representations jointly with the retrieval objective \cite{sun2023learningtokenizegenerativeretrieval, tang2024listwisegenerativeretrievalmodels} %todo recheck
, and enables the optimization of document representations and retrieval objectives \cite{TODO} %todo too vague?
add while showing comparable performance on evaluation metrics
\cite{bevilacqua2022autoregressivesearchenginesgenerating, zeng2024planningaheadgenerativeretrieval, tang2024listwisegenerativeretrievalmodels}.  %todo citations for specfific claims, not end of sentence todo!
Despite these promising characteristics, generative retrieval systems exhibit distinct failure modes that differ fundamentally from those observed in traditional retrieval architectures. While \gs{IR} failures typically manifest as low similarity scores in vector space, generative retrieval failures can include hallucination of non-existent document identifiers, incorrect pruning during autoregressive generation, poor performance on dynamically expanding corpora, and challenges with scalability \cite{Metzler_2021, Ji_2023, zeng2024planningaheadgenerativeretrieval, pradeep2023doesgenerativeretrievalscale}, among others. %TODO cite each citation to specific failure mentioned
It is also worth noting that some \gs{GR} implementations employ auxiliary data structures for constrained generation, making them not fully end-to-end differentiable.\textbf{recheck}

Existing literature on \gs{GR} documents various challenges and failure cases. However, these failures are typically discussed in isolation with individual paper proposing new methods or architectures. There is little systematic taxonomy %todo really? https://arxiv.org/pdf/2404.14851 might be close
that classifies the types of failures that occur in generative retrieval systems, their relative frequencies in practice, or their underlying causes. %TODO am I even analyzing frequency?
This thesis addresses this gap by developing a taxonomy of failures, biases, and limitations for \gs{GR} systems by synthesizing documented failure cases from the literature and conducting analysis of retrieval outputs from the frameworks \gs{SEAL} \cite{bevilacqua2022autoregressivesearchenginesgenerating} and \gs{MINDER} models \cite{li2023multiviewidentifiersenhancedgenerative} on the \gf{nq} dataset. The \gs{nq} dataset \cite{kwiatkowski-etal-2019-natural} is a benchmark of Google search queries paired with full Wikipedia pages, annotated with the exact section and phrases of the page containing the answer.

As explained by \citet{li2025matchinggenerationsurveygenerative}, \gs{GR} processes can be roughly segmented into two components, document retrieval and response generation. In this thesis, we will focus on the failure modes of the former, with briefly discussing the failure modes of the latter as well.

\subsection{Research Question}

This thesis addresses two interconnected research questions:

\textbf{RQ1:} What is a systematic classification of the failures, behavioral biases, and structural limitations observed in generative retrieval systems?

\textbf{RQ2:} What are the most frequent failure modes observed in \gs{SEAL}'s and \gs{MINDER}'s n-gram-based retrieval, and to which mechanisms in the retrieval process can these failures be attributed?

\subsection{Contributions}

\textbf{TODO REWORK}
This thesis makes the following contributions:

\begin{enumerate}
    \item A synthesis of failure modes documented across generative retrieval research to understand and summarize the issues present in \gs{GR} and their mitigations.
    \item A mapping from abstract failure modes to observable indicators.
    \item Quantitative characterization of failure patterns via the \gs{SEAL} and \gs{MINDER} framework.
    \item Discussion of \gs{SOTA} challenges, solutions, and future research directions.


\end{enumerate}
The code used for this thesis is available on \href{https://github.com/richietk/thesis}{Github}.


\pagebreak

\section{Generative Retrieval}

In this section we introduce the concept of \gf{GR} more in depth.




\subsection{Information Retrieval}

 %todo recheck rewrite if needed

\subsection{DocID Design}

\gf{docid} design is important for retrieval effectiveness, as it replaces traditional external indices with the models internal parametric memory. A \gs{docid} serves as a unique ``index'' that the model must autoregressively generate to point toward a specific document or passage. Initial foundational works categorized these identifiers into three main types: unstructured atomic identifiers (such as unique random integers), naively structured string identifiers (such as titles), and semantically structured, numeric identifiers (such hierarchical cluster paths). While atomic IDs require the model to memorize random associations, semantic IDs group similar documents together under shared prefix tokens, improving retrieval quality \cite{pradeep2023doesgenerativeretrievalscale}.

String identifiers use natural language such as document titles, n-grams, URLs, or synthetic pseudo-queries, which allow the model to use its pre-trained linguistic knowledge rather than learning a new symbolic system from scratch. Substring or n-gram identifiers allow any span of text within a document to act as its identifier. In \gs{SEAL} and \gs{MINDER}, the documents aren't assigned a single identifier per document, unlike in other frameworks such as DSI \cite{tay2022transformermemorydifferentiablesearch} or GENRE \cite{sun2023learningtokenizegenerativeretrieval}. A single document can have multiple possible identifiers, as evidenced in \gs{SEAL}, which treats all n-grams within the passage as valid identifiers. Additionally, \gs{MINDER} also generates pseudo-queries for each document, %note: for each document or passage?
which it can occasionally use as ``n-grams''. While string-based IDs are more interpretable and generalizable to dynamic corpora where document collections expand, they often suffer from ``false pruning'', where the sequence-based generation discards a relevant document early in the beam search if the model fails to predict the correct prefix token. We will focus on string-based \glsplural{docid} in this thesis.

Other \glsplural{docid} types include semantically structured identifiers are designed to capture the relationships between documents using hierarchical categorization methods, such as by recursively applying k-means clustering to document embeddings, creating a tree where each root-to-leaf path serves as a DocID. Because they share prefixes with similar documents, they facilitate semantic matching, but their sequential nature makes them, just like string-based \glsplural{docid}, susceptible to false pruning. Unstructured atomic identifiers treat each document as a single, unique token added directly to the model's vocabulary. Unlike sequential designs, retrieval involves only a single decoding step, where the model sorts the logits of all \gs{docid} tokens to produce a ranked list. While this makes them immune to false pruning and very FLOP efficient during inference, they require a large increase in model parameters as the corpus scales \cite{pradeep2023doesgenerativeretrievalscale}. %This is because atomic \glsplural{docid} predict the document in a single softmax, the output layer must contain one set of weights per document, causing the parameter count to grow with the collection size \cite{nguyen2023generativeretrievaldenseretrieval, kuo2024surveygenerativeinformationretrieval}.
Because they are often randomly initialized, they lack inherent semantic connections to the text, forcing the model to rely entirely on memorizing the association between document content and its identifier during the training phase.

\gs{docid} design is consistently identified as a core challenge affecting these representation issues. Numerical \glsplural{docid} offer efficiency but suffer from limited generalization and overfitting to initial training sets, while string-based \glsplural{docid} maintain better semantic alignment but are more susceptible to false pruning \cite{zhang2024generativeretrievaltermset, zhang2025replicationexplorationgenerativeretrieval, 10.1145/3589335.3641239}.

\subsection{SEAL Architecture}

\gf{SEAL} \cite{bevilacqua2022autoregressivesearchenginesgenerating}, addresses \gs{docid} design challenges by using existing substrings as its retrieval targets. Such challenges specifically addressed are the unavailability of unique metadata (like titles) for all documents, the insufficient granularity of page- and document-level identifiers, as well as the need to force a structure onto the search space, such as hierarchical cluster trees, which can be difficult to construct for large-scale benchmarks. %todo recheck why are these specifically adressed/improved with ngrams
\gs{SEAL} chunks documents into passages and defines identifiers as any n-gram within those passages. An BART model is trained to generate distinctive, query-relevant n-grams that identify documents containing them. \gs{SEAL} constrains generation during decoding so that the model only generates n-grams actually existing in the corpus. For ranking, \gs{SEAL} aggregates information from multiple generated n-grams while downweighing overlaps if an n-gram overlaps with a higher-scoring one already selected. This is implemented so that the final score does not overscore based on repetitive or redundant document content.

\paragraph{Retrieval Mechanism}

During the generation phase, \gs{SEAL} enforces that at each decoding step, the language model can only generate tokens that would extend the current partial n-gram into a sequence that exists somewhere in the corpus. As an example, when \gs{SEAL} has generated the partial sequence ``earthquakes can be,'', ``earthquakes can be predicted'' can only be generated if this complete 4-gram exists as a contiguous substring in at least one document \cite{bevilacqua2022autoregressivesearchenginesgenerating}. ``earthquakes can be predicted'' exists in the corpus in the sentence ``discussing why the claim that earthquakes can be predicted is false.''

After n-gram generation, in the retrieval phase, for each n-gram, \gs{SEAL} identifies every passage containing that n-gram and assign a relevance score based on the n-gram's uniqueness and frequency in the corpus.
Passages are then ranked by additively aggregating the scores from all n-grams they contain. The FM-index, which \gs{SEAL} uses, provides a range of rows in the Burrows-Wheeler Transform matrix, and each row corresponds to an occurrence of that n-gram in the corpus. \gs{SEAL}then maps these occurrences back to their respective document/passage id-s.This means that a document scores highly not because any single n-gram uniquely identifies it, but because it contains multiple high-scoring n-grams. This provides robustness, since even if some highly ranked n-grams point to an irrelevant document, other n-grams can still guide retrieval toward relevant documents. %todo recheck the part about FM / BWT for clarity.

This architecture helps explain why \gs{SEAL} can overcome vocabulary mismatch issues. While the model cannot generate n-grams that do not exist in the corpus, it can find paraphrases and related terms as long as those appear in corpus documents using BART's parametric knowledge.

\paragraph{Scoring Mechanism}
\label{par:scormech}
\gs{SEAL}'s failure modes are directly influenced by its scoring function. \gs{SEAL} computes n-gram scores by combining the language model's conditional probability of an n-gram given a query with corpus frequency to favor distinctive n-grams.

The unconditional n-gram probability is computed from corpus statistics:
\begin{equation}
    P(n) = \frac{F(n, R)}{\sum_{d \in R} |d|}
    \label{eq:unconditional-prob}
\end{equation}
where $F(n, R)$ is the frequency of n-gram $n$ in corpus $R$, and $|d|$ is the length of document $d$.

The n-gram score combines conditional and unconditional probabilities:
\begin{equation}
    w(n, q) = \max\left(0, \log \frac{P(n|q)(1 - P(n))}{P(n)(1 - P(n|q))}\right)
    \label{eq:ngram-score}
\end{equation}

This formulation promotes n-grams that have high conditional probability given the query ($P(n|q)$), computed by BART, but low unconditional probability in the corpus ($P(n)$). This means that distinctive, query-relevant n-grams are prioritized.

It is important to highlight that $P(n|q)$ is computed autoregressively by BART and depends on both the n-gram $n$ and the query $q$. Consequently, the same n-gram string can receive different scores for different queries. Therefore n-gram scores are query-dependent.

Finally, the document score aggregates contributions from multiple non-overlapping n-grams:
\begin{equation}
    W(d, q) = \sum_{n \in K^{(d)}} w(n, q)^\alpha \cdot \text{cover}(n, K^{(d)})
    \label{eq:doc-score}
\end{equation}
where $K^{(d)}$ is the subset of generated n-grams $n$ matching document $d$. 
An n-gram is included in $K^{(d)}$ only if it has at least one occurrence 
in $d$ that does not positionally overlap with a higher-scoring n-gram's 
occurrence. The parameter $\alpha$ is a hyperparameter. The coverage weight $\text{cover}(n, K^{(d)})$ downweights individual n-grams whose tokens overlap with tokens of higher-scoring n-grams in the same document, preventing passages from receiving inflated scores when they contain many lexically similar n-grams.\cite{bevilacqua2022autoregressivesearchenginesgenerating}. %todo clear enough or should I expand?

Here it is worth noting that \gs{SEAL}, generates fixed token-length ngrams (by default $k=10$, except for titles). However, they save all partially-decoded sequences from the beam-search \cite{bevilacqua2022autoregressivesearchenginesgenerating}. Because the system records this history, the final score of a passage is always the sum of strings of different lengths. As expected, n-gram length is strongly correlated with frequency ($\rho=-0.835, p<0.001$). Consequentially, 

To illustrate \gs{SEAL}'s n-gram based scoring, lets examine the query ``who sings does he love me with reba''. The top-ranked passage (titled ``Linda Davis'') matched the following keys (excerpt):

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lrr}
        \toprule
        \textbf{N-gram} & \textbf{Corpus Freq. $F(n,R)$} & \textbf{Score $w(n,q)$} \\
        \midrule
        \texttt{</s> Linda Davis @@} & 9 & 216.33 \\
        \texttt{ "Does He Love} & 23 & 196.06 \\
        \texttt{ Reba} & 1,851 & 103.56 \\
        \texttt{ country singles} & 777 & 83.47 \\
        \texttt{ country music singer} & 4,024 & 39.20 \\
        \bottomrule
    \end{tabular}
    \caption{Example n-gram keys for query ``who sings does he love me with reba''. Lower corpus frequency correlates with higher scores for query-relevant n-grams.}
    \label{tab:ngram-example}
\end{table}

It is also worth noting that while the additive nature of the scoring mechanism could theoretically bias retrieval toward longer documents in a corpus of varying document lengths, this effect is neutralized by dividing $F(n, R)$ by the total number of tokens in the corpus $R$.

\subsection{MINDER Architecture}
\label{sec:minderarch}
The \gf{MINDER} framework \cite{li2023multiviewidentifiersenhancedgenerative} extends the n-gram based approach of \gs{SEAL} by introducing synthetic and multiview identifiers. \gs{MINDER} is motivated by the observation that single-view identifiers, such as document titles or extractive n-grams, often lack the contextualized information necessary to satisfy complex queries, especially if those require rephrasing.

To address this, \gs{MINDER} assigns three distinct ``views'' of identifiers to each passage:
\begin{enumerate}
     \item Title of the document
     \item N-grams, similar to \gs{SEAL}
     \item Pseudo-queries, i.e. synthetic identifiers generated via a query-generation model that reflect potential queries
\end{enumerate}

The inclusion of pseudo-queries is intended to bridge the gap between user queries and document content. While \gs{SEAL} is limited to the exact word orderings present in the corpus, \gs{MINDER}'s thus allows the model to match queries against rephrased or summarized versions of the document via pseudo-queries.

\gs{MINDER} relies on the conditional probability $P(i\allowbreak|q)$ of the identifier $i$ given query $q$, computed by the autoregressive language model. Because pseudo-queries are typically much longer than n-grams, their raw log-probabilities are naturally lower due to the multiplicative nature of autoregressive decoding. To ensure that pseudo-queries can compete with shorter identifiers, \gs{MINDER} introduces a biased score to offset this length penalty. The final relevance score for a passage $p$ is an aggregation of the scores from all predicted identifiers that appear in that passage across all three views:
\begin{equation}
   S(q, p) = \sum_{i \in I_p} s(i, q)
   \label{eq:minder-aggregation}
\end{equation}

where $I_p$ is the set of identifiers generated by the model for passage $p$, and $s(i, q)$ is the language model score for identifier $i$.

To manage multiple identifier types, \gs{MINDER} identifier-specific prefix tokens (e.g., \texttt{<TS>} for titles, \texttt{<QS>} for queries). During inference, the model is prompted with an identifier prefix and the query, and constrains the generation to valid sequences within the respective view.

\subsection{Evolution of SEAL/MINDER}

While this thesis focuses primarily on the foundational \gs{SEAL} and \gs{MINDER} architectures, it is essential to mention their successors like LTRGR and DGR.

All four systems share a common a BART-based autoregressive language model paired with an FM-Index for constrained decoding. \gs{MINDER} builds upon \gs{SEAL} by adding ``multiview identifiers'', such as pseudoqueries. \gs{LTRGR} introduced a second training phase using a supervised Rank Loss, to minimize the score of negative passages relative to positive ones. \gs{DGR} uses a cross-encoder model as a to imrpove the ranking list.

This thesis intentionally focuses its failure analysis on the core of such systems, as \gs{LTRGR} and \gs{DGR} function as ``optimization layers'' built upon the \gs{MINDER}. Therefore, many of the fundamental behavioral biases are inherited from the underlying generative mechanics. In case that does not hold, we will specifically mention the other systems. %TODO!!!!


\section{Literature Review of Failure Modes} 
\label{sec:failure-modes-literature}

Despite the advantages, \gs{GR} systems exhibit several distinct failure modes, behavioral biases, and structural limitations that motivate this thesis.

\paragraph{False Pruning}
\label{par:false-pruning}

Prefix pruning, also referred to as false pruning occurs during autoregressive generation of \glsplural{docid} when the beam search prematurely discards the prefix of a relevant docid because its cumulative probability at a specific step is lower than that of competing candidates \cite{zhang2024generativeretrievaltermset, zeng2024planningaheadgenerativeretrieval, sun2023learningtokenizegenerativeretrieval, zeng2023scalableeffectivegenerativeinformation}. This occurs as beam search tends to get stuck in the local optima \cite{stahlberg2019nmtsearcherrorsmodel}, as it perceives only preceding tokens and lacks visibility into subsequent ones, and thus, once a prefix is pruned, the system cannot recover the correct document regardless of subsequent token probabilities. There have been efforts to combat this, such as implementing backtracking \cite{Zhou2005BeamStackSI} and replacing beam-search with a set-based sampling \cite{zhang2024generativeretrievaltermset}. Non-autoregressive generation does not exhibit this issue \cite{valluri2024scalingvocabularynonautoregressivemodels, zhao2026diffugrgenerativedocumentretrieval}. Empirical analysis by \citet{zeng2024planningaheadgenerativeretrieval} indicate that even with large beam sizes, constrained beam search often fails to match brute-force decoding performance, suggesting that relevant \glsplural{docid} are frequently eliminated during search.

The autoregressive nature of decoding inherently favors shorter n-grams over longer ones, \cite{yuan2024generativedenseretrievalmemory} drowning out highly specific and discriminative n-grams. Additionally, longer n-grams naturally introduce more chances of false pruning. These indicate that the global optimum itself could be a short or irrelevant prefix, making the model unable to accurately pinpoint correct passages by preferring the shortest valid \gs{docid} path or the one with the most frequent initial tokens, regardless of their actual relevance to the query \cite{zeng2023scalableeffectivegenerativeinformation}.

\paragraph{Contiguous n-grams}
\textbf{TODO!!!!}
As discussed in the previous paragraph \ref{par:false-pruning}, a fundamental structural constraint of autoregressive, beam-search and n-gram-based retrieval is that such n-grams must be strictly contiguous substrings within the corpus. This means the model cannot match n-grams that would semantically connect a query to a passage if the relevant terms are separated by intervening words or are cut off between passages.

Consider a passage stating ``Tom likes pizza and tomatoes'' and a query ``does Tom like tomatoes?'' The relevant n-gram ``likes tomatoes'' does not exist as a contiguous substring in the passage due to the intervening words ``pizza'' and ``and.''  This issue is partially mitigated by the fact that n-grams ``and tomatoes,'' ``pizza and tomatoes,'' ``tomatoes.'' are still generated and scored, however this is not a robust solution. Introducing skip-grams \cite{TODO}, using methods such as permutation-invariant decoding \cite{zhang2024generativeretrievaltermset}, or forcing overlap between passages \cite{TODO} could theoretically be solutions to this problem.


\paragraph{Hallucination}
\label{par:hallucination}


Related is the problem of hallucination, in which the model generates invalid (with respect to the pre-defined document index) identifiers. Constrained decoding is often used to eliminate this problem with solutions such as constraining the docid generation through an FM-index \cite{bevilacqua2022autoregressivesearchenginesgenerating} or a trie \cite{decao2021autoregressiveentityretrieval}. Using Bloom filters \cite{10.1145/362686.362692} could be another method \textbf{TODO THIS ISNT VALIDATED}. There have been several implementations of constrained decoding, such as \gs{SEAL} \cite{bevilacqua2022autoregressivesearchenginesgenerating} and \gs{MINDER} \cite{li2023multiviewidentifiersenhancedgenerative}.
 
Several %several?
GR solutions use intentional hallucination, such as \gs{MINDER}\cite{li2023multiviewidentifiersenhancedgenerative}, where the generation of additional identifiers, such as pseudoqueries is achieved through intentional, constrained hallucination of docid generation.

\paragraph{Forgetfulness}
\label{par:forgetfulness}

One of the problems in GR is the apparent forgetting of fine-grained features \cite{yuan2024generativedenseretrievalmemory}. While generative models capture coarse-grained semantic clusters effectively, they struggle to accurately memorize and decode fine-grained document distinctions \cite{wang2025mindrefmimickinghumanmemory}. According to \citet{yuan2024generativedenseretrievalmemory}, error rates (defined as the \% of tokens predicted at a position that does not match the \gs{GT}token in a docid) for NCI \cite{wang2023neuralcorpusindexerdocument} increase significantly at later token positions in \gs{docid} sequences, rising from approximately 1\% at the first position to over 12\% by the sixth position.

\paragraph{Indistinct Identifiers}
\label{par:distinct-ids}

\gs{GR} systems also suffer from cases where the mapping from document content to identifiers is not sufficiently distinct \cite{zhang-etal-2025-multi-level}. When using synthetic queries \cite{tang2023semanticenhanceddifferentiablesearchindex} % https://arxiv.org/pdf/1904.08375 could be relevant but it just appends the pseudoqueries, not uses as docids
or semantic clustering \cite{tay2022transformermemorydifferentiablesearch} as identifiers, multiple documents may share quasi-identical or highly similar representations, causing the model to conflate distinct documents \cite{Cheng_Dou_Zhu_Li_2025, sun2023learningtokenizegenerativeretrieval, li2025matchinggenerationsurveygenerative}.

\paragraph{Scalability}
\label{par:scalability}

Scalability limitations become evident as corpus size increases, as \gs{GR} models exhibit limitations in terms of capacity \cite{nguyen2023generativeretrievaldenseretrieval, zeng2023scalableeffectivegenerativeinformation}. The ``parameter budget'' of the transformer can become insufficient to encode nuances of a massive corpus. Empirical studies published by \citet{pradeep2023doesgenerativeretrievalscale} show sharp performance decline on T5 \cite{t5-raffel2023exploringlimitstransferlearning} as corpora scale from 100,000 to 8.8 million passages. For example, the T5-XL model achieves only around 30\% of the MRR@10 score on the full MS MARCO corpus compared to the base corpus.

Associated with scalability is the issue that oftentimes increasing model size does not yield proportional improvements. According to the aforementioned study \cite{pradeep2023doesgenerativeretrievalscale}, scaling T5 from XL (3B parameters) to XXL (11B parameters) degraded retrieval performance when using naive numeric identifiers on the full MS MARCO dataset, despite identical training configurations.

Additionally, unstructured atomic identifiers, while considered non-mainstream %https://arxiv.org/pdf/2404.00684
assign each document a single unique integer identifier \cite{tay2022transformermemorydifferentiablesearch}, requiring the model's output vocabulary to equal the corpus size. Since the output projection layer has dimensions of hidden size Ã— vocabulary size \cite{10.1145/3726302.3730076, tay2022transformermemorydifferentiablesearch}, %is this relevant
it makes this approach prohibitively expensive for large corpora. As an example, 8.8 million documents (the amount of passages in MsMarcoFull \cite{bajaj2018msmarcohumangenerated}) with a hidden size of 768 (the hidden dimension of the T5-base model \cite{t5-raffel2023exploringlimitstransferlearning})
adds approximately 6.8 billion parameters to the output layer alone, making this approach prohibitively expensive for large corpora. In contrast, sequential identifiers reuse a fixed vocabulary across multiple decoding steps, keeping parameter count constant regardless of corpus size \cite{kuo2024surveygenerativeinformationretrieval, pradeep2023doesgenerativeretrievalscale}. \textbf{recheck this!}

\paragraph{Dynamic Corpus}
In environments where corpora change, \gs{GR} models exhibit two opposing failure modes \cite{Chen_2023, zhang2025replicationexplorationgenerativeretrieval, yuan2024generativedenseretrievalmemory, mehta2023dsiupdatingtransformermemory}. %are these the right citations?
Forgetting occurs when updating a model with new documents degrades its ability to retrieve previously indexed documents. Experiments demonstrate that sequential indexing of new batches can cause indexing accuracy for the initial corpus to drop by more than 25 points, necessitating retraining and increasing costs \cite{mehta2023dsiupdatingtransformermemory}. The opposing failure is an excessive bias against recency, where models fail to retrieve newly added content, favoring documents from the original training corpus \cite{zhang2025replicationexplorationgenerativeretrieval}.

\gs{GR} models also exhibit forgetting during initial training as well. Analysis of training dynamics shows models frequently ``forget'' and ``re-learn'' document-to-identifier mappings, with approximately 88\% of documents undergoing at least one forgetting event during training \cite{mehta2023dsiupdatingtransformermemory}.

An additional issue identified is the problem that newly added documents are practically inaccessible until model weights are updated and the model retrained, unlike dense retrieval systems which can index new embeddings instantly without retraining the encoder \cite{kuo2024surveygenerativeinformationretrieval}.

\paragraph{Interpretability}
Lastly, the lack of interpretability in generation processes presents challenges for understanding and debugging failures. Unlike sparse retrieval where term matching is transparent \cite{bm25robertson2009, splade10.1145/3404835.3463098}, or dense retrieval where embedding similarity can be analyzed \cite{karpukhin2020densepassageretrievalopendomain}, the internal decision processes of generative retrieval is significantly more opaque \cite{10.1145/3589335.3641239, ross2021evaluatinginterpretabilitygenerativemodels}.

\subsection{Failure Modes in Response Generation}
\label{subsec:failure-respgen}

After the \gs{GR} framework retrieved a document (or a list thereof), there usually is a component for generating a response for the user, summarizing the results \cite{TODO}.

Summarizing with \gp{LLM} provides failure modes such as hallucination \cite{Huang_2025} and not being up-to-date with latest information \cite{vu-etal-2024-freshllms}. These are usually tackled by methods such as self-reflection \cite{wei2023chainofthoughtpromptingelicitsreasoning, asai2023selfraglearningretrievegenerate} or by incorporating a non-parametric memory component, such as \gs{DPR}, to condition generation on retrieved documents at runtime \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, the latter commonly known as \gs{RAG}.
While the failure modes of \gs{RAG} systems are out-of-scope for this thesis, we briefly summarize the most critical failure modes here:
\textbf{TODO}

\section{Proposed Failure Taxonomy}
\label{sec:proposed-failure-taxonomy}
\textbf{TODO NOT READY}

\begin{table}[htbp]
    \centering
    \caption{Proposed Taxonomy of Generative Retrieval Failures.}
    \label{tab:failure-summary-tax}
    \begin{tabular}{lll}
        \toprule
        \textbf{Category} & \textbf{Specific Failure Mode} \\
        \midrule
        \textbf{Data-Level} & Annotation Errors \\
        & Answer Cut-off between Passages \\
        \midrule
        \textbf{Representational ?} & Metadata Overreliance \\
        & Cultural Bias \\
        & Model Capacity \\
        \midrule
        \textbf{Algorithmic ?} & DocID Discriminativeness ?\\
        & Short DocID Preference \\\\
        & False Pruning \\
        \bottomrule
    \end{tabular}
\end{table}


\textbf{OUTDATED!!!}
\begin{table}[htbp]
    \centering
    \caption{Classification of potential retrieval failure modes for \gs{SEAL} and \gs{MINDER}.}
    \label{tab:failure-summary}
    \begin{tabular}{lp{6cm}l}
        \toprule
        \textbf{Type} & \textbf{Potential Failure Mode} & \textbf{Applicability} \\
        \midrule
        \textbf{Measurable} & Nonspecific $n$-grams & Both \\
        & Too many unigrams & Both \\
        & Redundant $n$-grams & Both \\
        & Query--$n$-gram not overlapping & Both \\
        & Answer not in $n$-gram & Both \\
        & Single $n$-gram dominating & Both \\
        & Title repetition in top-$k$ & Both \\
        & No pseudo-queries as n-gram & \gs{MINDER} \\
        \midrule
        \textbf{Non-measurable} & False pruning & Both \\
        & Answer cut-off btwn. passages & Both \\
        & Optima not found in beam search & Both \\
        & Scalability issues & Both \\
        & Handling of dynamic corpora & Both \\
        & Training learning bottlenecks & Both \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Methodology}

Our methodology is threefold. First, we ran \gs{SEAL} and \gs{MINDER} with their default settings on the \gf{nq} \cite{kwiatkowski-etal-2019-natural}, analyzing their outputs. Second, we will run the models on the \gs{MSMARCO} dataset \cite{bajaj2018msmarcohumangenerated} as well.%todo explain reasoning why
Third, we will perform ablation studies based on our identifier failure modes from the literature review  in Section \ref{sec:failure-modes-literature} and the failure modes identified during our analysis of the outputs.

We chose \gf{nq} as it is an academic standard for retrieval benchmarking. The \gs{nq} retrieval corpus consists of a Wikipedia dump split into approximately 21 million passages of 100 tokens each.

The \gs{SEAL} and \gs{MINDER} output dataset contains the results of 6,515 queries from the \gl{nq} benchmark. For each query, the following relevant information is available:

\begin{itemize}
    \item The original natural language query
    \item \gs{GT} answer string(s)
    \item Annotated \gs{GT}passages containing the answer
    \item Negative and hard negative passages
    \item The model's top-100 retrieved passages, ranked by passage score, each containing:
    \begin{itemize}
        \item Passage title and text
        \item Aggregate document score
        \item The set of matched identifiers and their respective scores
    \end{itemize}
\end{itemize}

From the output data available, the n-gram keys are deemed most useful to understand \gs{SEAL}'s and\gs{MINDER}'s retrieval decisions. Each key entry contains three parts:
\begin{enumerate}
    \item The n-gram string (e.g., \texttt{`` Reba''}, \texttt{``</s> Linda Davis @@''})
    \item The corpus frequency of how many times this n-gram appears across all documents
    \item The n-gram score computed via the scoring function
\end{enumerate}

The \gs{MINDER} output is structurally identical to that of \gs{SEAL}'s, including being ran on the same 6,515 queries. The main difference arises from the pseudoqueries, which are appended to the \textit{text}.

The output dataset (of both systems) have an average per-query annotated correct passage count of 8.46, with the minimum being 1 and maximum being 101. This is important to analyze to accurately identify evaluation metrics. To illustrate the distribution, please consult the histogram below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/positive_ctxs_histogram.png}
\end{figure}

\paragraph{Evaluation Metrics}

We use R-precision as the overall retrieval quality of both \gs{SEAL} and \gs{MINDER} and hits@1 (being identical to precision@1) to quantify whether the model successfully retrieved a positive context in its top results for a query. We also use hits@10, since retrieval is rarely only about the first result only. We chose $k=10$ somewhat arbitrarily, as it provides a balance between being too strict by only measuring the top results and being too lax by allowing lower-scored retrieved passages to be included in the analysis. We avoid using Precision@k and Recall@k with $k>1$ to avoid misrepresentation of queries where the amount of ground truths is less than $k$.

For the analysis, results are calculated side-by-side for both systems analyzed when appropriate. To analyze distribution, we bin the results into deciles and report them. Additionally, when appropriate, we report the Spearman $rho$ values to analyze monotonic correlation between variables.


\section{Failure Analysis}

%stop saying "we observed a correlation". it should be sth like "the system exhibits X failure mode, as evidenced by correlation Y" although idk if correls are even important anymore.


\paragraph{Metadata Dependency}

%focus not just on titles, but on metadata itself
%Current: "Titles contribute 40% of the score. GUESS: this is a representation failure where the model failed to learn the content of the passages and has overfitted to the metadata. So is the model an actual robust retriever or a fancy title matcher, in which case why not just use a standard fuzzy search algorithm...

Both \gs{SEAL} and \gs{MINDER} rely passage metadata heavily to retrieve the correct passages. For \gs{SEAL}, in the top-100 retrieved passages among the 6515 queries analyzed, titles account for 39.85\% of total score. For \gs{MINDER}, titles make up  29.6\% and pseudoqueries 19.67\% of total score. In cases where the answer string is found exactly both as a ``title n-gram'' and ``nontitle n-gram'', the title n-gram receieves on average 2866\% more score in \gs{SEAL} and 2499\% in\gs{MINDER}. Interestingly, this overreliance drops when we only account for successfully retrieved passages, with titles accounting for 29.1\% for \gs{SEAL} and 20.6\% for \gs{MINDER}(11,27\% for pseudoqueries). The scoring differential between the answer string being a ``title n-gram'' and ``nontitle n-gram'' also drops to 1876\% for \gs{SEAL} and 1439\% For \gs{MINDER}. This can indicate that when the model is ``correct'', it is using more of the body-textThis can be considered a ``feature'' on cases where there is clear metadata in the dataset (e.g. NQ) and cases where retrieving the correct document is more important than retrieving the current passage. However, if our aim is to create a generalized GR model, this should be considered a ``failure''.

Both \citet{bevilacqua2022autoregressivesearchenginesgenerating} and \citet{li2023multiviewidentifiersenhancedgenerative,li2023learningrankgenerativeretrieval,li2024distillationenhancedgenerativeretrieval} note that performance drops when titles are unavailable \cite{bevilacqua2022autoregressivesearchenginesgenerating,li2023multiviewidentifiersenhancedgenerative,li2023learningrankgenerativeretrieval,li2024distillationenhancedgenerativeretrieval}. \citet{bevilacqua2022autoregressivesearchenginesgenerating}, however, does not report \gs{SEAL} ablation results without titles as identifiers. \gs{MINDER}performs 10.3\% worse recall@5 and 7.4\% worse recall@20 on NQ without titles and pseudoqueries. On \gs{MSMARCO} \cite{bajaj2018msmarcohumangenerated}, \gs{SEAL} had a 30.7\% worse Recall@5 than BM25 on \gs{MSMARCO}, with \gs{MINDER} only marignally (3.1\%) better than BM25, likely due to pseudoqueries. Neither \gs{SEAL}nor \gs{MINDER} achieved a Recall@5 larger than 30\% on \gs{MSMARCO}. LTRGR and DGR achieved 40.2\% and 42.9\%, respectively. It is also worth noting that NQ is based on highly structured Wikipedia passages, while MSMarco is less cleanly structured with messier or lacking metadata.

Our analysis shows that For \gs{SEAL}, title score is higher by ~280\% on average for positive ``ground truth'' retrieved passages than negative ones. However, the median is only ~41\% and the $\sigma$ ~1113\%, indicating that there is a subset of queries where the title is massively scored, indicating that the model uses it as quasi the only identifier, while for others the title is much less helpful. For \gs{MINDER}, accordingly, the values are ~325\%, 57.5\%, ~939\%. An extreme example was found in\gs{SEAL}, where for the query ``who sang this is how we do it'', correct passages have ~545x more title score than incorrect passages (in passages where a title-as-ngram is present). In these cases, title matching is extremely discriminative, which makes it well suited for datasets with clear metadata, but potentially bad for ones without.

\paragraph{Cultural Bias}

English content in public corpora is itself not culturally neutral, as a substantial portion reflects Western (particularly US and Eurocentric) perspectives and framing simply because such sources dominate English public corpora. As BART (which \gs{SEAL} and \gs{MINDER} uses) was trained on the English Wikipedia and on BookCorpus \cite{7410368, liu2019robertarobustlyoptimizedbert}, it can be presumed that BART would implicitly default to Western interpretations in ambiguous cases. Other commonly used language models, such as T5 and LLaMa-3 \cite{t5-raffel2023exploringlimitstransferlearning, grattafiori2024llama3herdmodels} were also trained on overwhelmingly English data. Similar bias was also demonstrated for LLMs %which LLMs, autoregressive? or all kind of? also demonstrated for encoder-decoder seq2seq? for bidirectional masked models? T5? bart? bert? is bart even autoregressive? which does each paper analyze?
by multiple papers \cite{naous2024havingbeerprayermeasuring, aowal2023detectingnaturallanguagebiases, 10.1145/3597307}. 

Similarly, the NQ dataset is also based on the English Wikipedia \cite{kwiatkowski-etal-2019-natural}, adding onto the issue. Such a ``Western-centricity'' can be also be shown in other commonly used datasets, such as MSMarco cite{bajaj2018msmarcohumangenerated}, Trivia QA \cite{joshi-etal-2017-triviaqa} and the KILT datasets \cite{petroni2021kiltbenchmarkknowledgeintensive}.

For \gs{SEAL}, this is best illustrated by the query: ``where did the southern song have their capital''.

Although the correct answer is Lin'an (the query referring to the Southern Song Dynasty), \gs{SEAL} focused entirely on passages about the southern United States, especially about about Tennessee and Virginia. The model likely %likely? cant you be sure?
found a quasi-perfect ``query keyword match'' for such passages and connected ``southern'' with the southern US, ``song'' with country music, the 'Dixie' song and the 'Music City' sobriquet of Nashville, Tennessee, and, building onto these, ``capital'' with Nashville being the capital of Tennesse and Richmond being the capital of Virginia and the former capital of the confederate South. These substrings, while common, were likely propped up by their implicit connection to the US in the dataset and BART, even though they possess different meanings in a US geography context versus a 12th-century Chinese history context.

There is no ground truth passages retrieved in the \textit{top-100} retrieved passages, which consists entirely of passages about the southern United States. This is also an example of \textit{false pruning}, as \gs{SEAL}seemingly %seemingly?
discarded the tokens related to the Song Dynasty early on. 

An important observation is that \gs{MINDER} successfully retrieved the relevant passages. All queries in the \textit{top-100} retrieved passages were in some way relevant to Ancient China, with the first ground-truth passage appearing at rank-4. Both systems use BART-large as their autoregressive language model and the same dataset to build their FM-index. The observed success is therefore likely due to the use of pseudoqueries. Since BART is capable of generating the correct terms, failure in \gs{SEAL} happened likely due to failing to disambiguate the context, in which pseudoqueries assisted. For the highest-ranked retrieved \gs{GT} passage, \gs{MINDER} generated pseudoqueries such as \textit{what was the capital of the song dynasty in north china} and \textit{who took over the capital of the song dynasty}.

%note:_ other interesting queries: is banana a fruit or berry, when was the louisiana purchase



\paragraph{Annotation Issues}
\label{sec:annotation}

% this is not a failure of the GR system but of the dataset. Although GR is evaluated on such dataset, so implicitly it influences it. Maybe move out to discussion or limitations? or idk?


The reliability of GR evaluation depends on the quality of \gs{GT} annotations in benchmark datasets. When those contain annotation errors, such as relevant passages incorrectly labeled as negative or vice versa, the measured performance metrics no longer accurately reflect a system's true retrieval capabilities. These can lead to misleading conclusions being published that do not generalize to real-world scenarios or accurately represent comparisons.

For QA tasks, multiple passages may contain semantically equivalent answers but only a subset are annotated as ground truth. The authors of MS MARCO, TriviaQA, and NQ each acknowledge this concern \cite{bajaj2018msmarcohumangenerated, joshi-etal-2017-triviaqa,kwiatkowski-etal-2019-natural}.

%ideas for improvements:  try it on impartial datasets. what is even an impartial dataset? what is even the point of these systems, do they have to be impartial? do they have to be generalized? do they have to be only good at Wikipedia QA? or just QA with clear metadata and clear annotation?

\textbf{TODO BELOW EITHER REWRITE OR IDK or maybe add the tables as appendix or idk. this below isnt really relevant to actual failure modes, similarly how most correl analysis isnt relevant either.}

During our analysis, we identified such cases where the annotation can cause misleading evaluation results. Here we present 3 such examples:
\textbf{Example 1: ``which apostle had a thorn in his side'' (answer: Paul).} The ground-truth passage %the only \gs{GT}passage?
is from the Wikipedia article ``Thorn in the flesh,'' which explains that Paul the Apostle used this phrase in 2 Corinthians 12:7--9. \gs{SEAL}'s %why report exactly \gs{SEAL} its outdated
 top result %who cares about top-1 result, why not report top-k?
was from Galatians 3, which mentions ``Paul the Apostle'' as the author of that epistle but contains nothing about a thorn. Even though \gs{SEAL} seemed to be able to identify ``Paul'' as a relevant term, potentially from BART's parametric knowledge, Galatians 3 is the wrong chapter entirely. \gs{SEAL} matched on generic terms (``Paul,'' ``apostle'') but never generated n-grams containing ``thorn.'' %never, not even in the top-k?

\textbf{Example 2: ``how many judges currently serve on the supreme court'' (answer: nine).} The ground-truth passage %which \gs{GT}passage?
 directly states the current composition of the Court. \gs{SEAL}'s %why report exactly \gs{SEAL} its outdated
  top result is a historical passage stating that in 1869, ``the Circuit Judges Act returned the number of justices to nine, where it has since remained.'' This is borderline, because the reader could infer the answer, but the passage is not specifically about the current number of judges.

\textbf{Example 3: ``who is the youngest judge currently sitting on the u.s. supreme court'' (answer: Neil Gorsuch).} The ground-truth passage discusses Gorsuch's nomination process and him being the youngest sitting justice. \gs{SEAL}'s top result %who cares about top-1 result, why not report top-k?. and why\gs{SEAL}?
directly states: ``Neil Gorsuch is the youngest justice sitting, at 50 years of age.'' This is a success, because \gs{SEAL} found a passage that answers the question, even if not annotated.




These examples show that these cases are a mix of such cases. To determine how many fall into each category, in section~\ref{sec:annot}, we apply LLM-as-a-judge to classify whether each retrieved passage genuinely answers the question or not. % why run in on \gs{SEAL} and not on the whole NQ dataset? or is this good?

\textbf{TODO FROM HERE}
If most of the 349 cases are genuine retrievals, then \gs{SEAL}'s effective top-10 recall is closer to 81.5\% rather than 76.1\%. If most are coincidental matches, then 76.1\% remains the accurate figure. Similar calculations can be done for \gs{MINDER}.

For each case, we present a large language model (\texttt{claude-sonnet-4-5\-20250929}) with the original question, the expected answer, and the retrieved passage text, asking it to determine whether the passage genuinely answers the question or whether the answer string's presence is coincidental. The model was chosen based on expected reasoning capabilities and cost of operation.

The judge classifies each case into one of three categories:
\begin{itemize}
    \item \textbf{YES}: The passage directly and unambiguously answers the question.
    \item \textbf{NO}: The answer string appears coincidentally and the passage does not answer the question.
    \item \textbf{PARTIAL}: The passage provides relevant information from which the answer could be inferred, but does not state it directly.
\end{itemize}

\paragraph{Overfitting - Underfitting}
\textbf{UNVALIDATED}
If the model is trained through too few epochs, it can underfit and retrieval can be noisy. If it it trained on too many, the model memorizes the training set too well and loses the ability to generalize to new queries. This is especially problematic if the model is expected to ``zero-shot'' retrieve % TODO WORKING
or when the model did not see the documents beforehand %arent these the same thing?
%todo: rethink how batch sizes, max-updates and epochs are to each other and how to calculate amount of epochs for this specific.

%can overfitting happen also on queries like if the model only learned one type of query style then it won't know what to do if the user phrases a query in a different or a new way? or different types of queries?

% does performance matter on the specific prompts and prefixes used during training and during generation?

\paragraph{Model capacity}
\textbf{UNVALIDATED}
\label{sec:model-capacity}

Smaller models might not have capacity to memorize enough docids.

\paragraph{Length Bias} %todo is my analysis already done relevant?
\textbf{UNVALIDATED}
\label{sec:length-bias}
The model might prefer short, common ngrams vs specific, long identifiers.

\paragraph{Latency}
\textbf{UNVALIDATED}
\label{sec:latency}
Beam search with constraints is probably significantly slower than dot-product (or similar) searches %SOURCE AND WHAT IS BEING DONE AGAINST THIS ISSUE ?




\paragraph{Vocabulary - Tokenization}
\textbf{UNVALIDATED}
\label{sec:vocab} %todo wording

\subparagraph{Subword} %todo wording
\textbf{UNVALIDATED}

If an important identifier (e.g. a proper noun like Lin'an from the TODO example) is split into many meaningless subwords, the model might struggle to generate it consistently compared to common words. This can happen even through BART uses BPE \cite{TODO}, which allows it to to handle ``out-of-vocabulary'' words by breaking them down into known subunits. %note: does index.py in \gs{SEAL} operate on tokenIDs or subwords? arent they the same here?

\subparagraph{Coverage} %todo wording
\textbf{UNVALIDATED}
If BART doesn't have good representations for domain-specific jargon, e.g. for rare medical terms, retrieval can fail. 


\paragraph{Zero-Shot Retrieval}
\textbf{UNVALIDATED}
\label{sec:zero-shot}
Supervised GR models usually perform significantly worse on datasets they weren't explicitly trained on \cite{liu2024robustnessgenerativeinformationretrieval, zhang2025replicationexplorationgenerativeretrieval}.
Since they usually lack robust matching capabilities like that of BM25 or Contriever \cite{TODO} in zero-shot settings because they tend to overfit the training query distribution %source? or did I make that up?

\paragraph{Unseen Documents}
\textbf{UNVALIDATED}
\label{sec:unseen-docs}

GR by default %what does by default mean
cannot retrieve documents added post-training %cannot? Are there methods or systems that make it able?
. While the probability is not infinite, as documents added ex-post still get indexed by the FM-index, they are not represented in the training distribution %source

Othertimes, a full retraining is required to index new information. %source

\paragraph{Sampling Failures}
\textbf{UNVALIDATED}
\label{sec:sampling-fail}
If the model is trained on irrelevant substring, it confuses the model's query-document mapping %source
%depending on how the sampling is done, either random or via a heuristic to prefer ngrams that share words with query, if the sampler picks an irrelevant substring, the model can technically be trained to map a wrong string to the query.
% however using more epochs can make it still work statistically if in most cases (?) it find a relevant sample 
% SOURCE?!!
% also it might be relevant that BART/T5 does have semantic knowledge from parametric knowledge and knows what each word mean usually SOURCE

%are valid identifiers treated as false negatives if they are not sampled targets or no?
% is there one sampled target per epoch (or whatever metric) or multiple? why not the other way?

% what happens if the sampled ngram is common phrase (e.g. "the history of" "it was crucial for the")

\subsection{Generative Failures}
\label{sec:gen-fail}
Using the identified failure modes in Table \ref{tab:failure-summary}, we present the analysis results pertaining to generation below.

\paragraph{Low Lexical Specificity}
\label{sec:failure-low-lex-spec}

%the Scoring Function fails to penalize high-frequency terms enough relative to the model's confidence. The model gets "stuck" in a local optimum of generating generic words  which technically appear in the document but are useless for passage-level retrieval. isnt this an optimization failure where the model generatessafe, high-freq nonsense instead of discriminative identifiers. which I guess is ok for document-level retrieval but not for passage-level, WHICH WOULD BE THE MAIN GOAL OF Q&A

N-gram corpus frequency reflects how distinctive a generated n-gram is. Rare n-grams provide stronger evidence for specific documents, while common n-grams match many topically-related passages. We analyze the average corpus frequency of the top-scored n-grams to determine whether the frequency (and thus score) of n-grams contribute to retrieval outcomes.

Across 6,515 queries, the average mean corpus frequency of the top-5 scored n-grams for the top-1 retrieved passage is 28753 for \gs{SEAL} and 21310 for \gs{MINDER}, while the same value using all-ngrams for such passage is 382,136 and 497,399, respectively. $k=5$ was arbitrarily selected as a representation of the n-grams with the strongest influence on the overall score of each passage.

Table \ref{tab:nonspec-decile-freq} summarizes the results. The decimals of the frequencies have been truncated for readability. 

\begin{table}[htbp]
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{Hits@$k$ success rate by n-gram frequency deciles.}
    \label{tab:nonspec-decile-freq}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\gs{SEAL}}} & \multicolumn{2}{c}{\textbf{\gs{MINDER}}} \\
        \textbf{Decile} & \textbf{Freq. Range} & \textbf{H@1 / H@10} & \textbf{Freq. Range} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 2.0--91          & 71.0\% / 92.8\% & 1--16          & 76.5\% / 93.9\% \\
        D2  & 91--356        & 51.5\% / 85.1\% & 16--55         & 65.9\% / 89.4\% \\
        D5  & 2,940--5,698   & 43.0\% / 76.3\% & 513--1,359     & 45.8\% / 80.2\% \\
        D8  & 18,581--35,824 & 31.4\% / 70.4\% & 7,434--16,582  & 36.7\% / 72.1\% \\
        D10 & 81,049--719,139& 25.9\% / 65.8\% & 44,813--1.7M     & 26.8\% / 60.7\% \\
        \bottomrule
    \end{tabular}
\end{table}

We can see that in D1, where the top-1 passage's top-5 scored n-grams average frequency is low, both hits@1 and hits@10 is surprisingly high for both systems, with \gs{MINDER} performing marginally better, while in D10, where the same average frequency is considerably high (implying generic n-grams), the retrieval performance drops significantly for both systems. The Spearmanâ€™s $\rho$ between such frequency and hits@1 is $-0.238$ and $-0.290$ respectively, both being $p < 0.001$, indicating that average n-gram frequency is inversely correlated with retrieval success.

Notably, using average frequency across all n-grams (instead of top-5) shows no significant correlation with performance (\gs{SEAL}: $\rho=0.029,\space p=0.02$; \gs{MINDER}: $\rho=-0.008,\space p=0.51$), suggesting that the presence of highly-ranked generic terms, rather than overall genericity, drives performance degradation. This happens because such high-frequency n-grams match many topically-related passages, diluting discriminative power among passages. While \gs{SEAL}'s scoring function theoretically downweights common n-grams through the $P(n)$ term in Equation~\ref{eq:ngram-score}, this analysis reveals that high-frequency n-grams can still dominate the top-ranked positions. \gs{MINDER} does not penalize high frequency n-grams. Instead, it artifically boosts the score of pseudoqueries \cite{li2023multiviewidentifiersenhancedgenerative}.

\paragraph{Preference for Unigrams}
\label{sec:unigrams}

% isnt this a failure of autoreg beam seach? bc the model greedily selects high-probability short tokens  because they are "safer" predictions than long, specific phrases? idk maybe the architecture inherently penalizes specificity in favor of high-frequency short tokens.


Here we examine whether the analyzed systems disproportionately generate short n-grams (unigrams and bigrams) rather than longer, more distinctive multi-word sequences. Longer n-grams generally provide stronger discriminative power, as multi-word phrases are far less likely to appear across multiple passages than individual words.

We quantify this generation bias by measuring the fraction of generated n-grams that consist of a single token in the top-1 retrieved passage. Across 6,515 queries, the mean unigram proportion is $0.869 \pm 0.066$ (median 0.878) for \gs{SEAL} and $0.853 \pm 0.071$ (median 0.863) for \gs{MINDER}. The average n-gram length is $1.28 \pm 0.18$ words (median 1.24) for \gs{SEAL}, and $1.48 \pm 0.39$ (median 1.35) for \gs{MINDER}. This indicates that, on average, more than four-fifths of generated n-grams are unigrams for both systems. \gs{MINDER} shows slightly higher variance in length ($\sigma=0.39$ vs. $0.18$).

We categorize queries into deciles based on the unigram proportion. Queries in D1 (lowest unigram proportion) achieve considerably higher hit rates than those in D10, as evidenced by Table \ref{tab:unigram-dec}.


\begin{table}[htbp]
    \centering
    \caption{Hits@$k$ success rate by unigram proportion deciles.}
    \label{tab:unigram-dec}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\gs{SEAL}}} & \multicolumn{2}{c}{\textbf{\gs{MINDER}}} \\
        \textbf{Decile} & \textbf{Div. Range} & \textbf{H@1 / H@10} & \textbf{Div. Range} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 0.51--0.78 & 67.7\% / 91.2\% & 0.48--0.76 & 73.4\% / 92.8\% \\
        D2  & 0.78--0.82 & 61.6\% / 87.1\% & 0.76--0.80 & 69.4\% / 92.2\% \\
        D3  & 0.82--0.84 & 48.8\% / 82.5\% & 0.80--0.83 & 56.8\% / 85.6\% \\
        D5  & 0.86--0.88 & 44.2\% / 81.0\% & 0.85--0.86 & 47.8\% / 81.9\% \\
        D8  & 0.91--0.92 & 28.4\% / 69.9\% & 0.89--0.91 & 31.8\% / 73.7\% \\
        D10 & 0.94--1.00 & 22.6\% / 60.5\% & 0.93--1.00 & 21.2\% / 55.9\% \\
        \bottomrule
    \end{tabular}
\end{table}

The results are interesting for \gs{MINDER} specifically, as \gs{MINDER} is able to use its generated pseudoqueries (which tend to be longer) as n-grams. This correlates with the results presented below at Section \ref{sec:pseudo-minder-contrib}, indicating that pseudoqueries appearing in n-grams provides the retrieval model with a higher hitrate. \gs{MINDER} also shows a higher hit rate at lower unigram proportion deciles.

The Spearman $\rho$ between unigram proportion and hits@k for \gs{SEAL} is $\rho - 0.278$ for $k=1$ and  $\rho - 0.220$ for $k=10$, while for \gs{MINDER} is $\rho - 0.330$ for $k=1$ and  $\rho - 0.260$ for $k=10$. Each value is statistically significant at $p<0.001$.

To understand the results more illustratively, we summarized the table in the following chart, clearly showing \gs{MINDER}'s marginally higher success rate at low unigram proportions. The hit rates converge at higher unigram proportions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/ngram_length_bias_comparison.png}
\end{figure}


\paragraph{Query-to-n-gram Overlap}
\label{sec:query-ngram-overlap}

Here we measure the lexical alignment between query terms and the n-grams. We use \texttt{GPT2TokenizerFast} for this. High overlap indicates that the model successfully generated n-grams containing query-relevant terms, while low overlap suggests a mismatch where generated n-grams, though potentially topically related, do not directly address the query's specific terms. We quantify this using the fraction of query tokens that appear in any generated n-gram in the highest-scored retrieved passage, denoted by $C(Q, G_n)$, where $Q$ denotes the query tokens and $G_n$ denotes the tokens of the generated n-grams.

Our analysis across 6,515 queries shows that the token-level lexical overlap varies substantially across the top-1 retrieved passages. As visible in Table~\ref{tab:p-unique-coverage}, there is a positive correlation between lexican overlap and retrieval success. For \gs{SEAL}, $\rho = 0.323$ for  $k=1$ and $\rho = 0.190 $ for  $k=10$. For \gs{MINDER}, the Spearman correlations are $\rho = 0.380$ and $\rho = 0.202$, respectively. All values are statistically significant at $p<0.001$. 

\begin{table}[H]
    \centering
    \caption{Hits@$k$ by $C(Q, G_n)$ deciles.}
    \label{tab:p-unique-coverage}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\gs{SEAL}}} & \multicolumn{2}{c}{\textbf{\gs{MINDER}}} \\
        \textbf{Decile} & \textbf{$C(Q, G_n)$} & \textbf{H@1 / H@10} & \textbf{$C(Q, G_n)$} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 0.00--0.27 & 12.4\% / 55.1\% & 0.00--0.36 & 14.4\% / 61.5\% \\
        D2  & 0.29--0.36 & 24.5\% / 70.2\% & 0.37--0.44 & 25.7\% / 67.9\% \\
        D3  & 0.36--0.40 & 29.6\% / 74.5\% & 0.45--0.50 & 35.1\% / 71.2\% \\
        D5  & 0.46--0.50 & 42.3\% / 76.9\% & 0.58--0.63 & 44.2\% / 79.7\% \\
        D8  & 0.61--0.67 & 53.4\% / 78.1\% & 0.76--0.80 & 62.8\% / 85.7\% \\
        D10 & 0.73--1.00 & 70.0\% / 88.4\% & 0.90--1.00 & 77.4\% / 88.6\% \\
        \bottomrule
    \end{tabular}
\end{table}

As visible from Table~\ref{tab:p-unique-coverage}, the hit rate of \gs{SEAL} and \gs{MINDER} are relatively similar, and are positively correlated with $C(Q, G_n)$. Similar to other failure modes, low query coverage typically arises when the model generates semantically related but lexically distinct n-grams (e.g., for a query containing ``automobile,'' the model could generate n-grams with ``car'') or when the model generates generic topical n-grams that lack the specific query terms needed to isolate the answer (e.g., a query about ``Victorian architecture in London'' generates only ``architecture'' and ``London'').


\paragraph{Answer as N-gram}
\label{sec:answer-ngram}

% do the models have access to the answer string beforehand? does the model works best when it memorizes (generates the answer string) rather than generalizes (generates context)? need to reread the paper again..
% how would the model react to trivia QA? isnt in triviaqa the answer string being in the passage is an inherent indicator for being annotated\gs{GT}? need to reread paper again...
We analyze whether retrieval success correlates with the model's ability to generate the specific answer string as an identifier, or whether it relies on the surrounding context. While generating the answer string provides a valuable anchor for a passage, the primary goal of \gs{GR} is document identification, not direct question answering.

Table \ref{tab:answer-presence-impact} illustrates the results of our analysis. We looked at whether among the top-1 and top-10 scored retrieved passages, at least one of them contained the answer string or not and what the Hits@1 value is for these cases. The results are mostly similar between \gs{SEAL} and \gs{MINDER}, with most top-1 ranked retrieved passages not containing the answer string, yet when they do, the respective query's Hits@1 rate doubles and Hits@10 increases significantly.

\begin{table}[htbp]
    \centering
    \caption{Performance metrics conditioned on answer string presence.}
    \label{tab:answer-presence-impact}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\gs{SEAL}}} & \multicolumn{2}{c}{\textbf{\gs{MINDER}}} \\
        \textbf{Metric} & \textbf{Pres.} & \textbf{Abs.} & \textbf{Pres.} & \textbf{Abs.} \\
        \midrule
        Queries ($N$) & 1,096  & 5,419  & 1,457  & 5,058  \\
        $\bar{\text{Hits@1}}$        & 75.0\% & 34.9\% & 75.4\% & 38.2\% \\
        $\bar{\text{Hits@10}}$       & 93.5\% & 72.6\% & 93.0\% & 74.3\% \\
        \bottomrule
    \end{tabular}
\end{table}

We can see that in cases where the answer string is identified among the n-grams, the success rate jumps significantly for both systems. This indicates that direct identification is a strong predictor of retrieval success. Both \gs{SEAL} and \gs{MINDER} are able to retrieve (at least) one correct passage in around ~35\% of cases at the first place and ~73\% of cases among the top-10 retrieved, suggesting that the model is able to identify the semantic environment in which an answer is likely to be in relatively convincingly. That said, the performance gap between the two cases indicates that when the model fails to generate the specific answer as an identifier, the remaining contextual cues are oftentimes too generic to distinguish the correct passage from topically similar alternatives.


\subsection{Scoring Failures}

Just like in Section \ref{sec:gen-fail}, we present the analysis results of the identified failure modes (Table \ref{tab:failure-summary}) pertaining to scoring below.



\paragraph{Overreliance on a Single Identifier}
\label{sec:failure-6}

% its less about bad queries relying on one big score. Are\gs{SEAL}/\gs{MINDER} are designed to work byaggregating weak signals or is this not an issue? When it relies on a single n-gram, it breaks the design philosophy. or does it? is this even a failure mode?

Overreliance on a single identifier measures the extent to which the top-1 ranked passage's total score is concentrated in one high-scoring n-gram versus distributed across multiple n-grams. We define this as the ratio of the highest scored n-gram score and the sum of all matched n-gram scores for the passage.

Our analysis across 6,515 queries reveals that lower such ratio correlates with higher retrieval success, as shown in Table~\ref{tab:identifier-dominance}. For \gs{SEAL}, we observe a Spearman correlation of $\rho = -0.115$ ($p < 0.001$), with success rates declining from 48.6\% in the lowest decile (D1) to 29.6\% in the highest decile (D10).

\begin{table}[htbp]
    \centering
    \caption{Hits@1 by top n-gram ratio deciles (max score / sum scores).}
    \label{tab:identifier-dominance}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\gs{SEAL}} (Mean: 0.44)} & \multicolumn{2}{c}{\textbf{\gs{MINDER}} (Mean: 0.36)} \\
        \textbf{Decile} & \textbf{Range} & \textbf{Hits@1} & \textbf{Range} & \textbf{Hits@1} \\
        \midrule
        D1  & 0.12--0.30 & 48.6\% & 0.12--0.25 & 58.0\% \\
        D2  & 0.30--0.35 & 45.8\% & 0.25--0.28 & 59.3\% \\
        D3  & 0.35--0.38 & 45.4\% & 0.28--0.31 & 55.8\% \\
        D5  & 0.41--0.44 & 44.3\% & 0.33--0.35 & 45.1\% \\
        D8  & 0.50--0.53 & 35.4\% & 0.40--0.43 & 40.2\% \\
        D10 & 0.57--0.85 & 29.6\% & 0.48--0.94 & 32.5\% \\
        \bottomrule
    \end{tabular}
\end{table}

This trend is also noticable in \gs{MINDER} ($\rho = -0.169$, $p < 0.001$), which also exhibits a lower mean ratio of 0.36 compared to \gs{SEAL}'s 0.44 and a higher hit rate in all deciles. This shift toward more distributed evidence is assumed to be a consequence of \gs{MINDER}'s multiview identifiers, since by aggregating scores from titles, pseudo-queries, and substrings, \gs{MINDER} theoretically reduces reliance on any single identifier.

This finding aligns with the additive scoring design, which benefits from distributed evidence by providing redundant confirmation across multiple n-grams. %TODO SOURCE
Conversely, if the top-scoring n-gram matches many passages or identifies the wrong one, the remaining low-scoring n-grams may provide insufficient corrective evidence to ensure correct ranking.



\paragraph{Pseudo-Query Contribution in\gs{MINDER}}
\label{sec:pseudo-minder-contrib}
Here we analyze the contribution of synthetic identifiers in the \gs{MINDER} framework. As detailed in Section \ref{sec:minderarch}, \gs{MINDER} incorporates pseudo-queries alongside titles and substrings to capture high-level semantic information. Our analysis of 6,515 queries reveals an interesting divergence between the quantity of generated pseudo-queries and their influence on the final ranking.

Numerically, pseudo-queries constitute a negligible fraction of the generated identifiers, accounting for only 0.32\% of the total n-grams (87714 out of 26.9 million). However, their contribution to the total accumulated score is disproportionately high, representing 4.52\% of the total score mass (14.8 million out of 328.5 million). This confirms that \gs{MINDER}'s scoring mechanism successfully uses sparse but highly discriminative synthetic identifiers. 

Interestingly, titles still dominate the picture the scoring disproportionately, as only 1.08\% of ngrams are titles yet contribute 23.2\% of the total score. If we combine pseudoqueries and titles, the resulting numbers are 1.4\% and 27.72\%, respectively.

\section{Discussion}

\textbf{TODO}

\subsection{Key Findings}

\textbf{TODO}

\subsection{Generalizability} %of method and findings

\textbf{TODO}

\section{Conclusion} % incl future work

\textbf{TODO}

\section*{AI Use Disclosure}
Some portions of code for this thesis were generated with the assistance of \textit{Claude by Antropic} and \textit{ChatGPT by OpenAI} large language models. All AI-generated code was reviewed, edited, and verified by the author.


\clearpage
\appendix

\section{LLM-as-Judge prompt}

The prompt used for the LLM-as-Ludge component in Section \ref{sec:annot} is pasted below:

\lstset{breaklines=true}
\begin{lstlisting}
Question: [QUESTION]
Expected answer: [ANSWER]
Retrieved passage title: [PASSAGE TITLE]
Retrieved passage text: [PASSAGE TEXT]

Does this passage answer the question?
- YES: The passage directly answers the question.
- NO: The answer string appears coincidentally but does not answer the question.
- PARTIAL: Contains relevant information but requires inference.

The answer explanation should not exceed 100 words. 
\end{lstlisting}