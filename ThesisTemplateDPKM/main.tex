
\newcommand{\adrian}[1]{\textcolor{orange}{#1}}

\section{Introduction}

\glsxtrfull{gr} represents an emerging framework in \glsxtrfull{ir} that aims to consolidate retrieval processes into end-to-end autoregressive models, uses a language model instead of indices to encode the entire document corpus into the parameters of a single transformer. While these systems offer promising advantages over traditional approaches, they also exhibit distinct failure modes, behavioral biases, and structural limitations. Unlike traditional approaches that rely on separate vector index structures for document embeddings and multi-stage pipelines, generative retrieval systems directly generate document identifiers through autoregressive language models, consolidating the retrieval process into a single end-to-end model \cite{kuo2024surveygenerativeinformationretrieval, li2025matchinggenerationsurveygenerative, zhang2024generativeretrievaltermset}. This offers several compelling advantages, such as eliminating the need for dense vector indexes and their associated \glsxtrfull{ANN} search structures, reducing memory footprint and enabling the model to learn document representations jointly with the retrieval objective \cite{sun2023learningtokenizegenerativeretrieval, tang2024listwisegenerativeretrievalmodels}.

Despite these promising characteristics, generative retrieval systems exhibit distinct failure modes that differ fundamentally from those observed in traditional retrieval architectures. While dense retrieval failures typically manifest as low similarity scores in vector space, generative retrieval failures can include hallucination of non-existent document identifiers, incorrect pruning during autoregressive generation, poor performance on dynamically expanding corpora, and challenges with scalability \cite{Metzler_2021, Ji_2023, zeng2024planningaheadgenerativeretrieval, pradeep2023doesgenerativeretrievalscale}. It is also worth noting that some \glsxtrshort{gr} implementations employ auxiliary data structures for constrained generation, making them not fully end-to-end differentiable.\textbf.

Existing literature on \glsxtrshort{gr} documents various challenges and failure cases. However, these failures are typically discussed in isolation with individual paper proposing new methods or architectures. There is no systematic taxonomy that classifies the types of failures that occur in generative retrieval systems, their relative frequencies in practice, or their underlying causes. %TODO am I even analyzing frequency?

This thesis addresses this gap by developing a taxonomy of failures, biases, and limitations for \glsxtrshort{gr} systems by synthesizing documented failure cases from the literature and conducting analysis of retrieval outputs from the \glsxtrshort{SEAL} and \glsxtrshort{MINDER} models \cite{bevilacqua2022autoregressivesearchenginesgenerating, li2023multiviewidentifiersenhancedgenerative} on the \glsxtrfull{nq} dataset. The \glsxtrshort{nq} dataset \cite{kwiatkowski-etal-2019-natural} is a benchmark of Google search queries paired with full Wikipedia pages, annotated with the exact section and phrases of the page containing the answer.


\subsection{Research Question}

This thesis addresses two interconnected research questions:

\textbf{RQ1:} What is a systematic classification of the failures, behavioral biases, and structural limitations observed in generative retrieval systems?

\textbf{RQ2:} What are the most frequent failure modes observed in \glsxtrshort{SEAL}'s and \glsxtrshort{MINDER}'s n-gram-based retrieval, and which failures stem from the generation component versus the scoring mechanism?

\subsection{Contributions}

This thesis makes the following contributions:

\begin{enumerate}
    \item A synthesis of failure modes documented across generative retrieval research, organized into five categories: inference and decoding failures, memorization and representation issues, scalability limitations, dynamic corpus problems, and architectural constraints.

    \item A mapping from abstract failure modes to observable indicators measurable from inference outputs.

    \item Quantitative characterization of failure patterns via the \glsxtrshort{SEAL} and \glsxtrshort{MINDER} framework, including correlation analysis between failure indicators and retrieval success.


\end{enumerate}
The code used for this thesis is available on \href{https://github.com/richietk/thesis}{Github}.


\pagebreak

\section{Background}

In this section we introduce the concept of \glsxtrfull{gr} and discuss the important details of the \glsxtrshort{SEAL} and \glsxtrshort{MINDER} architecture.

\subsection{Information Retrieval}

Traditional \glsxtrfull{ir} has long been structured around the ``index-retrieve-then-rank'' pipeline \cite{10.1145/3589335.3641239, Metzler_2021}. Sparse retrieval methods, such as BM25, rely on lexical representations but suffer when terms do not overlap exactly \cite{Choi_2022, biswas2024efficientinterpretableinformationretrieval, gao2021coilrevisitexactlexical}. Dense retrieval (DR) methods using dual-encoder architectures (e.g., DPR \cite{karpukhin2020densepassageretrievalopendomain}) address this by encoding queries and documents into semantic vector representations. However, DR systems still depend on separate vector index structures and Approximate Nearest Neighbor (ANN) indexes \cite{xiong2020approximatenearestneighbornegative, li2023constructingtreebasedindexefficient}, and queries and documents are encoded independently with limited semantic interaction \cite{ni2021largedualencodersgeneralizable}. %todo is that a "bad thing"?

(\glsxtrshort{gr}) diverges from traditional architectures by aiming to directly generate document identifiers through autoregressive language models, consolidating the retrieval process into a single end-to-end model \cite{zhang2024generativeretrievaltermset, li2025matchinggenerationsurveygenerative, kuo2024surveygenerativeinformationretrieval}. While this aspirational goal is not always fully achieved in practice, as many systems introduce additional components, the unified architecture still offers several advantages when realized: it eliminates the need for separate dense vector indexes, reduces memory footprint significantly, and enables the optimization of document representations and retrieval objectives %todo too vague?
\cite{bevilacqua2022autoregressivesearchenginesgenerating, zeng2024planningaheadgenerativeretrieval, tang2024listwisegenerativeretrievalmodels}. Systems like \glsxtrshort{SEAL} achieves 7x less storage than DPR while showing comparable accuracy@k and exact match metrics on the \glsxtrfull{nq} 320k dataset \cite{bevilacqua2022autoregressivesearchenginesgenerating}. Another \glsxtrshort{gr} system, \glsxtrshort{PAG} \cite{zeng2024planningaheadgenerativeretrieval} required approximately 7x less memory to index the entire corpus compared to single-vector dense retrieval models and comparable memory needs to BM25. It achieved comparable or better MRR@10 results on the MS-MARCO Dev set compared to sparse and dense retrieval models. Notably, PAG achieved a 70.1\% improvement over BM25 and an 11.2\% improvement over TAS-B . While some \glsxtrshort{gr} implementations employ auxiliary data structures for constrained generation (such as \glsxtrshort{SEAL}'s use of the FM-Index), these differ fundamentally from the separate encoding and indexing stages required by traditional systems. Additionally, the pre-trained language models used in \glsxtrshort{gr} can perform effectively in zero- and few-shot scenarios when limited labeled data is available \cite{Metzler_2021}.

\subsection{DocID Design}

\glsxtrfull{docid} design is the one of the key decisive factors for retrieval effectiveness, as it replaces traditional external indices with the model’s internal parametric memory. %todo rewrite for clarity
A \glsxtrshort{docid} serves as a unique ``index'' that the model must autoregressively generate to point toward a specific document or passage. Initial foundational works categorized these identifiers into three main types: unstructured atomic identifiers (such as unique random integers), naively structured string identifiers (such as titles), and semantically structured, numeric identifiers (such hierarchical cluster paths). While atomic IDs require the model to memorize random associations, semantic IDs group similar documents together under shared prefix tokens, improving retrieval quality \cite{pradeep2023doesgenerativeretrievalscale}.

String identifiers use natural language such as document titles, n-grams, URLs, or synthetic pseudo-queries, which allow the model to use its pre-trained linguistic knowledge rather than learning a new symbolic system from scratch. Substring or n-gram identifiers allow any span of text within a document to act as its identifier. In \glsxtrshort{SEAL} and \glsxtrshort{MINDER}, the documents aren't assigned a single identifier per document, unlike in other frameworks such as DSI \cite{tay2022transformermemorydifferentiablesearch} or GENRE \cite{sun2023learningtokenizegenerativeretrieval}. A single document can have multiple possible identifiers, as evidenced in \glsxtrshort{SEAL}, which treats all n-grams within the passage as valid identifiers. Additionally, \glsxtrshort{MINDER} also generates pseudo-queries for each document, %note: for each document or passage?
which it can occasionally use as ``n-grams''. While string-based IDs are more interpretable and generalizable to dynamic corpora where document collections expand, they often suffer from ``false pruning'', where the sequence-based generation discards a relevant document early in the beam search if the model fails to predict the correct prefix token. We will focus on string-based \glspl{docid} in this thesis.

Other \glspl{docid} types include semantically structured identifiers are designed to capture the relationships between documents using hierarchical categorization methods, such as by recursively applying k-means clustering to document embeddings, creating a tree where each root-to-leaf path serves as a DocID. Because they share prefixes with similar documents, they facilitate semantic matching, but their sequential nature makes them, just like string-based \glspl{docid}, susceptible to false pruning. Unstructured atomic identifiers treat each document as a single, unique token added directly to the model's vocabulary. Unlike sequential designs, retrieval involves only a single decoding step, where the model sorts the logits of all \glsxtrshort{docid} tokens to produce a ranked list. While this makes them immune to false pruning and very FLOP efficient during inference, they require a large increase in model parameters as the corpus scales \cite{pradeep2023doesgenerativeretrievalscale}. %This is because atomic \glspl{docid} predict the document in a single softmax, the output layer must contain one set of weights per document, causing the parameter count to grow with the collection size \cite{nguyen2023generativeretrievaldenseretrieval, kuo2024surveygenerativeinformationretrieval}.
Because they are often randomly initialized, they lack inherent semantic connections to the text, forcing the model to rely entirely on memorizing the association between document content and its identifier during the training phase.

\subsection{SEAL Architecture}

\glsxtrfull{SEAL} \cite{bevilacqua2022autoregressivesearchenginesgenerating}, addresses \glsxtrshort{docid} design challenges by using existing substrings as its retrieval targets. Such challenges specifically addressed are the unavailability of unique metadata (like titles) for all documents, the insufficient granularity of page- and document-level identifiers, as well as the need to force a structure onto the search space, such as hierarchical cluster trees, which can be difficult to construct for large-scale benchmarks. %todo recheck why are these specifically adressed/improved with ngrams
\glsxtrshort{SEAL} chunks documents into passages and defines identifiers as any n-gram within those passages. An BART model is trained to generate distinctive, query-relevant n-grams that identify documents containing them. \glsxtrshort{SEAL} constrains generation during decoding so that the model only generates n-grams actually existing in the corpus. For ranking, \glsxtrshort{SEAL} aggregates information from multiple generated n-grams while downweighing overlaps if an n-gram overlaps with a higher-scoring one already selected. This is implemented so that the final score does not overscore based on repetitive or redundant document content.

\paragraph{Retrieval Mechanism}

During the generation phase, \glsxtrshort{SEAL} enforces that at each decoding step, the language model can only generate tokens that would extend the current partial n-gram into a sequence that exists somewhere in the corpus. As an example, when \glsxtrshort{SEAL} has generated the partial sequence ``earthquakes can be,'', ``earthquakes can be predicted'' can only be generated if this complete 4-gram exists as a contiguous substring in at least one document \cite{bevilacqua2022autoregressivesearchenginesgenerating}. ``earthquakes can be predicted'' exists in the corpus in the sentence ``discussing why the claim that earthquakes can be predicted is false.''

After n-gram generation, in the retrieval phase, for each n-gram, \glsxtrshort{SEAL} identifies every passage containing that n-gram and assign a relevance score based on the n-gram's uniqueness and frequency in the corpus.
Passages are then ranked by additively aggregating the scores from all n-grams they contain. The FM-index, which \glsxtrshort{SEAL} uses, provides a range of rows in the Burrows-Wheeler Transform matrix, and each row corresponds to an occurrence of that n-gram in the corpus. SEAL then maps these occurrences back to their respective document/passage id-s.This means that a document scores highly not because any single n-gram uniquely identifies it, but because it contains multiple high-scoring n-grams. This provides robustness, since even if some highly ranked n-grams point to an irrelevant document, other n-grams can still guide retrieval toward relevant documents. %todo recheck the part about FM / BWT for clarity.

This architecture helps explain why \glsxtrshort{SEAL} can overcome vocabulary mismatch issues. While the model cannot generate n-grams that do not exist in the corpus, it can find paraphrases and related terms as long as those appear in corpus documents using BART's parametric knowledge.

\paragraph{Scoring Mechanism}
\label{par:scormech}
\glsxtrshort{SEAL}'s failure modes are directly influenced by its scoring function. \glsxtrshort{SEAL} computes n-gram scores by combining the language model's conditional probability of an n-gram given a query with corpus frequency to favor distinctive n-grams.

The unconditional n-gram probability is computed from corpus statistics:
\begin{equation}
    P(n) = \frac{F(n, R)}{\sum_{d \in R} |d|}
    \label{eq:unconditional-prob}
\end{equation}
where $F(n, R)$ is the frequency of n-gram $n$ in corpus $R$, and $|d|$ is the length of document $d$.

The n-gram score combines conditional and unconditional probabilities:
\begin{equation}
    w(n, q) = \max\left(0, \log \frac{P(n|q)(1 - P(n))}{P(n)(1 - P(n|q))}\right)
    \label{eq:ngram-score}
\end{equation}

This formulation promotes n-grams that have high conditional probability given the query ($P(n|q)$), computed by BART, but low unconditional probability in the corpus ($P(n)$). This means that distinctive, query-relevant n-grams are prioritized.

It is important to highlight that $P(n|q)$ is computed autoregressively by BART and depends on both the n-gram $n$ and the query $q$. Consequently, the same n-gram string can receive different scores for different queries. Therefore n-gram scores are query-dependent.

Finally, the document score aggregates contributions from multiple non-overlapping n-grams:
\begin{equation}
    W(d, q) = \sum_{n \in K^{(d)}} w(n, q)^\alpha \cdot \text{cover}(n, K^{(d)})
    \label{eq:doc-score}
\end{equation}
where $K^{(d)}$ is the subset of generated n-grams $n$ matching document $d$. 
An n-gram is included in $K^{(d)}$ only if it has at least one occurrence 
in $d$ that does not positionally overlap with a higher-scoring n-gram's 
occurrence. The parameter $\alpha$ is a hyperparameter. The coverage weight $\text{cover}(n, K^{(d)})$ downweights individual n-grams whose tokens overlap with tokens of higher-scoring n-grams in the same document, preventing passages from receiving inflated scores when they contain many lexically similar n-grams.\cite{bevilacqua2022autoregressivesearchenginesgenerating}. %todo clear enough or should I expand?

Here it is worth noting that \glsxtrshort{SEAL}, generates fixed token-length ngrams (by default $k=10$, except for titles). However, they save all partially-decoded sequences from the beam-search \cite{bevilacqua2022autoregressivesearchenginesgenerating}. Because the system records this history, the final score of a passage is always the sum of strings of different lengths. As expected, n-gram length is strongly correlated with frequency ($\rho=-0.835, p<0.001$). Consequentially, 

To illustrate \glsxtrshort{SEAL}'s n-gram based scoring, lets examine the query ``who sings does he love me with reba''. The top-ranked passage (titled ``Linda Davis'') matched the following keys (excerpt):

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lrr}
        \toprule
        \textbf{N-gram} & \textbf{Corpus Freq. $F(n,R)$} & \textbf{Score $w(n,q)$} \\
        \midrule
        \texttt{</s> Linda Davis @@} & 9 & 216.33 \\
        \texttt{ "Does He Love} & 23 & 196.06 \\
        \texttt{ Reba} & 1,851 & 103.56 \\
        \texttt{ country singles} & 777 & 83.47 \\
        \texttt{ country music singer} & 4,024 & 39.20 \\
        \bottomrule
    \end{tabular}
    \caption{Example n-gram keys for query ``who sings does he love me with reba''. Lower corpus frequency correlates with higher scores for query-relevant n-grams.}
    \label{tab:ngram-example}
\end{table}

It is also worth noting that while the additive nature of the scoring mechanism could theoretically bias retrieval toward longer documents in a corpus of varying document lengths, this effect is neutralized by dividing $F(n, R)$ by the total number of tokens in the corpus $R$.

\subsection{MINDER Architecture}
\label{sec:minderarch}
The \glsxtrfull{MINDER} framework \cite{li2023multiviewidentifiersenhancedgenerative} extends the n-gram based approach of \glsxtrshort{SEAL} by introducing synthetic and multiview identifiers. \glsxtrshort{MINDER} is motivated by the observation that single-view identifiers, such as document titles or extractive n-grams, often lack the contextualized information necessary to satisfy complex queries, especially if those require rephrasing.

To address this, \glsxtrshort{MINDER} assigns three distinct ``views'' of identifiers to each passage:
\begin{enumerate}
     \item Title of the document
     \item N-grams, similar to \glsxtrshort{SEAL}
     \item Pseudo-queries, i.e. synthetic identifiers generated via a query-generation model that reflect potential queries
\end{enumerate}

The inclusion of pseudo-queries is intended to bridge the gap between user queries and document content. While \glsxtrshort{SEAL} is limited to the exact word orderings present in the corpus, \glsxtrshort{MINDER}'s thus allows the model to match queries against rephrased or summarized versions of the document via pseudo-queries.

\glsxtrshort{MINDER} relies on the conditional probability $P(i\allowbreak|q)$ of the identifier $i$ given query $q$, computed by the autoregressive language model. Because pseudo-queries are typically much longer than n-grams, their raw log-probabilities are naturally lower due to the multiplicative nature of autoregressive decoding. To ensure that pseudo-queries can compete with shorter identifiers, \glsxtrshort{MINDER} introduces a biased score to offset this length penalty. The final relevance score for a passage $p$ is an aggregation of the scores from all predicted identifiers that appear in that passage across all three views:
\begin{equation}
   S(q, p) = \sum_{i \in I_p} s(i, q)
   \label{eq:minder-aggregation}
\end{equation}

where $I_p$ is the set of identifiers generated by the model for passage $p$, and $s(i, q)$ is the language model score for identifier $i$.

To manage multiple identifier types, \glsxtrshort{MINDER} identifier-specific prefix tokens (e.g., \texttt{<TS>} for titles, \texttt{<QS>} for queries). During inference, the model is prompted with an identifier prefix and the query, and constrains the generation to valid sequences within the respective view.

\subsection{Failure Modes in Generative Retrieval} 
\label{sec:failure-modes-literature}

Despite these advantages, \glsxtrshort{gr} systems exhibit several distinct failure modes, behavioral biases, and structural limitations that motivate this thesis. We organize these into five categories: inference and decoding failures, memorization and representation issues, scalability limitations, dynamic corpus problems, and interpretability concerns.

\paragraph{Inference and Decoding}

Inference and decoding failures are primarily driven by the problem of prefix pruning, also referred to as false pruning. This occurs during autoregressive generation of \glspl{docid} when beam search prematurely discards the prefix of a relevant document because its cumulative probability at an early step is lower than competing candidates \cite{zhang2024generativeretrievaltermset, zeng2024planningaheadgenerativeretrieval, sun2023learningtokenizegenerativeretrieval}. Once a prefix is pruned, the system cannot recover the correct document regardless of subsequent token probabilities. This failure stems from the myopic nature of autoregressive decoding, in which the model perceives only preceding tokens and lacks visibility into subsequent tokens that might contribute to a high final relevance score. While the scale of this issue depends significantly on the model architecture, empirical analyses indicate that even with large beam sizes (e.g., 1000), constrained beam search often fails to match brute-force decoding performance, suggesting that relevant \glspl{docid} are frequently eliminated during search \cite{zeng2023scalableeffectivegenerativeinformation}.

The autoregressive nature of decoding inherently favors shorter n-grams over longer ones, drowning out highly specific and discriminative n-grams. Additionally, longer n-grams naturally introduce more chances of false pruning. These indicate that the global optimum itself could be a short or irrelevant prefix, making the model unable to accurately pinpoint correct passages by preferring the shortest valid \glsxtrshort{docid} path or the one with the most frequent initial tokens, regardless of their actual relevance to the query.

Related is the problem of hallucination, in which the model generates invalid identifiers. Constrained decoding used in \glsxtrshort{SEAL} and \glsxtrshort{MINDER} eliminates this failure mode from this analysis. Please note that constrained decoding also applies for \glsxtrshort{MINDER}'s pseudo-queries during decoding. During the initial generation of those pseudo-queries, rephrasing is intended and not a failure, so it is not considered hallucination.

\paragraph{Memorization and Representation}
One of the memorization is the apparent forgetting of fine-grained features. While generative models capture coarse-grained semantic clusters effectively, %source
they struggle to accurately memorize and decode fine-grained document distinctions. Experimental comparisons reveal that error rates %what error rates
for \glsxtrshort{gr} models increase significantly at later token positions in \glsxtrshort{docid} sequences, rising from approximately 1\% at the first position to over 12\% by the sixth position. This indicates degradation in memory accuracy for detailed document features \cite{yuan2024generativedenseretrievalmemory}.

\glsxtrshort{gr} systems also suffer from cases where the mapping from document content to identifiers is not sufficiently distinct. When using synthetic queries or semantic clustering as identifiers, multiple documents may share quasi-identical or highly similar representations, causing the model to conflate distinct documents \cite{yang2023autosearchindexerendtoend}. This is exacerbated by the fact that in many cases, long documents (e.g. articles) were used for memorization while short queries (e.g. chunks) are used for retrieval.

\glsxtrshort{docid} design is consistently identified as a core challenge affecting these representation issues. Numerical \glspl{docid} offer efficiency but suffer from limited generalization and overfitting to initial training sets, while string-based \glspl{docid} maintain better semantic alignment but are more susceptible to false pruning \cite{zhang2024generativeretrievaltermset, zhang2025replicationexplorationgenerativeretrieval, 10.1145/3589335.3641239}.

\paragraph{Scalability}
Scalability limitations become evident as corpus size increases, as \glsxtrshort{gr} models exhibit limitations in terms of capacity \cite{nguyen2023generativeretrievaldenseretrieval, zeng2023scalableeffectivegenerativeinformation}. For example, in some models the fixed parameter budget of the transformer becomes insufficient to encode nuances of a massive corpus. Empirical studies published by Pradeep et.al. (2023) %todo any way to have a function for this?
\cite{pradeep2023doesgenerativeretrievalscale} show sharp performance decline as corpora scale from 100,000 to 8.8 million passages. For example, a T5-XL model achieves only ~30\% of the MRR@10 score on the full MS MARCO corpus compared to the base corpus, which means it significantly underperforms compared to smaller subsets. 

Associated with scalability is the issue that oftentimes increasing model size does not yield proportional improvements. According to the on the aforementioned study \cite{pradeep2023doesgenerativeretrievalscale}, scaling T5 from XL (3B parameters) to XXL (11B parameters) degraded retrieval performance when using naive string identifiers on the full MS MARCO dataset, %what is "naive" here and is it relevant?
despite identical training configurations. Additionally, atomic identifiers (e.g., digits 0-9) assign each document a single unique output token, requiring the model's output vocabulary to equal the corpus size. Since the output projection layer has dimensions of hidden size × vocabulary size, scaling to 8.8 million documents with a hidden size of 768 adds approximately 6.8 billion parameters to the output layer alone, making this approach prohibitively expensive for large corpora. In contrast, sequential identifiers like naive string IDs reuse a fixed vocabulary across multiple decoding steps, keeping parameter count constant regardless of corpus size. \cite{kuo2024surveygenerativeinformationretrieval, pradeep2023doesgenerativeretrievalscale}. %quickly recheck so I understand well

\paragraph{Dynamic Corpus}
In environments where corpora change, \glsxtrshort{gr} models exhibit two opposing failure modes \cite{Chen_2023, zhang2025replicationexplorationgenerativeretrieval, yuan2024generativedenseretrievalmemory, mehta2023dsiupdatingtransformermemory}. Forgetting occurs when updating a model with new documents degrades its ability to retrieve previously indexed documents. Experiments demonstrate that sequential indexing of new batches can cause indexing accuracy for the initial corpus to drop by more than 25 points, necessitating retraining and increasing costs \cite{mehta2023dsiupdatingtransformermemory}. The opposing failure is an excessive bias against recency, where models fail to retrieve newly added content, favoring documents from the original training corpus. %source!

\glsxtrshort{gr} models also exhibit forgetting during initial training as well. Analysis of training dynamics shows models frequently ``forget'' and ``re-learn'' document-to-identifier mappings, with approximately 88\% of documents undergoing at least one forgetting event during training \cite{mehta2023dsiupdatingtransformermemory}.

An additional issue identified is the problem that newly added documents are practically inaccessible until model weights are updated and the model retrained, unlike dense retrieval systems which can index new embeddings instantly without retraining the encoder \cite{kuo2024surveygenerativeinformationretrieval}.

\paragraph{Interpretability}
Lastly, the lack of interpretability in generation processes presents challenges for understanding and debugging failures. Unlike sparse retrieval where term matching is transparent, or dense retrieval where embedding similarity can be analyzed, the internal decision processes of generative retrieval is significantly more opaque \cite{10.1145/3589335.3641239, ross2021evaluatinginterpretabilitygenerativemodels}.

\section{Methodology}

\subsection{Available Data Structures}

We ran \glsxtrshort{SEAL} and \glsxtrshort{MINDER} with their default settings on the \glsxtrfull{nq} dataset created by \href{https://research.google/pubs/natural-questions-a-benchmark-for-question-answering-research/}{Google}. We chose this as the dataset is the academic standard for retrieval benchmarking. The \glsxtrfull{nq} retrieval corpus consists of a Wikipedia dump split into approximately 21 million passages of 100 tokens each \cite{karpukhin2020densepassageretrievalopendomain}. The systems analyzed retrieve from all 21 million chunks independently. If multiple chunks from the same article match the query, they can all appear in the results. 

The \glsxtrshort{SEAL} output dataset contains 6,515 queries from the \glsxtrlong{nq} benchmark. For each query, the following relevant information is available:

\begin{itemize}
    \item \textbf{question}: The original natural language query
    \item \textbf{answers}: Ground-truth answer string(s)
    \item \textbf{positive\_ctxs}: Ground-truth relevant passages containing the answer
    \item \textbf{ctxs}: \glsxtrshort{SEAL}'s top-100 retrieved passages, ranked by passage score, each containing:
    \begin{itemize}
        \item Passage title and text
        \item Aggregate document score
        \item The set of matched n-gram keys and their respective scores
    \end{itemize}
\end{itemize}

From the output data available, the n-gram keys are most useful to understand \glsxtrshort{SEAL}'s retrieval decisions. Each key entry contains three parts:
\begin{enumerate}
    \item The n-gram string (e.g., \texttt{`` Reba''}, \texttt{``</s> Linda Davis @@''})
    \item The corpus frequency of how many times this n-gram appears across all documents
    \item The n-gram score computed via the scoring function
\end{enumerate}

The \glsxtrshort{MINDER} output is structurally identical to that of \glsxtrshort{SEAL}'s, including being ran on the same 6,515 queries. The difference arises from the pseudoqueries, which are appended to the \textit{text}, as well as the fact that multiview identifiers can be contained in n-grams.

The output dataset (of both systems) have an average per-query positive context count of 8.46, with the minimum being 1 and maximum being 101. This is important to analyze to accurately identify evaluation metrics. To illustrate the distribution, please consult the histogram below.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/positive_ctxs_histogram.png}
\end{figure}

\subsection{Evaluation}

We decided to focus on the following metrics: R-precision as the overall retrieval quality of both \glsxtrshort{SEAL} and \glsxtrshort{MINDER} and hits@1 (being identical to precision@1) to quantify whether the model successfully retrieved a positive context in its top results for a query. We also use hits@10, since retrieval is rarely only about the first result only. We chose $k=10$ somewhat arbitrarily, as it provides a balance between being too strict by only measuring the top results and being too lax by allowing lower-scored retrieved passages to be included in the analysis. Although they might be used occasionally, we avoid using Precision@k and Recall@k with $k>1$ to avoid misrepresentation of queries where the amount of ground truths is less than $k$.

For the analysis, results are calculated side-by-side for both systems analyzed when appropriate. To analyze distribution, we bin the results into deciles and report them. Additionally, when appropriate, we report the Spearman $rho$ values to analyze monotonic correlation between variables.

\subsection{Failure Taxonomy}

Below we categorize identified theoretical failure modes whether they are measurable considering our available data.

\begin{table}[htbp]
    \centering
    \caption{Classification of potential retrieval failure modes for \glsxtrshort{SEAL} and \glsxtrshort{MINDER}.}
    \label{tab:failure-summary}
    \begin{tabular}{lp{6cm}l}
        \toprule
        \textbf{Type} & \textbf{Potential Failure Mode} & \textbf{Applicability} \\
        \midrule
        \textbf{Measurable} & Nonspecific $n$-grams & Both \\
        & Too many unigrams & Both \\
        & Redundant $n$-grams & Both \\
        & Query--$n$-gram not overlapping & Both \\
        & Answer not in $n$-gram & Both \\
        & Single $n$-gram dominating & Both \\
        & Title repetition in top-$k$ & Both \\
        & No pseudo-queries as n-gram & \glsxtrshort{MINDER} \\
        \midrule
        \textbf{Non-measurable} & False pruning & Both \\
        & Answer cut-off btwn. passages & Both \\
        & Optima not found in beam search & Both \\
        & Scalability issues & Both \\
        & Handling of dynamic corpora & Both \\
        & Training learning bottlenecks & Both \\
        \bottomrule
    \end{tabular}
\end{table}

The measureable categories will be analyzed below. Tor the other identified failure modes, the main reason for non-measurability is the fact that we only possess output data of \glsxtrshort{SEAL} and \glsxtrshort{MINDER} on the \glsxtrshort{nq} dataset, without having access to training data or internal logs.

\paragraph{Answer cut-off between passages }
A fundamental structural constraint of n-gram-based retrieval is that n-grams must be strictly contiguous substrings within the corpus. Both models ensure that any generated n-grams exists as a continuous sequence in at least one document (aside from \glsxtrshort{MINDER}'s pseudo-queries). This means the model cannot match n-grams that would semantically connect a query to a passage if the relevant terms are separated by intervening words or are cut off between passages.

Consider a passage stating ``Tom likes pizza and tomatoes'' and a query ``does Tom like tomatoes?'' The semantically relevant n-gram ``likes tomatoes'' does not exist as a contiguous substring in the passage due to the intervening words ``pizza'' and ``and.''  This issue is partially mitigated by the fact that n-grams ``and tomatoes,'' ``pizza and tomatoes,'' ``tomatoes.'' are still generated and scored, however this is not a robust solution. Including skip-grams in addition to ngrams or forcing overlap between passages could theoretically be solutions to this problem. These are, however, out of scope for this thesis.

\subsection{Data Validation} %(The LLM-as-a-Judge procedure)
\label{sec:llm-judge}

We identified cases where \glsxtrshort{SEAL} or \glsxtrshort{MINDER} retrieves a passage containing the answer, but this passage differs from the annotated ground-truth.

To identify such cases, we check whether the answer string appears in any top-$k$ retrieved passage based on token-level overlap using \texttt{GPT2TokenizerFast}. We use $k=10$, as discussed above. Table~\ref{tab:answer-passage-mismatch} presents the results.

\begin{table}[htbp]
    \centering
    \caption{Retrieval outcomes based on answer string matching ($k=10$).}
    \label{tab:answer-passage-mismatch}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Outcome} & \textbf{$n$} & \textbf{\%} & \textbf{$n$} & \textbf{\%} \\
        \midrule
        Answer in top-$k$ (Hits@$k$)     & 4,961 & 76.1\% & 5,113 & 78.5\% \\
        Answer in unannotated passage   & 349   & 5.4\%  & 362   & 5.6\%  \\
        Answer string not in top-$k$    & 1,205 & 18.5\% & 1,040 & 16.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

For \glsxtrshort{SEAL} in 5.4\% of the queries, for \glsxtrshort{MINDER} in 5.6\% of the queries, the answer string appears in (at least) one of the top-10 retrieved passages that is not the annotated ground truth. However, string matching alone does not determine whether the passage genuinely answers the question.
 
To understand what the 349 ``answer in different passage'' cases actually look like, we examine three examples in detail extracted form the \glsxtrshort{SEAL} outputs. Similar cases can be found in \glsxtrshort{MINDER}'s output too.

\textbf{Example 1: ``which apostle had a thorn in his side'' (answer: Paul).} The ground-truth passage is from the Wikipedia article ``Thorn in the flesh,'' which explains that Paul the Apostle used this phrase in 2 Corinthians 12:7--9. \glsxtrshort{SEAL}'s top result was from Galatians 3, which mentions ``Paul the Apostle'' as the author of that epistle but contains nothing about a thorn. Even though \glsxtrshort{SEAL} seemed to be able to identify ``Paul'' as a relevant term (potentially from BART's parametric), Galatians 3 is the wrong chapter entirely. \glsxtrshort{SEAL} matched on generic terms (``Paul,'' ``apostle'') but never generated n-grams containing ``thorn.''

\textbf{Example 2: ``how many judges currently serve on the supreme court'' (answer: nine).} The ground-truth passage directly states the current composition of the Court. \glsxtrshort{SEAL}'s top result is a historical passage stating that in 1869, ``the Circuit Judges Act returned the number of justices to nine, where it has since remained.'' This is borderline, because the reader could infer the answer, but the passage is not specifically about the current number of judges.

\textbf{Example 3: ``who is the youngest judge currently sitting on the u.s. supreme court'' (answer: Neil Gorsuch).} The ground-truth passage discusses Gorsuch's nomination process and him being the youngest sitting justice. \glsxtrshort{SEAL}'s top result directly states: ``Neil Gorsuch is the youngest justice sitting, at 50 years of age.'' This is a success, because \glsxtrshort{SEAL} found a passage that answers the question, even if not annotated.

These examples show that these cases are a mix of such cases. To determine how many fall into each category, in section~\ref{sec:llm-judge}, we apply LLM-as-a-judge to classify whether each retrieved passage genuinely answers the question or not. If most of the 349 cases are genuine retrievals, then \glsxtrshort{SEAL}'s effective top-10 recall is closer to 81.5\% rather than 76.1\%. If most are coincidental matches, then 76.1\% remains the accurate figure. Similar calculations can be done for \glsxtrshort{MINDER}.

For each case, we present a large language model (\texttt{claude-sonnet-4-5\-20250929}) with the original question, the expected answer, and the retrieved passage text, asking it to determine whether the passage genuinely answers the question or whether the answer string's presence is coincidental. The model was chosen based on expected reasoning capabilities and cost of operation.

The judge classifies each case into one of three categories:
\begin{itemize}
    \item \textbf{YES}: The passage directly and unambiguously answers the question.
    \item \textbf{NO}: The answer string appears coincidentally and the passage does not answer the question.
    \item \textbf{PARTIAL}: The passage provides relevant information from which the answer could be inferred, but does not state it directly.
\end{itemize}

\section{Results}

\subsection{Preliminary Analysis}
\label{sec:failure-pattern-analysis}
%TODO FIND A BETTER NAME TODO
We analyze the distribution of generated n-gram keys across retrieved documents. We categorize each query based on Precision@2: PP (both correct), PN (rank-1 correct, rank-2 incorrect), NP (rank-1 incorrect, rank-2 correct), and NN (both incorrect). Table~\ref{tab:category-distribution} presents the distribution of queries across retrieval outcome categories. In our definition, positive means the retrieved passage corresponds to a ground truth passage, therefore ``correct'', while negative accordingly means ``incorrect''.

\begin{table}[htbp]
    \centering
    \caption{Distribution of queries across retrieval outcome categories ($N=6,515$).}
    \label{tab:category-distribution}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Category} & \textbf{Count} & \textbf{Percentage} & \textbf{Count} & \textbf{Percentage} \\
        \midrule
        PP & 1,287 & 19.8\% & 1,345 & 20.6\% \\
        PN & 1,427 & 21.9\% & 1,685 & 25.9\% \\
        NP & 702   & 10.8\% & 729   & 11.2\% \\
        NN & 3,099 & 47.6\% & 2,756 & 42.3\% \\
        \bottomrule
    \end{tabular}
\end{table}


For both architectures analyzed, nearly half of all queries result in NN cases. Combined with the NP cases, this indicates that for \glsxtrshort{SEAL} 58.4\% of queries, for \glsxtrshort{MINDER} 53.5\% fail to retrieve a relevant passage at rank~1. Conversely, 41.7\% and 46.5\% of queries achieve successful Precision@1 retrieval (PP~+~PN), respectively.

We also categorize keys into three sets: keys appearing exclusively in positive (ground-truth) passages, keys appearing exclusively in negative passages, and keys shared by both. Table~\ref{tab:key-distribution} presents the aggregate statistics. $\mu$ \% Keys Unique to $P$ defines the average percentage of keys that are unique to positive passages only across the top-k retrieved queries, $\mu$ \% Keys Unique to $N$ defined accordingly. $\mu$ \% Keys Shared ($P \cap N$) denotes the average percentage of keys that are shared between both negative and positive passages across the top-k retrieved queries. $\mu$ Score accordingly refers to the average n-gram score across queries for each categories.

\begin{table}[htbp]
    \centering
    \caption{Aggregate n-gram key statistics across all queries ($N=6,515, K=10$).}
    \label{tab:key-distribution}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Metric} & \textbf{\glsxtrshort{SEAL}} & \textbf{\glsxtrshort{MINDER}} \\
        \midrule
        $\mu$ \% Keys Unique to $P$          & 11.6\% & 12.3\% \\
        $\mu$ \% Keys Unique to $N$          & 78.2\% & 75.3\% \\
        $\mu$ \% Keys Shared ($P \cap N$)    & 10.2\% & 12.5\% \\
        \midrule
        $\mu$ Score (Unique $P$)             & 5.67 ($\pm$6.69)   & 6.12 ($\pm$7.01)  \\
        $\mu$ Score (Unique $N$)             & 6.66 ($\pm$2.90)   & 6.94 ($\pm$2.89)  \\
        $\mu$ Score (Shared)                 & 17.28 ($\pm$12.20)  & 17.34 ($\pm$11.49) \\
        \bottomrule
    \end{tabular}
\end{table}

Although the results between the two systems are fairly similar, with \glsxtrshort{MINDER} performing marginally better, it is noticable that both \glsxtrshort{SEAL} and \glsxtrshort{MINDER} generates substantially more keys matching negative passages than positive passages. Second, keys unique to negative passages receive marginally higher average scores than keys unique to positive passages, with keys shared having he highest average score. This pattern indicates that both systems assign higher scores to n-grams that are not discriminative enough, to either way.

The high variance in scores for unique positive keys compared to unique negative keys suggests that the models' ability to identify relevant n-grams is highly inconsistent. While some ``Unique P'' keys receive very high scores, many others likeyly receive negligible scores, dragging down the mean. Furthermore, the high variance in the 'Shared' category indicates a lack of discriminative power and their influence on the final ranking is unpredictable.

\subsection{Query Terms Corpus Coverage}

We analyzed the relationship between query term availability and retrieval success using SEAL's FM-Index. For each of 6,515 Natural Questions queries, we measured the percentage of query words present in the Wikipedia corpus and correlated this with retrieval performance (Hits@1 and Hits@10). Queries were stratified into deciles based on coverage scores. The mean coverage was $85.2\%$ with $90.5\%$ of all queries having $\geq70\%$ of their terms present in the corpus.


Results revealed no meaningful correlation between corpus coverage and retrieval success (Pearson's $r = 0.017$, $p = 0.165$ for Hits@1; $r = 0.001$, $p = 0.962$ for Hits@10). As shown in Table~\ref{tab:coverage}, queries in D1 achieved 42.1\% Hits@1 and 77.1\% Hits@10, while queries with complete coverage in D1 achieved 44.8\% Hits@1 and 77.4\% Hits@10, effectively identical performance. This null result demonstrates that SEAL's retrieval failures are not primarily driven by out-of-vocabulary terms or lexical gaps.

\begin{table}[H]
\centering
\caption{Retrieval performance across corpus coverage deciles. Coverage represents the percentage of query words present in the Wikipedia corpus.}
\label{tab:coverage}
\begin{tabular}{lcccc}
\hline
\textbf{Decile} & \textbf{Coverage} & \textbf{Hits@1} & \textbf{Hits@10} & \textbf{N} \\
\hline
D1 (lowest)  & 0.38--0.70 & 42.1\% & 77.1\% & 651 \\
D2           & 0.70--0.75 & 37.6\% & 74.7\% & 651 \\
D3           & 0.75--0.78 & 41.9\% & 76.5\% & 651 \\
D4           & 0.78--0.88 & 39.2\% & 73.4\% & 651 \\
D5           & 0.88--0.88 & 40.2\% & 77.1\% & 651 \\
D6           & 0.88--0.89 & 43.6\% & 76.7\% & 651 \\
D7           & 0.89--0.90 & 41.5\% & 75.7\% & 651 \\
D8           & 0.90--1.00 & 41.6\% & 79.3\% & 651 \\
D9           & 1.00--1.00 & 43.9\% & 73.6\% & 651 \\
D10 (highest)& 1.00--1.00 & 44.8\% & 77.4\% & 656 \\
\hline
\end{tabular}
\end{table}


\subsection{LLM-as-a-Judge Results}
\label{sec:llmresults}

According to the methodology explained in Section \ref{sec:llm-judge}, we adjust the estimate of \glsxtrshort{SEAL}'s and \glsxtrshort{MINDER}'s retrieval performance. Table~\ref{tab:adjusted-recall} presents the adjusted hits@10 under different inclusion criteria. \textbf{MINDER DATA IS TBD. TODO}

Table~\ref{tab:llm-judge-results} presents the classification results.

\begin{table}[htbp]
    \centering
    \caption{LLM-as-a-judge classification of answer-in-different-passage cases.}
    \label{tab:llm-judge-results}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Verdict} & \textbf{Count} & \textbf{Percentage} & \textbf{Count} & \textbf{Percentage} \\
        \midrule
        YES     & TBD & TBD & TBD & TBD \\
        NO      & TBD & TBD & TBD & TBD \\
        PARTIAL & TBD & TBD & TBD & TBD \\
        \bottomrule
    \end{tabular}
\end{table}

The results reveal that \textbf{TBD}

\begin{table}[htbp]
    \centering
    \caption{Adjusted hits@10 estimates based on LLM-as-a-judge results.}
    \label{tab:adjusted-recall}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Inclusion Criterion} & \textbf{Hits@10} & \textbf{\%\textsuperscript{*}} & \textbf{Hits@10} & \textbf{\%\textsuperscript{*}} \\
        \midrule
        Hit@10 & 4961 & 76.1\% & 5113 & 78.4\% \\
        + YES verdicts             & TBD & TBD & TBD & TBD \\
        + YES and PARTIAL verdicts & TBD & TBD & TBD & TBD \\
        \bottomrule
        \multicolumn{5}{l}{\small \textsuperscript{*}Percentage of the total 6,515 queries.}
    \end{tabular}
\end{table}

\textbf{TODO ANALYSIS TBD....}

Please note that we will refrain from ``updating the dataset'', as that would make our results incomparable with other papers' results on the standard \glsxtrshort{nq} dataset.

\subsection{Generative Failures}
\label{sec:gen-fail}
Using the identified failure modes in Table \ref{tab:failure-summary}, we present the analysis results pertaining to generation below.

\paragraph{Low Lexical Specificity}
\label{sec:failure-low-lex-spec}

N-gram corpus frequency reflects how distinctive a generated n-gram is. Rare n-grams provide stronger evidence for specific documents, while common n-grams match many topically-related passages. We analyze the average corpus frequency of the top-scored n-grams to determine whether the frequency (and thus score) of n-grams contribute to retrieval outcomes.

Across 6,515 queries, the average mean corpus frequency of the top-5 scored n-grams for the top-1 retrieved passage is 28753 for \glsxtrshort{SEAL} and 21310 for \glsxtrshort{MINDER}, while the same value using all-ngrams for such passage is 382,136 and 497,399, respectively. $k=5$ was arbitrarily selected as a representation of the n-grams with the strongest influence on the overall score of each passage.

Table \ref{tab:nonspec-decile-freq} summarizes the results. The decimals of the frequencies have been truncated for readability. 

\begin{table}[htbp]
    \centering
    \setlength{\tabcolsep}{3pt}
    \caption{Hits@$k$ success rate by n-gram frequency deciles.}
    \label{tab:nonspec-decile-freq}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Decile} & \textbf{Freq. Range} & \textbf{H@1 / H@10} & \textbf{Freq. Range} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 2.0--91          & 71.0\% / 92.8\% & 1--16          & 76.5\% / 93.9\% \\
        D2  & 91--356        & 51.5\% / 85.1\% & 16--55         & 65.9\% / 89.4\% \\
        D5  & 2,940--5,698   & 43.0\% / 76.3\% & 513--1,359     & 45.8\% / 80.2\% \\
        D8  & 18,581--35,824 & 31.4\% / 70.4\% & 7,434--16,582  & 36.7\% / 72.1\% \\
        D10 & 81,049--719,139& 25.9\% / 65.8\% & 44,813--1.7M     & 26.8\% / 60.7\% \\
        \bottomrule
    \end{tabular}
\end{table}

We can see that in D1, where the top-1 passage's top-5 scored n-grams average frequency is low, both hits@1 and hits@10 is surprisingly high for both systems, with \glsxtrshort{MINDER} performing marginally better, while in D10, where the same average frequency is considerably high (implying generic n-grams), the retrieval performance drops significantly for both systems. The Spearman’s $\rho$ between such frequency and hits@1 is $-0.238$ and $-0.290$ respectively, both being $p < 0.001$, indicating that average n-gram frequency is inversely correlated with retrieval success.

Notably, using average frequency across all n-grams (instead of top-5) shows no significant correlation with performance (\glsxtrshort{SEAL}: $\rho=0.029,\space p=0.02$; \glsxtrshort{MINDER}: $\rho=-0.008,\space p=0.51$), suggesting that the presence of highly-ranked generic terms, rather than overall genericity, drives performance degradation. This happens because such high-frequency n-grams match many topically-related passages, diluting discriminative power among passages. While \glsxtrshort{SEAL}'s scoring function theoretically downweights common n-grams through the $P(n)$ term in Equation~\ref{eq:ngram-score}, this analysis reveals that high-frequency n-grams can still dominate the top-ranked positions. \glsxtrshort{MINDER} does not penalize high frequency n-grams. Instead, it artifically boosts the score of pseudoqueries \cite{li2023multiviewidentifiersenhancedgenerative}.

\paragraph{Preference for Unigrams}
\label{sec:unigrams}

Here we examine whether the analyzed systems disproportionately generate short n-grams (unigrams and bigrams) rather than longer, more distinctive multi-word sequences. Longer n-grams generally provide stronger discriminative power, as multi-word phrases are far less likely to appear across multiple passages than individual words.

We quantify this generation bias by measuring the fraction of generated n-grams that consist of a single token in the top-1 retrieved passage. Across 6,515 queries, the mean unigram proportion is $0.869 \pm 0.066$ (median 0.878) for \glsxtrshort{SEAL} and $0.853 \pm 0.071$ (median 0.863) for \glsxtrshort{MINDER}. The average n-gram length is $1.28 \pm 0.18$ words (median 1.24) for \glsxtrshort{SEAL}, and $1.48 \pm 0.39$ (median 1.35) for \glsxtrshort{MINDER}. This indicates that, on average, more than four-fifths of generated n-grams are unigrams for both systems. \glsxtrshort{MINDER} shows slightly higher variance in length ($\sigma=0.39$ vs. $0.18$).

We categorize queries into deciles based on the unigram proportion. Queries in D1 (lowest unigram proportion) achieve considerably higher hit rates than those in D10, as evidenced by Table \ref{tab:unigram-dec}.


\begin{table}[htbp]
    \centering
    \caption{Hits@$k$ success rate by unigram proportion deciles.}
    \label{tab:unigram-dec}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Decile} & \textbf{Div. Range} & \textbf{H@1 / H@10} & \textbf{Div. Range} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 0.51--0.78 & 67.7\% / 91.2\% & 0.48--0.76 & 73.4\% / 92.8\% \\
        D2  & 0.78--0.82 & 61.6\% / 87.1\% & 0.76--0.80 & 69.4\% / 92.2\% \\
        D3  & 0.82--0.84 & 48.8\% / 82.5\% & 0.80--0.83 & 56.8\% / 85.6\% \\
        D5  & 0.86--0.88 & 44.2\% / 81.0\% & 0.85--0.86 & 47.8\% / 81.9\% \\
        D8  & 0.91--0.92 & 28.4\% / 69.9\% & 0.89--0.91 & 31.8\% / 73.7\% \\
        D10 & 0.94--1.00 & 22.6\% / 60.5\% & 0.93--1.00 & 21.2\% / 55.9\% \\
        \bottomrule
    \end{tabular}
\end{table}

The results are interesting for \glsxtrshort{MINDER} specifically, as \glsxtrshort{MINDER} is able to use its generated pseudoqueries (which tend to be longer) as n-grams. This correlates with the results presented below at Section \ref{sec:pseudo-minder-contrib}, indicating that pseudoqueries appearing in n-grams provides the retrieval model with a higher hitrate. \glsxtrshort{MINDER} also shows a higher hit rate at lower unigram proportion deciles.

The Spearman $\rho$ between unigram proportion and hits@k for \glsxtrshort{SEAL} is $\rho - 0.278$ for $k=1$ and  $\rho - 0.220$ for $k=10$, while for \glsxtrshort{MINDER} is $\rho - 0.330$ for $k=1$ and  $\rho - 0.260$ for $k=10$. Each value is statistically significant at $p<0.001$.

To understand the results more illustratively, we summarized the table in the following chart, clearly showing \glsxtrshort{MINDER}'s marginally higher success rate at low unigram proportions. The hit rates converge at higher unigram proportions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/ngram_length_bias_comparison.png}
\end{figure}

\paragraph{Overlapping N-gram Tokens}
\label{sec:overlapping-ngram-tokens}

We initially hypothesized that cases where the model produces semantically redundant n-grams with high token overlap would constitute a failure mode by inflating scores without providing new evidence. To test this, we measure a ratio defined as the number of unique tokens divided by the total number of tokens across all generated n-grams for the top-1 retrieved query, which we will refer to as ``diversity''. We split up the n-grams into tokens using \texttt{GPT2TokenizerFast}.

Table \ref{tab:token-diversity-ratio} illustrates the results. Contrary to our initial hypothesis, the results reveal a  negative correlation between diversity and hits@k. For \glsxtrshort{SEAL}, $\rho = -0.111$ for  $k=1$ and $\rho = -0.101 $ for  $k=10$. For \glsxtrshort{MINDER}, the Spearman correlations are $\rho = -0.289$ and $\rho = -0.242$, respectively. All values are statistically significant at $p<0.001$.

\begin{table}[H]
    \centering
    \caption{Hits@k by token-based diversity ratio deciles (unique / total tokens).}
    \label{tab:token-diversity-ratio}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Decile} & \textbf{Diversity} & \textbf{H@1 / H@10} & \textbf{Diversity} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 0.36--0.48 & 52.6\% / 84.4\% & 0.28--0.56 & 70.9\% / 91.4\% \\
        D2  & 0.48--0.49 & 46.3\% / 81.4\% & 0.56--0.64 & 64.3\% / 88.6\% \\
        D3  & 0.49--0.50 & 43.1\% / 78.2\% & 0.64--0.71 & 53.2\% / 85.6\% \\
        D5  & 0.51--0.52 & 40.4\% / 73.9\% & 0.76--0.79 & 53.7\% / 84.1\% \\
        D8  & 0.53--0.54 & 34.9\% / 68.1\% & 0.86--0.88 & 32.2\% / 73.2\% \\
        D10 & 0.56--0.63 & 32.4\% / 69.8\% & 0.92--1.00 & 22.0\% / 55.6\% \\
        \bottomrule
    \end{tabular}
\end{table}

It is interesting to note that \glsxtrshort{MINDER} has a considerably higher success rate at low diversity scenarios. This is expected, as \glsxtrshort{MINDER}'s generates titles, pseudo-queries, and substrings simultaneously, meaning that the model frequently repeats key tokens across these different identifier views. The highest success rate is found in the lowest diversity decile, which suggests that the model is able to reinforce a specific semantic cluster through multiview identifiers.

These findings suggest that token repetition is not a redundancy failure, but rather a signal of model certainty. When \glsxtrshort{SEAL} and \glsxtrshort{MINDER} are highly confident in a target document, it generates multiple overlapping n-grams that concentrate score on a specific semantic cluster. In contrast, high identifier diversity indicates a lack of model confidence, where the model produces a wide variety of topically related but lexically distinct tokens that fail to converge.


\paragraph{Query-to-n-gram Overlap}
\label{sec:query-ngram-overlap}

Here we measure the lexical alignment between query terms and the n-grams. We use \texttt{GPT2TokenizerFast} for this. High overlap indicates that the model successfully generated n-grams containing query-relevant terms, while low overlap suggests a mismatch where generated n-grams, though potentially topically related, do not directly address the query's specific terms. We quantify this using the fraction of query tokens that appear in any generated n-gram in the highest-scored retrieved passage, denoted by $C(Q, G_n)$, where $Q$ denotes the query tokens and $G_n$ denotes the tokens of the generated n-grams.

Our analysis across 6,515 queries shows that the token-level lexical overlap varies substantially across the top-1 retrieved passages. As visible in Table~\ref{tab:p-unique-coverage}, there is a positive correlation between lexican overlap and retrieval success. For \glsxtrshort{SEAL}, $\rho = 0.323$ for  $k=1$ and $\rho = 0.190 $ for  $k=10$. For \glsxtrshort{MINDER}, the Spearman correlations are $\rho = 0.380$ and $\rho = 0.202$, respectively. All values are statistically significant at $p<0.001$. 

\begin{table}[H]
    \centering
    \caption{Hits@$k$ by $C(Q, G_n)$ deciles.}
    \label{tab:p-unique-coverage}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Decile} & \textbf{$C(Q, G_n)$} & \textbf{H@1 / H@10} & \textbf{$C(Q, G_n)$} & \textbf{H@1 / H@10} \\
        \midrule
        D1  & 0.00--0.27 & 12.4\% / 55.1\% & 0.00--0.36 & 14.4\% / 61.5\% \\
        D2  & 0.29--0.36 & 24.5\% / 70.2\% & 0.37--0.44 & 25.7\% / 67.9\% \\
        D3  & 0.36--0.40 & 29.6\% / 74.5\% & 0.45--0.50 & 35.1\% / 71.2\% \\
        D5  & 0.46--0.50 & 42.3\% / 76.9\% & 0.58--0.63 & 44.2\% / 79.7\% \\
        D8  & 0.61--0.67 & 53.4\% / 78.1\% & 0.76--0.80 & 62.8\% / 85.7\% \\
        D10 & 0.73--1.00 & 70.0\% / 88.4\% & 0.90--1.00 & 77.4\% / 88.6\% \\
        \bottomrule
    \end{tabular}
\end{table}

As visible from Table~\ref{tab:p-unique-coverage}, the hit rate of \glsxtrshort{SEAL} and \glsxtrshort{MINDER} are relatively similar, and are positively correlated with $C(Q, G_n)$. Similar to other failure modes, low query coverage typically arises when the model generates semantically related but lexically distinct n-grams (e.g., for a query containing ``automobile,'' the model could generate n-grams with ``car'') or when the model generates generic topical n-grams that lack the specific query terms needed to isolate the answer (e.g., a query about ``Victorian architecture in London'' generates only ``architecture'' and ``London'').


\paragraph{Answer as N-gram}
\label{sec:answer-ngram}

We analyze whether retrieval success correlates with the model's ability to generate the specific answer string as an identifier, or whether it relies on the surrounding context. While generating the answer string provides a valuable anchor for a passage, the primary goal of \glsxtrshort{gr} is document identification, not direct question answering.

Table \ref{tab:answer-presence-impact} illustrates the results of our analysis. We looked at whether among the top-1 and top-10 scored retrieved passages, at least one of them contained the answer string or not and what the Hits@1 value is for these cases. The results are mostly similar between \glsxtrshort{SEAL} and \glsxtrshort{MINDER}, with most top-1 ranked retrieved passages not containing the answer string, yet when they do, the respective query's Hits@1 rate doubles and Hits@10 increases significantly.

\begin{table}[htbp]
    \centering
    \caption{Performance metrics conditioned on answer string presence.}
    \label{tab:answer-presence-impact}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Metric} & \textbf{Pres.} & \textbf{Abs.} & \textbf{Pres.} & \textbf{Abs.} \\
        \midrule
        Queries ($N$) & 1,096  & 5,419  & 1,457  & 5,058  \\
        $\bar{\text{Hits@1}}$        & 75.0\% & 34.9\% & 75.4\% & 38.2\% \\
        $\bar{\text{Hits@10}}$       & 93.5\% & 72.6\% & 93.0\% & 74.3\% \\
        \bottomrule
    \end{tabular}
\end{table}

We can see that in cases where the answer string is identified among the n-grams, the success rate jumps significantly for both systems. This indicates that direct identification is a strong predictor of retrieval success. Both \glsxtrshort{SEAL} and \glsxtrshort{MINDER} are able to retrieve (at least) one correct passage in around ~35\% of cases at the first place and ~73\% of cases among the top-10 retrieved, suggesting that the model is able to identify the semantic environment in which an answer is likely to be in relatively convincingly. That said, the performance gap between the two cases indicates that when the model fails to generate the specific answer as an identifier, the remaining contextual cues are oftentimes too generic to distinguish the correct passage from topically similar alternatives.


\subsection{Scoring Failures}

Just like in Section \ref{sec:gen-fail}, we present the analysis results of the identified failure modes (Table \ref{tab:failure-summary}) pertaining to scoring below.

\paragraph{Unique Articles among Passages}

Firstly, we analyze whether there is a correlation with retrieval failure and the amoung of unique titles retrieved in the top-k passages. For this, according to the other analyses, we use $k=10$. Thie analysis is possible, as Wikipedia articles have unique titles.

The results are illustrated in the following table and line chart:

\begin{table}[H]
    \centering
    \caption{Hits@k by title repetition count.}
    \label{tab:title-repetition-impact}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}}} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}}} \\
        \textbf{Max Repetition\textsuperscript{*}} & \textbf{$N$} & \textbf{H@1 / H@10} & \textbf{$N$} & \textbf{H@1 / H@10} \\
        \midrule
        1 & 23    & 30.4\% / 60.9\% & 38    & 28.9\% / 60.5\% \\
        2                 & 556   & 45.3\% / 70.1\% & 665   & 46.9\% / 72.9\% \\
        4                 & 653   & 42.1\% / 77.3\% & 819   & 46.8\% / 77.7\% \\
        6                 & 573   & 41.5\% / 76.1\% & 650   & 45.1\% / 79.5\% \\
        8                 & 567   & 44.3\% / 78.0\% & 609   & 49.8\% / 82.8\% \\
        10 & 1,606 & 38.0\% / 76.7\% & 990   & 42.1\% / 79.1\% \\
        \bottomrule
        \multicolumn{5}{l}{\small \textsuperscript{*}Maximum number of passages sharing the same title in the top-10 results.}
    \end{tabular}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/title_repetition_comparison.png}
\end{figure}

We can see that when stratified by maximum title repetition, i.e. the maximum number of passages from the same Wikipedia article per query, \glsxtrshort{MINDER} marginally outperforms \glsxtrshort{SEAL}, especially on queries where the system is relatively sure of the answer, as evidenced by a specific title appearing in several of the top-10 results. We can see that in cases where the max repetition is very low, i.e. the model couldn't confidently pinpoint a single document and is likely guessing, retrieval success suffers. Based on this analysis, there are significantly more cases with high title repetition than low title repetition, indicating that the models are fairly certain about the correct document (or, alternatively, are confidently incorrect about one).


\paragraph{Overreliance on a Single Identifier}
\label{sec:failure-6}

Overreliance on a single identifier measures the extent to which the top-1 ranked passage's total score is concentrated in one high-scoring n-gram versus distributed across multiple n-grams. We define this as the ratio of the highest scored n-gram score and the sum of all matched n-gram scores for the passage.

Our analysis across 6,515 queries reveals that lower such ratio correlates with higher retrieval success, as shown in Table~\ref{tab:identifier-dominance}. For \glsxtrshort{SEAL}, we observe a Spearman correlation of $\rho = -0.115$ ($p < 0.001$), with success rates declining from 48.6\% in the lowest decile (D1) to 29.6\% in the highest decile (D10).

\begin{table}[htbp]
    \centering
    \caption{Hits@1 by top n-gram ratio deciles (max score / sum scores).}
    \label{tab:identifier-dominance}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}} (Mean: 0.44)} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}} (Mean: 0.36)} \\
        \textbf{Decile} & \textbf{Range} & \textbf{Hits@1} & \textbf{Range} & \textbf{Hits@1} \\
        \midrule
        D1  & 0.12--0.30 & 48.6\% & 0.12--0.25 & 58.0\% \\
        D2  & 0.30--0.35 & 45.8\% & 0.25--0.28 & 59.3\% \\
        D3  & 0.35--0.38 & 45.4\% & 0.28--0.31 & 55.8\% \\
        D5  & 0.41--0.44 & 44.3\% & 0.33--0.35 & 45.1\% \\
        D8  & 0.50--0.53 & 35.4\% & 0.40--0.43 & 40.2\% \\
        D10 & 0.57--0.85 & 29.6\% & 0.48--0.94 & 32.5\% \\
        \bottomrule
    \end{tabular}
\end{table}

This trend is also noticable in \glsxtrshort{MINDER} ($\rho = -0.169$, $p < 0.001$), which also exhibits a lower mean ratio of 0.36 compared to \glsxtrshort{SEAL}'s 0.44 and a higher hit rate in all deciles. This shift toward more distributed evidence is assumed to be a consequence of \glsxtrshort{MINDER}'s multiview identifiers, since by aggregating scores from titles, pseudo-queries, and substrings, \glsxtrshort{MINDER} theoretically reduces reliance on any single identifier.

This finding aligns with the additive scoring design, which benefits from distributed evidence by providing redundant confirmation across multiple n-grams. %TODO SOURCE
Conversely, if the top-scoring n-gram matches many passages or identifies the wrong one, the remaining low-scoring n-grams may provide insufficient corrective evidence to ensure correct ranking.

\paragraph{Impact of N-gram Quantity}

While not a necessarily failure mode, given that \glsxtrshort{SEAL}'s scoring mechanism additively aggregates evidence from multiple n-grams, it is essential to examine whether the quantity of matched n-grams correlates with retrieval success. Intuitively, a higher number of matched n-grams should provide stronger cumulative evidence for a passage and a passage with more matched n-grams has a mathematical advantage over a passage with fewer matched n-grams. However, excessive generation may also introduce noise if additional n-grams are redundant or non-discriminative.

Across 6,515 queries, the number of n-grams matched to the top-1 retrieved passage has a mean of 41.5 (median: 42.0, $\sigma$: 6.3), with a range of 11 to 65. Table~\ref{tab:ngram-count} presents retrieval performance stratified by n-gram count deciles.

\begin{table}[htbp]
    \centering
    \caption{Hits@1 and mean n-gram corpus frequency $\bar{F}_c$ by n-gram count deciles for the top-1 retrieved passage.}
    \label{tab:ngram-count}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Decile} & \textbf{Range} & \textbf{$N$} & \textbf{Hits@1} & \textbf{$\bar{F}_c$} \\
        \midrule
        D1  & 11--34 & 828  & 36.0\% & 384,785 \\
        D2  & 35--36 & 495  & 37.4\% & 378,346 \\
        D3  & 37--39 & 1,017 & 39.5\% & 380,174 \\
        D4  & 40--40 & 425  & 46.1\% & 392,012 \\
        D5  & 41--42 & 861  & 45.2\% & 382,931 \\
        D6  & 43--43 & 448  & 43.8\% & 382,506 \\
        D7  & 44--45 & 795  & 45.0\% & 385,779 \\
        D8  & 46--47 & 622  & 42.4\% & 382,767 \\
        D9  & 48--49 & 432  & 41.7\% & 380,999 \\
        D10 & 50--65 & 592  & 41.6\% & 371,719 \\
        \bottomrule
    \end{tabular}
\end{table}

The relationship between n-gram count and retrieval success is notably weak (Spearman $\rho = 0.040$, $p = 0.001$), exhibiting a non-monotonic, inverted-U pattern. Hits@1 increases from 36.0\% for passages with low identifier counts (D1, $<$34) to a peak of 46.1\% in the fourth decile, subsequently plateauing and slightly declining to 41.6\% for passages in the highest decile (D10, $>$50). This trend suggests that while a minimum amount of evidence is required to identify a passage, accumulating n-grams beyond a moderate threshold yields diminishing returns and does not significantly influence hit-rate.

Notably, the mean n-gram corpus frequency ($\bar{F}_c$) remains stable across all deciles. This stability indicates that the hit-rate plateau is not caused by the model reverting to generic "filler" tokens at higher counts. Instead, the it likely stems from the scoring mechanism's inability to disambiguate between a small set of highly discriminative n-grams and the large aggregate score from multiple, less specific identifiers.

\paragraph{Pseudo-Query Contribution in MINDER}
\label{sec:pseudo-minder-contrib}
Here we analyze the contribution of synthetic identifiers in the \glsxtrshort{MINDER} framework. As detailed in Section \ref{sec:minderarch}, \glsxtrshort{MINDER} incorporates pseudo-queries alongside titles and substrings to capture high-level semantic information. Our analysis of 6,515 queries reveals an interesting divergence between the quantity of generated pseudo-queries and their influence on the final ranking.

Numerically, pseudo-queries constitute a negligible fraction of the generated identifiers, accounting for only 1.4\% of the total n-grams (379,080 out of 26.9 million). However, their contribution to the total accumulated score is disproportionately high, representing 27.7\% of the total score mass (91.1 million out of 328.5 million). This confirms that \glsxtrshort{MINDER}'s scoring mechanism successfully uses sparse but highly discriminative synthetic identifiers. %TODO recheck numbers


\section{Discussion}

\textbf{TODO}

\subsection{Key Findings}

\textbf{TODO}

\subsection{Generalizability} %of method and findings

\textbf{TODO}

\section{Conclusion} % incl future work

\textbf{TODO}

\section*{AI Use Disclosure}
Some portions of code for this thesis were generated with the assistance of \textit{Claude by Antropic} and \textit{ChatGPT by OpenAI} large language models. All AI-generated code was reviewed, edited, and verified by the author.


\clearpage
\appendix

\section{LLM-as-Judge prompt}

The prompt used for the LLM-as-Ludge component in Section \ref{sec:llm-judge} is pasted below:

\lstset{breaklines=true}
\begin{lstlisting}
Question: [QUESTION]
Expected answer: [ANSWER]
Retrieved passage title: [PASSAGE TITLE]
Retrieved passage text: [PASSAGE TEXT]

Does this passage answer the question?
- YES: The passage directly answers the question.
- NO: The answer string appears coincidentally but does not answer the question.
- PARTIAL: Contains relevant information but requires inference.

The answer explanation should not exceed 100 words. 
\end{lstlisting}