
\newcommand{\adrian}[1]{\textcolor{orange}{#1}}

\section{Introduction}

Generative retrieval represents an emerging paradigm in \glsxtrshort{ir} that aims to consolidate retrieval processes into end-to-end autoregressive models. While these systems offer promising advantages over traditional approaches, they also exhibit distinct failure modes, behavioral biases, and structural limitations that remain poorly understood. This chapter introduces the research problem and establishes the foundation for the thesis. Section 1.1 presents the motivation for studying failure modes in generative retrieval systems and explains the gap in existing research. Section 1.2 formulates the research questions that guide this thesis. Section 1.3 outlines the key contributions of this work. Finally, Section 1.4 provides an overview of the thesis structure and describes the content of subsequent chapters.

\subsection{Motivation}

\glsxtrfull{gr} is an emerging framework in \glsxtrshort{ir} systems. Unlike traditional approaches that rely on separate vector index structures for document embeddings and multi-stage pipelines, generative retrieval systems directly generate document identifiers through autoregressive language models, consolidating the retrieval process into a single end-to-end model \cite{kuo2024surveygenerativeinformationretrieval, li2025matchinggenerationsurveygenerative, zhang2024generativeretrievaltermset}. This offers several compelling advantages, such as eliminating the need for dense vector indexes and their associated \glsxtrfull{ANN} search structures, reducing memory footprint and enabling the model to learn document representations jointly with the retrieval objective \cite{sun2023learningtokenizegenerativeretrieval, tang2024listwisegenerativeretrievalmodels}.

Despite these promising characteristics, generative retrieval systems exhibit distinct failure modes that differ fundamentally from those observed in traditional retrieval architectures. While dense retrieval failures typically manifest as low similarity scores in vector space, generative retrieval failures can include hallucination of non-existent document identifiers, incorrect pruning during autoregressive generation, poor performance on dynamically expanding corpora, and challenges with scalability \cite{Metzler_2021, Ji_2023, zeng2024planningaheadgenerativeretrieval, pradeep2023doesgenerativeretrievalscale}. It is also worth noting that some \glsxtrshort{gr} implementations employ auxiliary data structures for constrained generation, making them not fully end-to-end differentiable.\textbf.

Existing literature on \glsxtrshort{gr} documents various challenges and failure cases. However, these failures are typically discussed in isolation with individual paper proposing new methods or architectures. There is no systematic taxonomy that comprehensively classifies the types of failures that occur in generative retrieval systems, their relative frequencies in practice, or their underlying causes.

This thesis addresses this gap by developing a taxonomy of failures, biases, and limitations for generative retrieval systems by synthesizing documented failure cases from the literature and conducting analysis of retrieval outputs from the \glsxtrshort{SEAL} and \glsxtrshort{MINDER} model \cite{bevilacqua2022autoregressivesearchenginesgenerating, li2023multiviewidentifiersenhancedgenerative} on the \glsxtrfull{nq} dataset. The \glsxtrshort{nq} dataset \cite{kwiatkowski-etal-2019-natural} is a benchmark of Google search queries paired with full Wikipedia pages, annotated with the exact section and phrases of the page containing the answer.


\subsection{Research Question}

This thesis addresses two interconnected research questions:

\textbf{RQ1:} What is a systematic classification of the failures, behavioral biases, and structural limitations observed in generative retrieval systems?

\textbf{RQ2:} What are the most frequent failure modes observed in \glsxtrshort{SEAL}'s n-gram-based generative retrieval, and which failures stem from the generation component versus the scoring mechanism?

\subsection{Contributions}

This thesis makes the following contributions:

\begin{enumerate}
    \item A synthesis of failure modes documented across generative retrieval research, organized into five categories: inference and decoding failures, memorization and representation issues, scalability limitations, dynamic corpus problems, and architectural constraints.

    \item A mapping from abstract failure modes to observable indicators measurable from inference outputs.

    \item Quantitative characterization of failure patterns via the \glsxtrshort{SEAL} framework ,\cite{bevilacqua2022autoregressivesearchenginesgenerating} including correlation analysis between failure indicators and retrieval success.


\end{enumerate}
The code used for this thesis is available under \textbf{TODO}.


\pagebreak

\section{Background and Related Work}

In this section we introduce the concept of \glsxtrfull{gr} and discuss the important details of the \glsxtrshort{SEAL} architecture.

\subsection{Information Retrieval}

Traditional \glsxtrfull{ir} has long been structured around the foundational ``index-retrieve-then-rank'' pipeline \cite{10.1145/3589335.3641239, Metzler_2021}. Sparse retrieval methods, such as BM25, rely on lexical representations but suffer from the lexical mismatch problem where semantic similarity is missed if terms do not overlap exactly \cite{Choi_2022, biswas2024efficientinterpretableinformationretrieval, gao2021coilrevisitexactlexical}. Dense retrieval (DR) methods using dual-encoder architectures (e.g., DPR) address this by encoding queries and documents into semantic vector representations \cite{karpukhin2020densepassageretrievalopendomain}. However, DR systems still depend on separate vector index structures and Approximate Nearest Neighbor (ANN) indexes \cite{xiong2020approximatenearestneighbornegative, li2023constructingtreebasedindexefficient}, and queries and documents are encoded independently with limited semantic interaction \cite{ni2021largedualencodersgeneralizable}.

Generative retrieval (\glsxtrshort{gr}) diverges from traditional architectures by aiming to directly generate document identifiers through autoregressive language models, consolidating the retrieval process into a single end-to-end model \cite{zhang2024generativeretrievaltermset, li2025matchinggenerationsurveygenerative, kuo2024surveygenerativeinformationretrieval}. While this aspirational goal is not always fully achieved in practice, as many systems introduce additional components, the unified architecture still offers several advantages when realized: it eliminates the need for separate dense vector indexes and their associated ANN search structures used in traditional dense retrieval, significantly reduces memory footprint, and enables the optimization of document representations and retrieval objectives \cite{bevilacqua2022autoregressivesearchenginesgenerating, zeng2024planningaheadgenerativeretrieval, tang2024listwisegenerativeretrievalmodels}. Systems like \glsxtrshort{SEAL} achieves 7x less storage than Dense Passage Retrieval while showing comparable accuracy@k and exact match metrics on the \glsxtrfull{nq} 320k dataset \cite{bevilacqua2022autoregressivesearchenginesgenerating}. Similarly, \glsxtrshort{PAG} required approximately 7x less memory to index the entire corpus compared to single-vector dense retrieval models and comparable memory needs to BM25. It achieved comparable or better MRR@10 results on the MS-MARCO Dev set compared to sparse and dense retrieval models. Notably, PAG achieved a 70.1\% improvement over BM25 and an 11.2\% improvement over TAS-B \cite{zeng2024planningaheadgenerativeretrieval}. While some \glsxtrshort{gr} implementations employ auxiliary data structures for constrained generation (such as \glsxtrshort{SEAL}'s FM-Index), these differ fundamentally from the separate encoding and indexing stages required by traditional systems. Additionally, the foundational pre-trained language models used in \glsxtrshort{gr} can perform effectively in zero- and few-shot scenarios when limited labeled data is available \cite{Metzler_2021}.


\subsection{Document Identifier Design}

\glsxtrfull{docid} design is the one of the key decisive factors for retrieval effectiveness, as it replaces traditional external indices with the model’s internal parametric memory. A \glsxtrshort{docid} serves as a unique ``index'' that the model must autoregressively generate to point toward a specific document. Initial foundational works categorized these identifiers into three main types: unstructured atomic identifiers (such as unique random integers), naively structured string identifiers (such as titles), and semantically structured, numeric identifiers (such hierarchical cluster paths). While atomic IDs require the model to memorize random associations, semantic IDs group similar documents together under shared prefix tokens to improve retrieval quality \cite{pradeep2023doesgenerativeretrievalscale}.

String identifiers use natural language such as document titles, URLs, or synthetic pseudo-queries, which allow the model to leverage its pre-trained linguistic knowledge rather than learning a new symbolic system from scratch. Substring or n-gram identifiers, such as those used in the \glsxtrshort{SEAL} framework, allow any span of text within a document to act as its identifier. in \glsxtrshort{SEAL} specifically, the documents aren't assigned a  single identifier per document, unlike in other frameworks such as DSI \cite{tay2022transformermemorydifferentiablesearch} or GENRE \cite{sun2023learningtokenizegenerativeretrieval}. A single document has multiple possible identifiers, as \glsxtrshort{SEAL} treats all n-grams within the passage as valid identifiers. While string-based IDs are more interpretable and generalizable to dynamic corpora where document collections expand, they often suffer from ``false pruning'', where a sequence-based generation discards a relevant document early in the beam search if the model fails to predict the correct prefix token.

Semantically structured identifiers are designed to capture the relationships between documents using hierarchical categorization methods, such as by recursively applying k-means clustering to document embeddings, creating a tree where each root-to-leaf path serves as a DocID. Because they share prefixes with similar documents, they facilitate semantic matching, but their sequential nature makes them, just like string-based \glspl{docid}, highly susceptible to false pruning. In this scenario, if a model fails to predict the correct initial cluster token with high enough probability during a beam search, the relevant document is discarded permanently from the candidate list before its full identifier can be generated.

Unstructured atomic identifiers treat each document as a single, unique token added directly to the model's vocabulary. Unlike sequential designs, retrieval involves only a single decoding step, where the model sorts the logits of all \glsxtrshort{docid} tokens to produce a ranked list. While this makes them immune to false pruning and very FLOP efficient during inference, they require a large increase in model parameters as the corpus scales \cite{pradeep2023doesgenerativeretrievalscale}. This is because atomic \glspl{docid} predict the document in a single softmax, the output layer must contain one set of weights per document, causing the parameter count to grow with the collection size \cite{nguyen2023generativeretrievaldenseretrieval, kuo2024surveygenerativeinformationretrieval}. Because they are often randomly initialized, they lack inherent semantic connections to the text, forcing the model to rely entirely on memorizing the association between document content and its identifier during the training phase.

\subsection{SEAL Architecture}

This thesis focuses empirically on \glsxtrshort{SEAL} (Search Engines with Autoregressive LMs) \cite{bevilacqua2022autoregressivesearchenginesgenerating}, which addresses \glsxtrshort{docid} design challenges by using actual document text as identifiers. Such challenges specifically adressed are the unavailability of unique metadata (like titles) for all documents, the insufficient granularity of page- and document-level identifiers, as well as the need to force a structure onto the search space, such as hierarchical cluster trees, which can be difficult to construct for large-scale benchmarks. \glsxtrshort{SEAL} defines document identifiers as any n-gram (substring) within the document. An autoregressive model (e.g., BART) is trained to generate distinctive, query-relevant n-grams that identify documents containing them. A critical component is the FM-Index, a compressed full-text substring index that constrains generation during decoding so that the model only generates n-grams actually existing in the corpus. For ranking, \glsxtrshort{SEAL} uses intersective scoring to aggregate information from multiple generated n-grams while filtering for overlaps. Specifically, ``intersective'' refers to a selection process where an n-gram's score is down-weighed if it overlaps with a higher-scoring n-gram already selected. This is implemented so that the final score does not overscore based on repetitive or redundant document content.

\subsubsection{Retrieval Mechanism}

During the generation phase, \glsxtrshort{SEAL} enforces that at each decoding step, the language model can only generate tokens that would extend the current partial n-gram into a sequence that exists somewhere in the corpus. As an example, when \glsxtrshort{SEAL} has generated the partial sequence ``earthquakes can be,'', ``earthquakes can be predicted'' can only be generated if this complete 4-gram exists as a contiguous substring in at least one document. 

After n-gram generation, in the retrieval phase, for each n-gram, \glsxtrshort{SEAL} uses the FM-index to identify every document containing that n-gram and assign a relevance score based on the n-gram's uniqueness and frequency in the corpus. Documents are then ranked by aggregating the scores from all n-grams they contain. This means that a document scores highly not because any single n-gram uniquely identifies it, but because it contains multiple high-scoring n-grams. The score aggregation is additive. This provides robustness, since even if one highly ranked n-gram points to an irrelevant document, other n-grams can still guide retrieval toward more relevant documents.

This architecture helps explain why \glsxtrshort{SEAL} can overcome vocabulary mismatch issues. While the model cannot generate n-grams that do not exist in the corpus, it can find paraphrases and related terms as long as those appear in corpus documents. \textbf{TODO RECHECK}

\subsubsection{Scoring Mechanism}

\glsxtrshort{SEAL}'s failure modes are directly influenced by its scoring function. \glsxtrshort{SEAL} computes n-gram scores by combining the language model's conditional probability with corpus frequency to favor distinctive n-grams.

The unconditional n-gram probability is computed from corpus statistics:
\begin{equation}
    P(n) = \frac{F(n, R)}{\sum_{d \in R} |d|}
    \label{eq:unconditional-prob}
\end{equation}
where $F(n, R)$ is the frequency of n-gram $n$ in corpus $R$, and $|d|$ is the length of document $d$.

The n-gram score combines conditional and unconditional probabilities:
\begin{equation}
    w(n, q) = \max\left(0, \log \frac{P(n|q)(1 - P(n))}{P(n)(1 - P(n|q))}\right)
    \label{eq:ngram-score}
\end{equation}

This formulation, inspired by TF-IDF and BM25, promotes n-grams that have high conditional probability given the query ($P(n|q)$), computed by the autoregressive language model (BART), but low unconditional probability in the corpus ($P(n)$). This means that distinctive, query-relevant n-grams are prioritized.

Finally, the document score aggregates contributions from multiple non-overlapping n-grams:
\begin{equation}
    W(d, q) = \sum_{n \in K^{(d)}} w(n, q)^\alpha \cdot \text{cover}(n, K^{(d)})
    \label{eq:doc-score}
\end{equation}
where $K^{(d)}$ is the set of non-overlapping n-grams matched in document $d$, $\alpha$ is a hyperparameter, and $\text{cover}(n, K(d))$ downweights n-grams whose tokens overlap with higher-scoring n-grams. The purpose of the coverage weight is to avoid the
overscoring of very repetitive documents, where
many similar ngrams are matched. \cite{bevilacqua2022autoregressivesearchenginesgenerating}

For example, given the query ``who sings does he love me with reba'', the top-ranked passage (titled ``Linda Davis'') matched the following keys:

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lrr}
        \toprule
        \textbf{N-gram} & \textbf{Corpus Freq. $F(n,R)$} & \textbf{Score $w(n,q)$} \\
        \midrule
        \texttt{</s> Linda Davis @@} & 9 & 216.33 \\
        \texttt{ "Does He Love} & 23 & 196.06 \\
        \texttt{ Reba} & 1,851 & 103.56 \\
        \texttt{ country singles} & 777 & 83.47 \\
        \texttt{ country music singer} & 4,024 & 39.20 \\
        \bottomrule
    \end{tabular}
    \caption{Example n-gram keys for query ``who sings does he love me with reba''. Lower corpus frequency correlates with higher scores for query-relevant n-grams.}
    \label{tab:ngram-example}
\end{table}

It is worth noting that while the additive nature of the scoring mechanism could theoretically bias retrieval toward longer documents in a corpus of varying document lengths, this effect is neutralized in our setup as the NQ corpus consists of uniform 100-token passages.

\subsection{MINDER Architecture}
\label{sec:minderarch}
The \glsxtrfull{MINDER} (Multiview Identifiers Enhanced Generative Retrieval) framework \cite{li2023multiviewidentifiersenhancedgenerative} extends the n-gram based approach of \glsxtrshort{SEAL} by introducing synthetic and multiview identifiers. \glsxtrshort{MINDER} is motivated by the observation that single-view identifiers, such as document titles or extractive n-grams, often lack the contextualized information necessary to satisfy complex queries, especially if those require rephrasing.

To address this, \glsxtrshort{MINDER} assigns three distinct views of identifiers to each passage:
\begin{enumerate}
     \item Title of the document
     \item N-grams, similar to \glsxtrshort{SEAL}
     \item Pseudo-queries, i.e. synthetic identifiers generated via a query-generation model that reflect potential queries
\end{enumerate}

The inclusion of pseudo-queries is intended to bridge the gap between user queries and document content. While \glsxtrshort{SEAL} is limited to the exact word orderings present in the corpus, \glsxtrshort{MINDER}'s synthetic view allows the model to match queries against rephrased or summarized versions of the document.

Unlike \glsxtrshort{SEAL}'s TF-IDF inspired log-odds scoring, \glsxtrshort{MINDER} relies primarily on the conditional probability $P(i|q)$ of the identifier $i$ given query $q$, computed by the autoregressive language model. Because pseudo-queries are typically much longer than extractive n-grams, their raw log-probabilities are naturally lower due to the multiplicative nature of autoregressive decoding. To ensure that pseudo-queries can compete with shorter identifiers, \glsxtrshort{MINDER} introduces a biased score to offset this length penalty. The final relevance score for a passage $p$ is an aggregation of the scores from all predicted identifiers that appear in that passage across all three views:
\begin{equation}
   S(q, p) = \sum_{i \in I_p} s(i, q)
   \label{eq:minder-aggregation}
\end{equation}

where $I_p$ is the set of identifiers generated by the model that are present in passage $p$, and $s(i, q)$ is the language model score for identifier $i$.

To manage multiple identifier types, \glsxtrshort{MINDER} uses an FM-Index similar to \glsxtrshort{SEAL} but incorporates identifier-specific prefix tokens (e.g., \texttt{<TS>} for titles, \texttt{<QS>} for queries). During inference, the model is prompted with an identifier prefix and the query, and the FM-index constrains the generation to valid sequences within the respective view.

\section{Methodology}

\subsection{Failure Modes in Generative Retrieval} 
\label{sec:failure-modes-literature}

Despite these advantages, \glsxtrshort{gr} systems exhibit several distinct failure modes, behavioral biases, and structural limitations that motivate this thesis. I organize these into five categories: inference and decoding failures, memorization and representation issues, scalability limitations, dynamic corpus problems, and architectural constraints.

\paragraph{Inference and Decoding Failures}

Inference and decoding failures are primarily driven by the problem of prefix pruning, also referred to as false pruning. This occurs during autoregressive generation of document identifiers when beam search prematurely discards the prefix of a relevant document because its cumulative probability at an early step is lower than competing candidates \cite{zhang2024generativeretrievaltermset, zeng2024planningaheadgenerativeretrieval, sun2023learningtokenizegenerativeretrieval}. Once a prefix is pruned, the system cannot recover the correct document regardless of subsequent token probabilities. This failure stems from the myopic nature of autoregressive decoding, in which the model perceives only preceding tokens and lacks visibility into subsequent tokens that might contribute to a high final relevance score. While the scale of this issue depends significantly on the model architecture, empirical analyses indicate that even with large beam sizes (e.g., 1000), constrained beam search often fails to match brute-force decoding performance, suggesting that relevant \glspl{docid} are frequently eliminated during search \cite{zeng2023scalableeffectivegenerativeinformation}.

However, the challenge of prefix pruning is further compounded by a fundamental misalignment between the model’s probability distribution and retrieval adequacy. Stahlberg and Byrne (2019) demonstrate that search errors in autoregressive models often act as a paradoxical ``safety net'' for underlying model deficiencies. By utilizing an exact inference procedure, they revealed that neural machine translation models frequently assign the highest global probability to the empty string, a phenomenon they attribute to an inherent bias toward shorter sequences. In the context of \glsxtrlong{gr}, this suggests that the failure to retrieve a relevant document identifier (docid) is not merely a failure of search heuristics to find the global optimum, but rather a model failure where the global optimum itself may be a short or irrelevant prefix. While \glsxtrshort{SEAL} uses constrained beam search to prevent the ``empty string'' problem the authors found, the underlying bias that the model potentially prefers the shortest valid DocID path or the one with the most frequent initial tokens, regardless of their actual relevance to the query, may still exist. Consequently, standard beam search ``succeeds'' only insofar as the decoder does not collapse into these high-probability but low-adequacy regions of the search space.\cite{stahlberg2019nmtsearcherrorsmodel}.

Related is the problem of hallucination, in which the model generates invalid identifiers, since without constrained decoding mechanisms, generative models may produce token sequences not corresponding to any valid document identifier within the corpus \cite{massarelli2020decodingstrategiesaffectverifiability, Ji_2023}. This stems from the probabilistic nature of language models, which often prioritize coherent linguistic patterns over valid identifier strings. While constrained decoding using prefix trees or FM-indexes can force generation of valid IDs, it does not inherently solve relevance ranking and introduces computational overhead \cite{bevilacqua2022autoregressivesearchenginesgenerating, pradeep2023doesgenerativeretrievalscale}.

\paragraph{Memorization and Representation Issues}
One of the memorization and representation issues specific to the capacity of \glsxtrshort{gr} models is the apparent forgetting of fine-grained features. While generative models capture coarse-grained semantic clusters effectively, they struggle to accurately memorize and decode fine-grained document distinctions. Experimental comparisons reveal that error rates for \glsxtrshort{gr} models increase significantly at later token positions in \glsxtrshort{docid} sequences, rising from approximately 1\% at the first position to over 12\% by the sixth position. This indicates degradation in memory accuracy for detailed document features \cite{yuan2024generativedenseretrievalmemory}.

\glsxtrshort{gr} systems also suffer from ``\glsxtrshort{docid} collision'' where the mapping from document content to identifiers is not sufficiently distinct. When using synthetic queries or semantic clustering as identifiers, multiple documents may share quasi-identical or highly similar representations, causing the model to conflate distinct documents \cite{yang2023autosearchindexerendtoend}. This is exacerbated by the indexing-retrieval gap, a distribution mismatch between data used for memorization (such as long documents) and data used for retrieval (such as short queries). Because the model maps document content to \glspl{docid} during indexing but queries to \glspl{docid} during retrieval, it often fails to bridge this semantic gap without data augmentation such as synthetic query generation \cite{kuo2024surveygenerativeinformationretrieval}.

More broadly, \glsxtrshort{docid} design is consistently identified as a core challenge affecting these representation issues. Numerical \glspl{docid} offer efficiency but suffer from limited generalization and overfitting to initial training sets, while string-based \glspl{docid}, such as titles, n-grams, or pseudo-queries maintain better semantic alignment but are more susceptible to false pruning \cite{zhang2024generativeretrievaltermset, zhang2025replicationexplorationgenerativeretrieval, 10.1145/3589335.3641239}.

\paragraph{Scalability Limitations}
Scalability limitations become evident as corpus size increases, \glsxtrshort{gr} models exhibit limitations in terms of capacity \cite{nguyen2023generativeretrievaldenseretrieval, zeng2023scalableeffectivegenerativeinformation}. For example, in some models the fixed parameter budget of the transformer becomes insufficient to encode nuances of a massive corpus. Empirical studies show sharp performance decline as corpora scale from 100,000 to 8.8 million passages. For example, a T5-XL model achieves only ~30\% of the MRR@10 score on the full MS MARCO corpus compared to the base corpus, which means it significantly underperforms compared to smaller subsets \cite{pradeep2023doesgenerativeretrievalscale}. This suggests memorization capacity does not scale smoothly with corpus size, leading to the model failing to distinguish between its targets \cite{pradeep2023doesgenerativeretrievalscale, yuan2024generativedenseretrievalmemory}.

Associated with scalability is the issue that oftentimes increasing model size does not yield proportional improvements. Based on an analysis by Pradeep et.al (2023)\cite{pradeep2023doesgenerativeretrievalscale}, scaling T5 from XL (3B parameters) to XXL (11B parameters) actually degraded retrieval performance when using naive string identifiers on large corpora, dropping from 26.7 to 24.3 MRR@10 on the full MS MARCO dataset despite identical training configurations. This is a counter-intuitive result given that generative retrieval is often assumed to be limited by model capacity. Additionally, atomic identifiers (e.g., digits 0-9) assign each document a single unique output token, requiring the model's output vocabulary to equal the corpus size. Since the output projection layer has dimensions of hidden size × vocabulary size, scaling to 8.8 million documents with a hidden size of 768 adds approximately 6.8 billion parameters to the output layer alone, making this approach prohibitively expensive for large corpora. In contrast, sequential identifiers like naive string IDs reuse a fixed vocabulary across multiple decoding steps, keeping parameter count constant regardless of corpus size. \cite{kuo2024surveygenerativeinformationretrieval, pradeep2023doesgenerativeretrievalscale}.

\paragraph{Dynamic Corpus Problems}
Dynamic corpus problems arise in environments where corpora change, \glsxtrshort{gr} models exhibit two opposing failure modes \cite{Chen_2023, zhang2025replicationexplorationgenerativeretrieval, yuan2024generativedenseretrievalmemory, mehta2023dsiupdatingtransformermemory}. ``Catastrophic forgetting'' occurs when updating a model with new documents degrades its ability to retrieve previously indexed documents. Experiments demonstrate that sequential indexing of new batches can cause indexing accuracy for the initial corpus to drop by more than 25 points, necessitating retraining and increasing costs \cite{mehta2023dsiupdatingtransformermemory}. The opposing failure is excessive recency bias, where models fail to retrieve newly added content, favoring documents from the original training corpus.

\glsxtrshort{gr} models also exhibits ``implicit forgetting'' during initial training itself. Analysis of training dynamics shows models frequently ``forget'' and ``re-learn'' document-to-identifier mappings between mini-batches, with approximately 88\% of documents undergoing at least one forgetting event during training \cite{mehta2023dsiupdatingtransformermemory}.

An additional issue identified is the problem that in \glsxtrlong{gr} newly added documents are practically inaccessible until model weights are updated and the model retrained, unlike dense retrieval systems which can index new embeddings instantly without retraining the encoder \cite{kuo2024surveygenerativeinformationretrieval}.

\paragraph{Architectural Constraints and Interpretability}
Lastly, the lack of interpretability in generation processes presents challenges for understanding and debugging failures. Unlike sparse retrieval where term matching is transparent, or dense retrieval where embedding similarity can be analyzed, the internal decision processes of generative retrieval is significantly more opaque \cite{10.1145/3589335.3641239, ross2021evaluatinginterpretabilitygenerativemodels}.


\subsection{Preliminary Analysis}
\label{sec:failure-pattern-analysis}

To understand the mechanisms behind \glsxtrshort{SEAL}'s retrieval failures, we analyze the distribution of generated n-gram keys across retrieved documents. We categorize each query based on the correctness of its top-2 retrieved passages: PP (both correct), PN (rank-1 correct, rank-2 incorrect), NP (rank-1 incorrect, rank-2 correct), and NN (both incorrect) to compare n-gram generation patterns across success and failure cases.

Table~\ref{tab:category-distribution} presents the distribution of queries across retrieval outcome categories.

\begin{table}[htbp]
    \centering
    \caption{Distribution of queries across retrieval outcome categories (N=6,515).}
    \label{tab:category-distribution}
    \begin{tabular}{llrl}
        \toprule
        \textbf{Category} & \textbf{Count} & \textbf{Percentage} & \textbf{Interpretation} \\
        \midrule
        PP & 1,287 & 19.8\% & Perfect success \\
        PN & 1,427 & 21.9\% & Partial success \\
        NP & 702 & 10.8\% & Partial failure \\
        NN & 3,099 & 47.6\% & Complete failure \\
        \bottomrule
    \end{tabular}
\end{table}

Nearly half of all queries (47.6\%) result in complete retrieval failure, where neither of the top-2 ranked passages contains the answer. Combined with the 10.8\% partial failures, this indicates that 58.4\% of queries fail to surface a relevant passage at rank~1. Conversely, 41.7\% of queries achieve successful retrieval (PP~+~PN), with roughly equal proportions of perfect versus partial success.

We also examine the n-gram keys generated by \glsxtrshort{SEAL} to understand why incorrect passages outrank correct ones. For each query, we partition keys into three sets: keys appearing exclusively in positive (ground-truth) passages, keys appearing exclusively in negative passages, and keys shared by both. Table~\ref{tab:key-distribution} presents the aggregate statistics.

\begin{table}[htbp]
    \centering
    \caption{Aggregate n-gram key statistics across all queries (N=6,515).}
    \label{tab:key-distribution}
    \begin{tabular}{lr}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Avg. unique keys in positive passages & 25.9 \\
        Avg. unique keys in negative passages & 185.2 \\
        Avg. shared keys & 22.8 \\
        \midrule
        Avg. score of unique positive keys & 6.45 \\
        Avg. score of unique negative keys & 9.67 \\
        Avg. score of shared keys (in positive context) & 19.54 \\
        Avg. score of shared keys (in negative context) & 31.47 \\
        \bottomrule
    \end{tabular}
\end{table}

Two patterns emerge. First, \glsxtrshort{SEAL} generates substantially more keys matching negative passages than positive passages, in an approximately 7:1 ratio. Second, keys unique to negative passages receive higher average scores (9.67) than keys unique to positive passages (6.45). Together, these patterns indicate that \glsxtrshort{SEAL} generates n-grams that are topically relevant but not specific enough to distinguish correct passages from semantically similar incorrect ones.

The scoring differential for shared keys is particularly interesting. When the same n-gram appears in both positive and negative passages, it receives a higher score in the negative context (31.47 vs.\ 19.54). This suggests that even semantically relevant keys contribute more strongly to incorrect rankings.

Table~\ref{tab:category-comparison} disaggregates these metrics by retrieval outcome category.

Let $|K_P^u|$ denote the count of n-gram keys appearing exclusively in positive passages, 
$\bar{w}_P^u$ the mean score of those keys, and $\Sigma_P^u$ their cumulative score. 
Analogous notation applies for negative-exclusive keys ($|K_N^u|$, $\bar{w}_N^u$, $\Sigma_N^u$).

We analyze n-gram distributions across the top-10 retrieved passages per query. While outcome categories (PP, PN, NP, NN) are defined by the top-2 positions, the broader window captures patterns explaining why correct passages may be retrieved but mis-ranked (e.g. appearing at positions 3-10) versus missed entirely.

\begin{table}[htbp]
    \centering
    \caption{N-gram key statistics by retrieval outcome category ($k$=10).}
    \label{tab:category-comparison}
    \begin{tabular}{lrrrrrr}
        \toprule
        \textbf{Category} & $|K_P^u|$ & $\bar{w}_P^u$ & $\Sigma_P^u$ & $|K_N^u|$ & $\bar{w}_N^u$ & $\Sigma_N^u$ \\
        \midrule
        PP & 54.6 & 12.98 & 955 & 125.8 & 8.13 & 1,411 \\
        PN & 27.7 & 8.87 & 264 & 170.5 & 7.80 & 1,855 \\
        NP & 29.8 & 5.83 & 235 & 174.1 & 8.21 & 1,972 \\
        NN & 12.3 & 2.76 & 81 & 219.2 & 11.49 & 3,958 \\
        \bottomrule
    \end{tabular}
\end{table}


The progression from PP to NN reveals a clear pattern. In successful retrievals (PP), positive-unique keys contribute substantial cumulative evidence ($\Sigma_P^u = 955$), while the gap with negative evidence remains moderate. In complete failures (NN), positive-unique keys contribute minimal evidence ($\Sigma_P^u = 81$) while negative-unique keys dominate with $\Sigma_N^u = 3{,}958$, approximately a ratio of 49:1.

The average score column is simiarly insightful. In NN cases, not only are there more negative-unique keys (219.2 vs.\ 12.3), but each negative key scores approximately 4x higher on average ($\bar{w}_N^u = 11.49$ vs.\ $\bar{w}_P^u = 2.76$). This suggests that in failure cases, the model generates ``higher-quality'' n-grams pointing to incorrect documents, not merely more of them. In contrast, for successful retrievals (PP), positive-unique keys score higher on average than negative-unique keys ($\bar{w}_P^u = 12.98$ vs.\ $\bar{w}_N^u = 8.13$), indicating that retrieval success correlates with generating distinctive, high-scoring n-grams that specifically identify the correct passage.

We can see that \glsxtrshort{SEAL} assigns incorrect passages to top ranks because the aggregate evidence from generated n-grams overwhelmingly favors those passages. While the model does not fail to generate relevant keys, it generates more keys that match irrelevant content than keys that match relevant content, and in failure cases those keys are individually higher-scoring as well.

\paragraph{Example}

Consider the query ``who sings does he love me with reba'' (category NN). \glsxtrshort{SEAL} generated 39 keys unique to positive passages with a cumulative score of 327, compared to 125 keys unique to negative passages with a cumulative score of 2,323. Despite the query's specificity, the model assigned seven times more score pointing toward incorrect documents, resulting in complete retrieval failure.

Table~\ref{tab:reba-ngram-examples} illustrates the contrast between keys unique to positive and negative passages for this query.

\begin{table}[htbp]
    \centering
    \small
    \caption{Example n-gram keys for query ``who sings does he love me with reba'', comparing keys unique to positive (ground-truth) passages versus keys unique to negative (incorrect) passages.}
    \label{tab:reba-ngram-examples}
    \begin{tabular}{p{0.35\textwidth}rr|p{0.35\textwidth}rr}
        \toprule
        \multicolumn{3}{c|}{\textbf{Unique to Positive (39 total)}} & \multicolumn{3}{c}{\textbf{Unique to Negative (125 total)}} \\
        \textbf{N-gram} & \textbf{Freq.} & \textbf{Score} & \textbf{N-gram} & \textbf{Freq.} & \textbf{Score} \\
        \midrule
        \texttt{Reba's} & 72 & 112.8 & \texttt{</s> Linda Davis @@} & 9 & 216.3 \\
        \texttt{country music artist} & 5,249 & 44.6 & \texttt{</s> Kathy Mattea @@} & 34 & 135.4 \\
        \texttt{He Love} & 36 & 26.3 & \texttt{</s> Reba McEntire @@} & 85 & 117.5 \\
        \texttt{chart,} & 23,699 & 24.1 & \texttt{country singles} & 777 & 83.5 \\
        \texttt{single from} & 46,518 & 20.2 & \texttt{Hot Country Sing} & 3,729 & 78.5 \\
        \midrule
        \multicolumn{3}{r|}{Sum: 327} & \multicolumn{3}{r}{Sum: 2,323} \\
        \bottomrule
    \end{tabular}
\end{table}

The keys unique to negative passages reveal that despite \glsxtrshort{SEAL} generating a highly distinctive n-gram \texttt{</s> Linda Davis @@} (the correct answer, with corpus frequency of only 9 and the highest score of 216.3), this key matched passages about Linda Davis that were \textit{not} the annotated ground truth. Additionally, \glsxtrshort{SEAL} generated title identifiers for related but incorrect artists such as Kathy Mattea and Reba McEntire herself, whose passages discuss other songs and collaborations. The 50 shared keys between positive and negative passages include the partial song title \texttt{"Does He Love} (score: 196.1) and the artist name \texttt{Reba} (corpus frequency: 1,851), which match numerous passages across the country music domain.

This example demonstrates \glsxtrshort{SEAL}'s failure coming from generating too many topically relevant n-grams that collectively overwhelm the evidence pointing to the correct passage. The additive scoring amplifies the impact of quantity over precision, as the 3.2x increase between the \textit{unique to positive} and the \textit{unique to negative} n-grams matched (125 vs.\ 39) led to a 7-fold increase in cumulative score (2,323 vs.\ 327).


\subsection{Failure Taxonomy for Empirical Analysis}
\label{sec:failure-taxonomy-empirical}

The literature-derived taxonomy presented in Section~\ref{sec:failure-modes-literature} identifies numerous failure modes across generative retrieval systems. However, not all failure modes are observable from inference outputs alone. This section establishes which failure categories can be empirically analyzed given the available \glsxtrshort{SEAL} output data, and which remain outside the scope of this analysis.

\subsubsection{Available Data Structure}

The \glsxtrshort{SEAL} output dataset contains 6,515 queries from the \glsxtrlong{nq} benchmark. For each query, the following relevant information is available:

\begin{itemize}
    \item \textbf{question}: The original natural language query
    \item \textbf{answers}: Ground-truth answer string(s)
    \item \textbf{positive\_ctxs}: Ground-truth relevant passages containing the answer, with passage identifiers
    \item \textbf{ctxs}: \glsxtrshort{SEAL}'s top-100 retrieved passages, ranked by intersective scoring, each containing:
    \begin{itemize}
        \item Passage title and text
        \item Aggregate document score
        \item Passage identifier
        \item The set of matched n-gram keys
    \end{itemize}
\end{itemize}

The n-gram keys are most useful to understand \glsxtrshort{SEAL}'s retrieval decisions. Each key entry contains three parts:
\begin{enumerate}
    \item The n-gram string (e.g., \texttt{`` Reba''}, \texttt{``</s> Linda Davis @@''})
    \item The corpus frequency $F(n, R)$---how many times this n-gram appears across all documents
    \item The n-gram score $w(n, q)$ computed via the scoring function
\end{enumerate}


\subsubsection{Observable Failure Modes}

Given this data structure, three categories of failure modes can be directly analyzed:

One of the most common failure modes is when generated n-grams lack sufficient specificity to distinguish relevant documents from semantically related but incorrect alternatives. The corpus frequency $F(n, R)$ directly quantifies this specificity, as an n-gram which appears frequently across the corpus theoretically provides minimal discriminative signal.

Our empirical analysis of 6,515 queries reveals an inverse correlation between n-gram frequency and retrieval success for \glsxtrshort{SEAL}. The average top-5 n-gram frequency shows a Spearman correlation of $\rho = -0.238$ ($p < 0.001$) with top-1 success. As shown in the decile analysis, queries where \glsxtrshort{SEAL} generates highly specific n-grams (Decile 1, freq.\ range 2--91) achieve a 71.0\% top-1 success rate. Conversely, when the model relies on generic n-grams (Decile 10, freq.\ range 81k--719k), success drops to just 25.9\%.

A similar trend is observed in \glsxtrshort{MINDER}. Despite its use of synthetic identifiers (pseudo-queries) intended to provide more context, it remains sensitive to frequency bias with a negative correlation of $\rho = -0.290$ ($p < 0.001$). \glsxtrshort{MINDER} performs at a roughly similar rate as \glsxtrshort{SEAL} does, with 76.5\% success in D1 and 26.8\% in the tenth decile. This suggests that the inclusion of multiview identifiers and psuedoqueries does not reliably improve retrieval quality over non-disrimative terms. %TODO was it even expected to improve precision? MINDER’s pseudo-queries are designed for recall (bridging the vocabulary gap), but often hurt precision because they are "hallucinated" versions of what might be in the doc. Or no?

While Equation~\ref{eq:ngram-score} downweights frequent n-grams via the $P(n)$ term, if the model assigns high conditional probability $P(n|q)$ to generic n-grams, they may still dominate the final ranking. The training signal does not distinguish between highly specific terms (like unique names) and broadly applicable domain terms (like ``country music singer'') and relatively specific but non-discriminative n-grams can get assigned a non-trivial score because the model's confidence in its relevance partially overcomes the frequency penalty. %TODO review again if its the models confidence or an algorithm which doesnt have "confidence"

Likewise, if short n-grams such as unigrams and bigrams dominate the contribution (as shown in Section \ref{sec:failure-7}), this issue appears The model may prefer generating shorter sequences because shorter sequences have higher cumulative probability, as each token multiplication reduces probability, as well as because common single words have stronger learned associations in the pre-trained model. Inverse correlation between n-gram frequency and retrieval success could thus help us identify such cases.

Secondly, we can identify a failure mode in the scoring mechanism too. To isolate failures in the scoring mechanism rather than n-gram generation, cases where \glsxtrshort{SEAL} retrieves the correct passage but ranks it below incorrect alternatives are useful to examine.

For example, on the query ``when does the new my hero academia movie come out'', the ground-truth passage appears at rank~3 with score 620.5, while two other passages from the same Wikipedia article rank higher with scores 709.4 and 663.9. All passages share similar high-scoring n-grams (e.g., \texttt{``</s> My Hero Academia: Two Heroes @@''}), but non-ground-truth passages accumulated higher aggregate scores from secondary n-grams. In this case, we can check the score differentials between correct and incorrect top-ranked passages and examine which shared high-scoring n-grams are to be found across multiple retrieved passages

Third, the mismatch between query phrasing and document content may also cause the model to generate irrelevant n-grams that fail to ``bridge the gap'' between query and document. This occurs when generated n-grams do not align with how the answer is expressed in the ground-truth passage. Low lexical overlap between query terms and high-scoring n-grams is therefore an issue to deal with. Cases where n-grams capture related concepts but miss the specific answer formulation are hard for model to match. Similar problem occurs if the query requires inference to match the document text.

\subsubsection{Non-Observable Failure Modes}

Several failure modes from the literature taxonomy cannot be directly measured from the available output data:

False pruning cannot be determined in this thesis. Determining whether beam search prematurely discarded the prefix of a discriminative n-gram sequence would require access to the full beam search trajectory, including all candidate sequences considered and rejected at each decoding step. The data at hand contains only the final set of n-grams that survived beam search. We cannot distinguish between cases where (a) the model never considered a relevant n-gram, versus (b) the model considered but incorrectly pruned it during decoding.

\glsxtrshort{SEAL}'s architecture directly prevents hallucinative failure modes through constrained decoding, which prevents the model to generate invalid identifiers. At each generation step, the model can only produce tokens that extend a valid corpus substring, guaranteeing that every generated n-gram exists in at least one document. While this eliminates hallucination as a failure mode for \glsxtrshort{SEAL}, it also means we cannot study hallucination patterns, which is a limitation if we were to generalize and compare the findings with other \glsxtrshort{gr} architectures that lack constrained decoding.

Evaluating whether beam search found the optimal solution under the model's scoring function would require comparison against exhaustive search over all possible n-gram sequences. Such brute-force decoding is computationally prohibitive and not available in the output data used in this thesis.

Assessing whether failures stem from the model's limited capacity to encode a large corpus would require us to perform a comparison across varied corpus sizes. The available data represents a single corpus configuration (\glsxtrlong{nq} with approximately 21 million passages), precluding a scalability and saturation analysis.

Similarly, the previously mentioned forgetting and cold-start problems that arise in settings where the corpus is dynamic are issues that are unobservable in our dataset, as the \glsxtrlong{nq} evaluation uses a static corpus snapshot.

Non-differentiable optimization, training instability, and struggles with learning fine-grained features that might occur during model training rather than inference are another category of issues we cannot account for in this thesis. Analysis of these failures would require access to training dynamics, gradient statistics, or intermediate checkpoints not available in inference outputs.

\subsubsection{Annotation Problems in Evaluation}

Standard retrieval evaluation compares retrieved passage identifiers against annotated ground-truth passages. However, the \glsxtrlong{nq} dataset annotates only specific passages, while multiple passages in the corpus may contain the correct answer. This can create an event where \glsxtrshort{SEAL} retrieves a passage containing the answer, but this passage differs from the annotated ground-truth.

To identify such cases, we check whether the answer string appears in any top-$k$ retrieved passage. Table~\ref{tab:answer-passage-mismatch} presents the results.

\begin{table}[htbp]
    \centering
    \caption{Retrieval outcomes based on answer string matching (N=6,515, $k$=2).}
    \label{tab:answer-passage-mismatch}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Outcome} & \textbf{Count} & \textbf{Percentage} \\
        \midrule
        Ground-truth passage found & 3,416 & 52.4\% \\
        Answer string in different passage & 426 & 6.5\% \\
        Answer string not found & 2,673 & 41.0\% \\
        \bottomrule
    \end{tabular}
\end{table}


In 426 queries (6.5\%), the answer string appears in (at least) one of the top-2 retrieved passages that is not the annotated ground truth. However, string matching alone does not determine whether the passage genuinely answers the question.

Understanding these cases requires understanding how the corpus is structured. The Questions retrieval corpus consists of a Wikipedia dump split into approximately 21 million passages of exactly 100 words each \cite{karpukhin2020densepassageretrievalopendomain}. These chunks are created by word count, not by sentence boundaries, so passages frequently end mid-sentence. For example, a passage about Linda Davis ends with: ``Her highest solo chart position'', which is clearly cut off. \glsxtrshort{SEAL} retrieves from all 21 million chunks independently, it does not retrieve one chunk per article and discard the rest. If multiple chunks from the same article match the query, they can all appear in the results. This also raises the question of whether low passage diversity (for example, when most or all retrieved passages come from the same article) reflects high model confidence, or instead indicates that alternative answers are being crowded out. Accordingly, an analysis of the number of unique documents among the top-k retrieved results is warranted.
 

To understand what the 426 ``answer in different passage'' cases actually look like, we examine three examples in detail.

\textbf{Example 1: ``which apostle had a thorn in his side'' (answer: Paul).} The ground-truth passage is from the Wikipedia article ``Thorn in the flesh,'' which explains that Paul the Apostle used this phrase in 2 Corinthians 12:7--9. \glsxtrshort{SEAL}'s top result is Galatians 3, which mentions ``Paul the Apostle'' as the author of that epistle but contains nothing about a thorn. Even though \glsxtrshort{SEAL} seemed to be able to identify ``Paul'' as a relevant term (potentially from BART's pretrained parameters), Galatians 3 is the wrong chapter entirely. \glsxtrshort{SEAL} matched on generic terms (``Paul,'' ``apostle'') but never generated n-grams containing ``thorn.''

\textbf{Example 2: ``how many judges currently serve on the supreme court'' (answer: nine).} The ground-truth passage directly states the current composition of the Court. \glsxtrshort{SEAL}'s top result is a historical passage stating that in 1869, ``the Circuit Judges Act returned the number of justices to nine, where it has since remained.'' This is borderline, because the reader could infer the answer, but the passage is not specifically about the current number of judges.

\textbf{Example 3: ``who is the youngest judge currently sitting on the u.s. supreme court'' (answer: Neil Gorsuch).} The ground-truth passage discusses Gorsuch's nomination process and him being the youngest sitting justice. \glsxtrshort{SEAL}'s top result directly states: ``Neil Gorsuch is the youngest justice sitting, at 50 years of age.'' This is a success, because \glsxtrshort{SEAL} found a passage that answers the question, even if not annotated.

These examples show that the 426 cases are a mix of such cases. To determine how many fall into each category, manual review or LLM-based classification is required. Section~\ref{sec:llm-judge} applies LLM-as-a-judge to classify whether each retrieved passage genuinely answers the question or not. If most of the 426 cases are genuine retrievals, then \glsxtrshort{SEAL}'s effective top-2 recall is closer to 59.0\% rather than 52.4\%. If most are coincidental matches, then 52.4\% remains the accurate figure.

\subsubsection{LLM-as-a-Judge Classification}
\label{sec:llm-judge}

To disambiguate the 426 cases where the answer string appears in a retrieved passage that differs from the annotated ground truth, we employ an LLM-as-a-judge approach. For each case, we present a large language model (\texttt{claude-sonnet-4-5-20250929}) with the original question, the expected answer, and the retrieved passage text, asking it to determine whether the passage genuinely answers the question or whether the answer string's presence is coincidental. The model was chosen based on expected reasoning capabilities and cost of operation.

The judge classifies each case into one of three categories:
\begin{itemize}
    \item \textbf{YES}: The passage directly and unambiguously answers the question.
    \item \textbf{NO}: The answer string appears coincidentally and the passage does not answer the question.
    \item \textbf{PARTIAL}: The passage provides relevant information from which the answer could be inferred, but does not state it directly.
\end{itemize}

Table~\ref{tab:llm-judge-results} presents the classification results.

\begin{table}[htbp]
    \centering
    \caption{LLM-as-a-judge classification of answer-in-different-passage cases (N=426).}
    \label{tab:llm-judge-results}
    \begin{tabular}{lrrl}
        \toprule
        \textbf{Verdict} & \textbf{Count} & \textbf{Percentage}\\
        \midrule
        YES & 208 & 48.8\% \\
        NO & 116 & 27.2\% \\
        PARTIAL & 102 & 23.9\% \\
        \bottomrule
    \end{tabular}
\end{table}

The results reveal that nearly half (48.8\%) of these ambiguous cases represent genuine retrieval successes where \glsxtrshort{SEAL} retrieved a valid answer-bearing passage that simply differs from the annotated ground truth passage(s). The 116 cases classified as NO (27.2\%) represent true coincidental matches where the answer string appears, but in an unrelated context, as exemplified by Example 1. The 102 borderline cases (23.9\%) typically involve passages that provide sufficient context for inference but lack explicit statements. To provide an additional example for the latter, for ``who made the first telephone in the world'' (expected answer: Alexander Graham Bell), the retrieved passage states ``The first telephone was invented by Antonio Meucci, but Alexander Graham Bell is credited with the development of the first practical telephone.'' This passage contains the answer but complicates the question's premise by distinguishing between ``first'' and ``first practical'' telephone.

Accordingly, we adjust the estimate of \glsxtrshort{SEAL}'s retrieval performance. Table~\ref{tab:adjusted-recall} presents the adjusted top-2 recall under different inclusion criteria.

\begin{table}[htbp]
    \centering
    \caption{Adjusted top-2 recall estimates based on LLM-as-a-judge classification.}
    \label{tab:adjusted-recall}
    \begin{tabular}{lrr}
        \toprule
        \textbf{Inclusion Criterion} & \textbf{Successful Retrievals} & \textbf{Recall} \\
        \midrule
        Ground-truth passage only (baseline) & 3,416 & 52.4\% \\
        + YES verdicts & 3,624 & 55.6\% \\
        + YES and PARTIAL verdicts & 3,726 & 57.2\% \\
        \bottomrule
    \end{tabular}
\end{table}

When counting only unambiguous successes (YES verdicts), \glsxtrshort{SEAL}'s effective top-2 recall increases from 52.4\% to 55.6\%, a 3.2 percentage point improvement. Including borderline cases (YES + PARTIAL) raises the estimate to 57.2\%. These adjustments suggest that \glsxtrshort{SEAL}'s true retrieval capability may have been underestimated by approximately 3--5 percentage points due to incomplete or inaccurate ground-truth annotations.

\subsubsection{Limitations of Data}
\label{sec:datalims}

A fundamental structural constraint of n-gram-based retrieval is that n-grams must be strictly contiguous substrings within the corpus. \glsxtrshort{SEAL}'s FM-Index ensures that any generated n-gram exists as a continuous sequence in at least one document \cite{bevilacqua2022autoregressivesearchenginesgenerating}. While such constrained decoding eliminates hallucination of invalid \glspl{docid}, the issue persists that the model cannot match n-grams that would semantically connect a query to a document if the relevant terms, which are separated by intervening words, would be connected.

Consider a passage stating ``Tom likes pizza and tomatoes'' and a query ``does Tom like tomatoes?'' The semantically relevant n-gram ``likes tomatoes'' does not exist as a contiguous substring in the passage due to the intervening words ``pizza'' and ``and.'' The FM-Index would theoretically reject or fail to map this n-gram to the document, even though the passage clearly contains both ``like'' and ``tomatoes'' in a relevant context.  During \glsxtrshort{docid} generation, the model may falsely prune prefixes from the beam search before it reaches the discriminative keywords. This false pruning occurs because the model generates tokens left-to-right and cannot recover from early missteps once a prefix is discarded from the search beam, as discussed before.

This issue is partially mitigated by the fact that n-grams that do exist contiguously in the passage are still generated, such as ``and tomatoes,'' ``pizza and tomatoes,'' or simply ``tomatoes.'' \glsxtrshort{SEAL} uses intersective scoring that aggregates scores from multiple generated n-grams, so even if the model cannot generate a specific multi-word n-gram due to intervening text, it can still retrieve the document by generating multiple shorter n-grams (such as separate unigrams or bigrams) that collectively identify the passage. 

Additionally, the \glsxtrshort{SEAL} framework is capable of rephrasing the question into semantically similar queries. The paper gives an example, where the query ``can you predict earthquakes'' successfully generates the n-gram ``earthquakes can be predicted'' \cite{bevilacqua2022autoregressivesearchenginesgenerating}. It is worth noting that \glsxtrshort{SEAL} is only capable of generating ngrams that exist in the corpus. ``earthquakes can be predicted'' exists in a passage in the corpus in the sentence ``Podcast discussing why the claim that earthquakes can be predicted is false.''

\textbf{TODO} This limitation affects the interpretation of failure cases in the empirical analysis, because when \glsxtrshort{SEAL} fails to retrieve a relevant passage, the failure may stem not from the absence of relevant terms in the document, but from the model's inability to generate a valid n-gram sequence that connects the query phrasing to the specific word ordering in the passage. Therefore we must examine whether alternative ngrams exist that could've plausibly identified the document. Additionally, using skip-grams instead of ngrams could theoretically be an alternate solution to the problem. That is, however, out of scope for this thesis.

Based on the observable failure modes, Table~\ref{tab:failure-classification} defines the classification scheme for empirical analysis.


\begin{table}[H]
    \centering
    \begin{threeparttable}
        \caption{Failure classification framework for \glsxtrshort{SEAL} output analysis (updated).}
        \label{tab:failure-classification}
        \begin{tabular}{p{0.25\textwidth} p{0.4\textwidth} p{0.4\textwidth}}
            \toprule
            \textbf{Failure Category} & \textbf{Definition} & \textbf{Observable Indicator} \\
            \midrule
            Low Lexical Specificity & 
            Top-scoring n-grams common across many passages, reducing discriminative power & 
            Avg. corpus frequency of top n-grams, \% of n-grams considered high-frequency \\
            \addlinespace
            Misranking Despite Retrieval\tnote{1} & 
            Correct passage is retrieved but misranked below incorrect passages & 
            Ground-truth passage appears in top-$K$ but not rank 1 \\
            \addlinespace
            Insufficient Query Coverage & 
            Generated n-grams fail to sufficiently cover query terms & 
            Proportion of unique query tokens present in generated n-grams \\
            \addlinespace
            Direct vs. Indirect Identification & 
            Generated n-grams do not contain the answer string & 
            Fraction of queries where at least one top-1 passage n-gram contains the answer string \\
            \addlinespace
            Correlated identifier generation & 
            Multiple n-grams share overlapping tokens, creating inflated scoring & 
            Ratio of unique tokens to total tokens across all n-grams \\
            \addlinespace
            Overreliance on a Single Identifier & 
            Total passage score is concentrated in one high-scoring n-gram & 
            max(n-gram score) / sum(n-gram scores) per passage \\
            \addlinespace
            Bias Toward Short Identifiers & 
            Generated n-grams skew toward short sequences (such as unigrams) rather than longer, distinctive n-grams & 
            Fraction of unigrams among generated n-grams \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item[1] The category \emph{Misranking Despite Retrieval} differs from the other failure modes in that it reflects a deficiency in the post-generation scoring mechanism rather than in \glsxtrshort{SEAL}'s n-gram generation, which can occur both as a downstream problem from generation quality and independently due to limitations in the scoring mechanism. All other categories represent failures originating from the generation of n-grams.
        \end{tablenotes}
    \end{threeparttable}
\end{table}

It is worth noting that not all retrieval errors reflect deficiencies in SEAL's generation or scoring components. Two identified external factors contribute to apparent failures independent of model quality: (1) alternative passages containing the correct answer could exist but are not labeled as ground truth (affecting 5.7\% of analyzed queries, as shown in Table~\ref{tab:answer-passage-mismatch}); (2) cases where the 100-word passage boundaries split relevant content mid-sentence or separate related information across multiple chunks. These factors directly impact measurable retrieval performance independent of SEAL's design choices.

\section{Empirical Results \& Analysis}


\subsection{Quantitative Overview}

Table~\ref{tab:failure-indicators} defines the observable indicator used for each failure mode. The Spearman $\rho$ values in Table~\ref{tab:failure-summary} represent the correlation between each indicator and binary retrieval success (1 if the top-1 passage is correct, 0 otherwise).

\begin{table}[H]
    \centering
    \caption{Observable indicators for each failure mode and interpretation of correlation direction.}
    \label{tab:failure-indicators}
    \begin{tabular}{p{0.22\textwidth} p{0.32\textwidth} p{0.38\textwidth}}
        \toprule
        \textbf{Failure Mode} & \textbf{Indicator Measured} & \textbf{Interpretation of $\rho$} \\
        \midrule
        Bias Toward Short Identifiers & Fraction of n-grams that are unigrams & $\rho < 0$: More unigrams $\rightarrow$ lower success \\
        \addlinespace
        Insufficient Query Coverage & Proportion of query tokens covered by n-grams & $\rho > 0$: Higher coverage $\rightarrow$ higher success \\
        \addlinespace
        Answer in n-grams & Whether answer string appears in any n-gram & Binary comparison only \\
        \addlinespace
        High-freq.\ n-grams & Avg.\ corpus frequency of top-5 n-grams & $\rho < 0$: Higher frequency $\rightarrow$ lower success \\
        \addlinespace
        Correlated identifier generation & Token diversity ratio (unique / total tokens) & $\rho < 0$: Higher diversity $\rightarrow$ lower success\textsuperscript{\dag} \\
        \addlinespace
        Overreliance on a Single Identifier & max(score) / sum(scores) for top-1 passage & $\rho < 0$: More concentrated $\rightarrow$ lower success \\
        \bottomrule
    \end{tabular}

    \vspace{0.5em}
    \footnotesize
    \textsuperscript{\dag} Counter-intuitive: repetition (low diversity) correlates with \textit{higher} success.
\end{table}

Table~\ref{tab:failure-summary} consolidates the empirical findings across all analyzed failure modes, quantifying their correlation with retrieval success and the performance differential between favorable and unfavorable conditions.

\begin{table}[H]
     \centering
     \caption{Correlation between measured indicators and retrieval success for \glsxtrshort{SEAL} on \glsxtrlong{nq} (N=6,515).}
     \label{tab:failure-summary}
     \begin{tabular}{p{0.25\textwidth} r p{0.35\textwidth} r}
         \toprule
         \textbf{Indicator} & \textbf{Spearman $\rho$} & \textbf{Success Rate} & \textbf{$\Delta$} \\
         \midrule
         Identifier Length & $-0.278$ & $>30\%$ multi-word: 80.8\% vs.\ $<10\%$: 27.5\% & 53.3 pp \\
        \addlinespace
        Query Token Overlap & $+0.290$ & High overlap ($>60\%$): 60.2\% vs.\ Low ($<30\%$): 20.2\% & 40.0 pp \\
     \addlinespace
     Answer String Presence & --- & Included: 75.0\% vs.\ Not included: 34.9\% & 40.1 pp \\
     \addlinespace
     Corpus Frequency & $-0.238$ & Low freq ($<500$): 59.8\% vs.\ High ($>5000$): 32.5\% & 27.3 pp \\
     \addlinespace
     Token Diversity\textsuperscript{\dag} & $-0.227$ & Low diversity: 55.3\% vs.\ High: 35.5\% & 19.8 pp \\
     \addlinespace
     Score Concentration & $-0.115$ & Distributed: 48.5\% vs.\ Concentrated: 29.4\% & 19.1 pp \\
     \addlinespace
     Retrieval-Ranking Gap & --- & 55.0\% of retrieved passages ranked $>1$ & --- \\       \bottomrule
 \end{tabular}

 \vspace{0.5em}
 \footnotesize
 \textsuperscript{\dag} Diversity is measured as the ratio of unique tokens to total tokens. \\
 pp = percentage points. $\Delta$ = difference in precision\@1 between the compared cohorts.
\end{table}

The strongest predictors of retrieval failure are bias toward short identifiers ($\rho = -0.278$) and low Insufficient Query Coverage ($\rho = +0.290$). When \glsxtrshort{SEAL} generates predominantly unigrams, success drops from 80.8\% to 27.5\%; when n-grams fail to cover query terms, success drops from 60.2\% to 20.2\%. High-frequency n-grams and Overreliance on a Single Identifier show weaker but still significant effects.

Notably, correlated identifier generation exhibits a counter-intuitive pattern: queries with more token repetition across n-grams achieve \textit{higher} success rates, suggesting that concentrated topical focus benefits retrieval. This finding indicates that not all initially hypothesized failure modes function as expected.

\subsection{Generative Failures}

\subsubsection{Low Lexical Specificity}
\label{sec:failure-1}

N-gram corpus frequency reflects how distinctive a generated n-gram is. Rare n-grams provide stronger evidence for specific documents, while common n-grams match many topically-related passages. We analyze the average corpus frequency of the top-scored n-grams (ranked by \glsxtrshort{SEAL}'s scoring function) to determine whether high-frequency n-grams contribute to retrieval failures.

Across 6,515 queries, the average mean corpus frequency of the top-5 scored n-grams is 28753. When stratified by frequency, for queries whose rank-1 retrieved passage has an average corpus frequency of its top-5 scored n-grams below 500, 59.8\% of those queries contain at least one positive (ground truth) passage (somewhere) in the top-1, 69.2\% in the top-2, 88.3\% in the top-10 retrieved results. Interestingly, for queries whose rank-1 retrieved passage has an average corpus frequency of its top-5 scored n-grams over 5000, 32.5\% of those queries contain at least one positive (ground truth) passage (somewhere) in the top-1, 43.7\% in the top-2, 70.1\% in the top-10 retrieved results. Please note thresholds like 500 and 5000 are arbitrary and can be freely adjusted. We can observe a significant drop in retrieval quality when the lexical uniqueness of the top scoring n-grams increase. Across all queries, the average corpus frequency of the top-5 scored n-grams in the rank-1 passage is weakly negatively correlated with precision\@1 (Spearman $\rho = -0.238$, $p < 0.001$).

This finding confirms that when \glsxtrshort{SEAL} generates high-frequency n-grams as its top-ranked identifiers, these n-grams match many topically-related passages, diluting discriminative power. In queries where top-scored n-grams has very high corpus frequency, the model struggles to distinguish the target passage from hundreds of similar alternatives. While \glsxtrshort{SEAL}'s scoring function theoretically downweights common n-grams through the $P(n)$ term in Equation~\ref{eq:ngram-score}, this analysis reveals that high-frequency n-grams still frequently dominate the top-ranked positions, suggesting that the conditional probability $P(n|q)$ assigned by the language model can override frequency-based penalties when the model strongly associates common terms with the query.

To illustrate when high $P(n|q)$ occurs for generic n-grams, consider the query ``who sings does he love me with reba.'' The model assigns high conditional probability to \texttt{`` country music singer''} (corpus frequency: 4,024, score: 39.20) because the query's semantic content (such as mentioning ``Reba'', a country artist, and ``sings'') strongly activates associations with the country music domain. During training, \glsxtrshort{SEAL} learned that passages answering questions about country artists frequently contain such phrases. The BART language model's pre-trained knowledge further reinforces this, as it associates ``Reba'' with ``country music.'' Despite the frequency penalty in Equation~\ref{eq:ngram-score}, the conditional probability is sufficiently high that this generic n-gram still contributes meaningfully to the ranking. Because the n-gram appears in over 4,000 passages discussing various country artists unrelated to the specific song, it matches many incorrect documents and dilutes the retrieval quality.

\subsubsection{Bias Toward Short Identifiers}
\label{sec:failure-7}

N-gram length bias examines whether \glsxtrshort{SEAL} disproportionately generates short n-grams (unigrams and bigrams) rather than longer, more distinctive multi-word sequences. Longer n-grams generally provide stronger discriminative power, as multi-word phrases are far less likely to appear across multiple passages than individual words. For example, the unigram ``president'' may occur in thousands of Wikipedia passages, whereas the 4-gram ``youngest president in history'' identifies a far more specific set of passages.

We quantify this bias by measuring the fraction of generated n-grams that consist of a single token in the top-1 retrieved passage. Across 6,515 queries, the mean unigram fraction is 0.869 (median 0.878), and the average n-gram length is 1.28 tokens (median 1.24), indicating that, on average, nearly 87\% of generated n-grams are unigrams. This indicates a strong skew toward short n-grams despite \glsxtrshort{SEAL}’s ability to generate longer sequences.

We further stratify queries into fixed bins based on the unigram fraction. Queries in the low unigram fraction bin (<0.7) achieve a success rate of 80.8\%, compared to 48.3\% for the medium bin (0.7–0.9) and 27.5\% for the high bin (>0.9). This indicates that passages with a higher proportion of multi-word n-grams are substantially more likely to retrieve the correct document, while passages dominated by unigrams correspond to poorer retrieval performance. Interestingly, only 130 queries fit into the low unigram fraction bin, 4103 into the medium, and 2282 into the high fraction bin, consistent with the high mean unigram fraction of 0.869.

Decile-based binning of the unigram fraction similarly shows the success rate decline as the unigram fraction increases, from 67.7\% in the lowest decile (0.51–0.78) to 22.6\% in the highest decile (0.94–1.0). The Spearman correlation $\rho$ between unigram fraction and precision\@1 is $-0.278 \space (p<0.001)$, indicating a statistically significant, but moderate negative relationship between unigram dominance and retrieval effectiveness.

The graph below shows the success rate of the top-1 retrieved passage mapped with the mean fraction of unigrams among the generated n-grams for each query, grouped by decile bins. Higher unigram fractions correlate with lower retrieval success, with success rates declining from 67.7\% in the lowest decile to 22.6\% in the highest decile ($\rho = -0.278$, $p < 0.001$)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/unigram_line_plot.png}
    \caption{precision\@1 rate by mean fraction of unigrams among n-grams generated (decile bins)}
\end{figure}

\subsubsection{Semantic Convergence vs. Identifier Diversity}
\label{sec:failure-5}

We originally hypothesized that cases where the model produces semantically redundant n-grams with high token overlap would constitute a failure mode by inflating scores without providing new evidence. To test this, we measure a ratio defined as the number of unique tokens divided by the total number of tokens across all generated n-grams for the top-1 retireved query, which we will refer to as ``diversity''.

Contrary to our initial hypothesis, the results reveal a negative correlation between diversity and retrieval success. For \glsxtrshort{SEAL}, we observe a Spearman correlation of $\rho = -0.227$ ($p < 10^{-76}$), where queries with most repetitive n-grams (Decile 1, diversity 0.50--0.78) achieved a 59.1\% success rate, compared to just 25.7\% for the most diverse queries (Decile 10, diversity 0.98--1.00).

\begin{table}[htbp]
    \centering
    \caption{Top-1 retrieval success rate by token diversity deciles (unique / total tokens).}
    \label{tab:token-diversity}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}} (Mean: 0.88)} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}} (Mean: 0.79)} \\
        \textbf{Decile} & \textbf{Div.\ Range} & \textbf{Success} & \textbf{Div.\ Range} & \textbf{Success} \\
        \midrule
        D1  & 0.50--0.78 & 59.1\% & 0.30--0.58 & 70.8\% \\
        D2  & 0.78--0.82 & 55.1\% & 0.58--0.67 & 60.9\% \\
        D3  & 0.82--0.85 & 52.2\% & 0.67--0.74 & 53.9\% \\
        D5  & 0.87--0.89 & 43.2\% & 0.79--0.83 & 51.5\% \\
        D8  & 0.93--0.95 & 31.3\% & 0.89--0.92 & 34.0\% \\
        D10 & 0.98--1.00 & 25.7\% & 0.95--1.00 & 20.6\% \\
        \bottomrule
    \end{tabular}
\end{table}

This trend is also visible in \glsxtrshort{MINDER} ($\rho = -0.279$, $p < 0.001$), which exhibits a lower mean diversity of 0.79 compared to \glsxtrshort{SEAL}'s 0.88. This is expected, as \glsxtrshort{MINDER}'s generates titles, pseudo-queries, and substrings simultaneously, which indicates that the model frequently repeats key tokens across these different identifier views. The highest success rate (70.8\%) is found in the lowest diversity decile, which suggests that the model is able to reinforce a specific semantic cluster through multiview identifiers.

These findings suggest that token repetition is not a redundancy failure, but rather a signal of model certainty. When \glsxtrshort{SEAL} or \glsxtrshort{MINDER} is highly confident in a target document, it generates multiple overlapping n-grams that concentrate score on a specific semantic cluster. In contrast, high identifier diversity often indicates a lack of model confidence, where the model produces a wide variety of topically related but lexically distinct tokens that fail to converge.


\subsubsection{Insufficient Query Coverage}
\label{sec:failure-3}

Query-to-n-gram overlap measures the lexical alignment between query terms and the n-grams generated by \glsxtrshort{SEAL}. High overlap indicates that the model successfully generated n-grams containing query-relevant terms, while low overlap suggests a mismatch where generated n-grams, though potentially topically related, do not directly address the query's specific terms. We quantify this using a coverage metric, where the proportion of unique query tokens that appear in any generated n-gram.

Our analysis across 6,515 queries shows that the token-level lexical overlap varies substantially across the top-1 retrieved passages. As shown in Table~\ref{tab:query-coverage}, there is a positive correlation between lexical overlap and retrieval success (Spearman $\rho = 0.290$, $p < 0.001$). Queries in the lowest coverage decile (D1, 0.00--0.22) achieve a success rate of only 16.3\%, whereas those in the highest decile (D9, 0.69--1.00) reach 67.9\%.

\begin{table}[htbp]
    \centering
    \caption{Top-1 retrieval success rate by query-to-n-gram coverage deciles.}
    \label{tab:query-coverage}
    \begin{tabular}{llrr}
        \toprule
        \textbf{Decile} & \textbf{Coverage Range} & \textbf{Success@1} & \textbf{Count} \\
        \midrule
        D1 & 0.000--0.222 & 16.3\% & 676 \\
        D2 & 0.231--0.273 & 23.5\% & 638 \\
        D3 & 0.286--0.375 & 34.8\% & 1,338 \\
        D4 & 0.385--0.444 & 42.4\% & 681 \\
        D5 & 0.455--0.500 & 46.3\% & 1,099 \\
        D6 & 0.533--0.556 & 48.8\% & 389 \\
        D7 & 0.562--0.625 & 51.1\% & 791 \\
        D8 & 0.636--0.667 & 62.3\% & 284 \\
        D9 & 0.692--1.000 & 67.9\% & 619 \\
        \bottomrule
    \end{tabular}
\end{table}
%TODO WHY 9 bins and not 10, recheck code.
The distribution of coverage reveals that approximately 50\% of queries fall into the D3--D5 range (0.28--0.50 overlap), indicating that for a typical query, \glsxtrshort{SEAL} matches fewer than half of the unique query tokens. Low query coverage typically arises when the model may generate semantically related but lexically distinct n-grams (e.g., for a query containing ``automobile,'' the model could generate n-grams with ``car'') or when the model generates generic topical n-grams that lack the specific query terms needed to isolate the answer (e.g., a query about ``Victorian architecture in London'' generates only ``architecture'' and ``London''). Intended deciles were calculated, but due to tied values at decile boundaries, only nine bins were realized for top-1 coverage. All reported ranges and success rates correspond to these nine bins.


\subsection{Scoring Failures}
\subsubsection{Misranking Despite Retrieval}
\label{sec:failure-2}

Misranking Despite Retrieval occurs when \glsxtrshort{SEAL} successfully retrieves the ground-truth passage within its top-$K$ candidates but assigns it a lower score than incorrect alternatives, causing it to be mis-ranked. This failure mode aims to isolate deficiencies in the scoring mechanism independent of generation quality, where the model has generated sufficient n-grams to match the correct passage, but the intersective scoring function $W(d, q)$ (Equation~\ref{eq:doc-score}) fails to rank it first.

Of 6,515 queries analyzed, the ground-truth passage appears in the top-100 retrieved results for 6,027 queries (92.5\%). However, among these, in 3,313 queries (55.0\%) the ground truth is ranked below position 1. The average rank of the first ground-truth passage retrieved is 7.7, with a median of 2, indicating that while \glsxtrshort{SEAL} often places the correct answer in the top-2 or top-3 positions, precise ranking remains challenging.

Among the 6,027 queries, 2,714 (45.0\%) rank the ground truth at position 1, 702 (11.6\%) at position 2, and 449 (7.4\%) at position 3. The fact that 55.0\% of successfully retrieved passages are found but mis-ranked suggests that \glsxtrshort{SEAL}'s generation component can identify relevant passages through n-gram matching, but the scoring function commonly misjudges their relative importance.

\subsubsection{Overreliance on a Single Identifier} %maybe change to top-5? or idk, gotta think through this
\label{sec:failure-6}

Overreliance on a single identifier measures the extent to which the top-1 ranked passage's total score is concentrated in one high-scoring n-gram versus distributed across multiple n-grams. We define this as the ratio of the highest scored n-gram score and the sum of all matched n-gram scores for the passage, which we will refer to as ``dominance''.

Our analysis across 6,515 queries reveals that lower such ratio correlates with higher retrieval success, as shown in Table~\ref{tab:identifier-dominance}. For \glsxtrshort{SEAL}, we observe a Spearman correlation of $\rho = -0.115$ ($p < 0.001$), with success rates declining from 48.6\% in the lowest dominance decile (D1) to 29.6\% in the highest decile (D10).

\begin{table}[htbp]
    \centering
    \caption{Top-1 retrieval success rate by top n-gram dominance deciles (max score / sum scores).}
    \label{tab:identifier-dominance}
    \begin{tabular}{l|rr|rr}
        \toprule
        & \multicolumn{2}{c|}{\textbf{\glsxtrshort{SEAL}} (Mean: 0.44)} & \multicolumn{2}{c}{\textbf{\glsxtrshort{MINDER}} (Mean: 0.36)} \\
        \textbf{Decile} & \textbf{Dom.\ Range} & \textbf{Success} & \textbf{Dom.\ Range} & \textbf{Success} \\
        \midrule
        D1  & 0.12--0.30 & 48.6\% & 0.12--0.25 & 58.0\% \\
        D2  & 0.30--0.35 & 45.8\% & 0.25--0.28 & 59.3\% \\
        D3  & 0.35--0.38 & 45.4\% & 0.28--0.31 & 55.8\% \\
        D5  & 0.41--0.44 & 44.3\% & 0.33--0.35 & 45.1\% \\
        D8  & 0.50--0.53 & 35.4\% & 0.40--0.43 & 40.2\% \\
        D10 & 0.57--0.85 & 29.6\% & 0.48--0.94 & 32.5\% \\
        \bottomrule
    \end{tabular}
\end{table}

This trend is also noticable in \glsxtrshort{MINDER} ($\rho = -0.169$, $p < 0.001$), which also exhibits a lower mean ratio of 0.36 compared to \glsxtrshort{SEAL}'s 0.44. This shift toward more distributed evidence is assumed to be a consequence of \glsxtrshort{MINDER}'s multiview identifiers, since by aggregating scores from titles, pseudo-queries, and substrings, \glsxtrshort{MINDER} theoretically reduces reliance on any single identifier.

This finding aligns with the intersective scoring design, which benefits from distributed evidence by providing redundant confirmation across multiple n-grams. %TODO SOURCE
Conversely, if the top-scoring n-gram matches many passages or identifies the wrong one, the remaining low-scoring n-grams may provide insufficient corrective evidence to ensure correct ranking.

\subsubsection{Impact of N-gram Quantity on Retrieval Success}

While not a failure mode, given that \glsxtrshort{SEAL}'s scoring mechanism aggregates evidence from multiple n-grams, it is essential to examine whether the quantity of matched n-grams correlates with retrieval success. Intuitively, a higher number of matched n-grams should provide stronger cumulative evidence for a passage, considering that SEAL's score is sum-based, and therefore a passage with more matched n-grams has a mathematical advantage over a passage with fewer matched n-grams. However, excessive generation may also introduce noise if additional n-grams are redundant or non-discriminative.

Across 6,515 queries, the number of n-grams matched to the top-1 retrieved passage has a mean of 41.5 (median: 42.0, $\sigma$: 6.3), with a range of 11 to 65. Table~\ref{tab:ngram-count} presents retrieval performance stratified by n-gram count deciles.

\begin{table}[htbp]
    \centering
    \caption{Retrieval success rate and mean n-gram corpus frequency ($\bar{F}_c$) by n-gram count deciles for the top-1 passage.}
    \label{tab:ngram-count}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Decile} & \textbf{Count Range} & \textbf{Queries} & \textbf{Success@1} & \textbf{Mean $\bar{F}_c$} \\
        \midrule
        D1  & 11--34 & 828  & 36.0\% & 384,785 \\
        D2  & 35--36 & 495  & 37.4\% & 378,346 \\
        D3  & 37--39 & 1,017 & 39.5\% & 380,174 \\
        D4  & 40--40 & 425  & 46.1\% & 392,012 \\
        D5  & 41--42 & 861  & 45.2\% & 382,931 \\
        D6  & 43--43 & 448  & 43.8\% & 382,506 \\
        D7  & 44--45 & 795  & 45.0\% & 385,779 \\
        D8  & 46--47 & 622  & 42.4\% & 382,767 \\
        D9  & 48--49 & 432  & 41.7\% & 380,999 \\
        D10 & 50--65 & 592  & 41.6\% & 371,719 \\
        \bottomrule
    \end{tabular}
\end{table}

The relationship between n-gram count and retrieval success is notably weak (Spearman $\rho = 0.040$, $p = 0.001$), exhibiting a non-monotonic, inverted-U pattern. Precision\@1 increases from 36.0\% for passages with low identifier counts (D1, $<$34) to a peak of 46.1\% in the fourth decile, subsequently plateauing and slightly declining to 41.6\% for passages in the highest decile (D10, $>$50). This trend suggests that while a minimum amount of evidence is required to identify a passage, accumulating n-grams beyond a moderate threshold yields diminishing returns and does not significantly influence precision\@1.

Here it is important to note that because the document score is computed as a summation of individual n-gram weights without normalization for quantity, in high-count scenarios, a passage may be ranked first not due to high precision, but because it contains a larger volume of moderately relevant vocabulary that the model associates with the query context. Notably, the mean n-gram corpus frequency ($\bar{F}_c$) remains stable across all deciles. This stability indicates that the performance plateau is not caused by the model reverting to generic "filler" tokens at higher counts. Instead, the lackluster retrieval quality likely stems from the scoring mechanism's inability to distinguish between a small set of highly discriminative n-grams and a large aggregate score from multiple, less specific identifiers.

\subsubsection{Pseudo-Query Contribution in \glsxtrshort{MINDER}}

In addition to examining \glsxtrshort{SEAL}'s n-gram distribution, we analyze the contribution of synthetic identifiers in the \glsxtrshort{MINDER} framework. As detailed in Section \ref{sec:minderarch}, \glsxtrshort{MINDER} incorporates pseudo-queries alongside titles and substrings to capture high-level semantic information. Our analysis of 6,515 queries reveals an interesting divergence between the quantity of generated pseudo-queries and their influence on the final ranking.

Numerically, pseudo-queries constitute a negligible fraction of the generated identifiers, accounting for only 1.4\% of the total n-grams (379,080 out of 26.9 million). However, their contribution to the total accumulated score is disproportionately high, representing 27.7\% of the total score mass (91.1 million out of 328.5 million). This confirms that \glsxtrshort{MINDER}'s scoring mechanism successfully uses sparse but highly discriminative synthetic identifiers. %TODO recheck numbers

\subsection{Direct vs. Indirect Document Identification}
\label{sec:failure-4}

We analyze whether retrieval success depends on the model's ability to generate the specific answer string as an identifier, or whether it relies on the surrounding context. While generating the answer string provides a valuable anchor for a passage, the primary goal of \glsxtrshort{SEAL} is document identification, not direct question answering.

Of 6,515 queries, the answer string appears in at least one generated n-gram for the top-1 retrieved passage in only 1,096 case (16.8\%). When this ``direct identification'' occurs, the success rate is exceptionally high at 75.0\%. However, in the vast majority of cases (83.2\%), the model's top-1 retrieved passage the answer string is never generated. In these cases, the success rate is 34.9\%.

This finding suggests that while direct identification is a much stronger predictor of success ($\Delta = 40.1$ pp), the model is able to identify the semantic environment in which an answer is likely to be in. For instance, for a query about a Supreme Court justice, the model may successfully retrieve the correct passage by generating contextual n-grams like ``Supreme Court,'' ``nominated by,'' or ``associate justice,'' even if it never generates the justice's name itself.  However, the performance gap between the two cases suggests that when the model fails to generate the specific answer as an identifier, the remaining contextual cues are oftentimes too generic to distinguish the correct passage from topically similar alternatives.


\section{Discussion}

\textbf{TODO}

\subsection{Key Findings}

The empirical analysis of \glsxtrshort{SEAL}'s retrieval outputs reveals several findings about the failure modes of n-gram-based generative retrieval systems and about SEAL itself.

\paragraph{Generation Quality Dominates Retrieval Success}
The strongest predictors of retrieval failure originate in the n-gram generation component rather than the scoring mechanism. Bias toward short identifiers ($\rho = -0.278$) and insufficient n-gram query coverage ($\rho = +0.290$) exhibit the largest effect sizes, with success rate differentials of 53.3 and 40.0 percentage points respectively between favorable and unfavorable conditions (Table~\ref{tab:failure-summary}). When \glsxtrshort{SEAL} generates predominantly unigrams (>90\% of n-grams), retrieval success drops to 27.5\%, compared to 80.8\% when multi-word n-grams comprise a larger share. This suggests that the autoregressive language model's tendency to favor short, high-probability sequences directly undermines retrieval effectiveness, as it diminishes discriminative power.

\paragraph{Specificity-Relevance}
The analysis reveals a fundamental tension between generating topically relevant n-grams and generating sufficiently specific ones. \glsxtrshort{SEAL} consistently generates more n-grams matching incorrect passages than correct ones (approximately 7:1 ratio, Table~\ref{tab:key-distribution}), and in failure cases, these negative-unique n-grams score higher on average ($\bar{w}_N^u = 11.49$ in PP vs.\ $\bar{w}_P^u = 2.76$ in NN cases). The model successfully identifies the semantic domain of a query but fails to generate n-grams distinctive enough to isolate the specific answer-bearing passage from semantically similar alternatives. We exemplified this by a query about a country music artist, where domain-typical phrases like ``country music singer'' receive high conditional probability despite appearing in thousands of passages.

\paragraph{Scoring Mechanism Limitations}
While generation quality is the primary determinant of success, the scoring mechanism exhibits independent failure modes. Among queries where the ground-truth passage appears in the top-100 results, 55.0\% rank it below position 1, indicating that \glsxtrshort{SEAL}'s intersective scoring function frequently misjudges relative relevance even when the generation component has produced sufficient evidence to retrieve the correct passage. The non-monotonic relationship between n-gram quantity and success (Table~\ref{tab:ngram-count}) further suggests that the additive scoring function can be overwhelmed by volume, ranking topically broad but less relevant passages above more precise alternatives.

\paragraph{Token Repetition}
Contrary to initial hypotheses, correlated identifier generation (token repetition across n-grams) correlates with \textit{higher} retrieval success ($\rho = -0.227$ for diversity, meaning lower diversity yields better results). Queries with concentrated topical focus, where key terms appear across multiple generated n-grams, achieve 55.3\% success compared to 35.5\% for high-diversity queries. This suggests that when \glsxtrshort{SEAL} reinforces specific semantic clusters through repeated vocabulary, it accumulates stronger evidence for the target passage.

\paragraph{Indirect Document Identification}
Only 16.8\% of the top-1 retrieved passage includes the answer in any generated n-gram, yet answer presence correlates with success (75.0\% vs.\ 34.9\%). This indicates that \glsxtrshort{SEAL} primarily operates through indirect identification, generating contextual n-grams that describe the semantic neighborhood of the answer rather than the answer itself. While this approach can succeed when context is distinctive, it fails when multiple passages share similar contextual vocabulary but differ in their specific answers. %TODO rethink this to be sure, I am not sure I am correct. Re-analyze if "answer string presence" is related to answer success through contextual/neighbor/synonyms

\paragraph{Annotation Artifacts in Evaluation}
The LLM-as-a-judge analysis reveals that 48.8\% of cases where the answer appears in a non-ground-truth passage represent genuine retrieval successes (Table~\ref{tab:llm-judge-results}). Adjusting for these annotation artifacts increases \glsxtrshort{SEAL}'s estimated top-2 recall from 52.4\% to 55.6\%--57.2\%. This finding has implications for interpreting retrieval benchmarks more broadly, as incomplete ground-truth annotations systematically penalize systems that find valid alternative evidence.

\subsection{Scope and Limitations}

Three important caveats apply to this analysis:

First, the observable failure modes are not mutually exclusive. The classification framework should permit multi-label assignment.

Second, the analysis is specific to \glsxtrshort{SEAL}'s n-gram-based architecture. The availability of matched n-gram keys with corpus frequencies provides interpretability not present in other \glsxtrshort{gr} systems. Any findings may not generalize to systems using different identifier designs (e.g., hierarchical cluster paths in DSI, or title-based identifiers in GENRE).

Third, as shown in Table~\ref{tab:answer-passage-mismatch}, 6.5\% of queries (426 cases) have the answer string in a different passage than the annotated ground truth. Whether these represent genuine retrieval successes or coincidental string matches requires manual verification.


Additionally, it is worth noting that the empirical analysis is conducted on the \glsxtrlong{nq} dataset, which focuses on open-domain question answering tasks. Failure patterns identified on this dataset may not fully generalize to other \glsxtrshort{ir} scenarios that present different challenges and query characteristics. The findings are primarily applicable to question-answering retrieval scenarios.

The analysis is based on \glsxtrshort{SEAL} model outputs and literature available as of late 2025/early 2026. As generative retrieval is a rapidly evolving field, new failure modes or mitigation strategies may emerge beyond those captured in this work.


\subsection{Generalizability} %of method and findings

\textbf{TODO}

\section{Conclusion} % incl future work

\textbf{TODO}

\section*{AI Use Disclosure}
Some portions of code for this thesis were generated with the assistance of \textit{Claude by Antropic} and \textit{ChatGPT by OpenAI} large language models. All AI-generated code was reviewed, edited, and verified by the author.


\clearpage
\appendix

\section{LLM-as-Judge prompt}

The prompt used for the LLM-as-Ludge component in \section{llm-judge} is pasted below:

\begin{verbatim}
Question: [QUESTION]
Expected answer: [ANSWER]
Retrieved passage title: [PASSAGE TITLE]
Retrieved passage text: [PASSAGE TEXT]

Does this passage answer the question?
- YES: The passage directly answers the question.
- NO: The answer string appears coincidentally but does not answer the question.
- PARTIAL: Contains relevant information but requires inference.

The answer explanation should not exceed 100 words. 
\end{verbatim}